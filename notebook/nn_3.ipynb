{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, os\n",
    "# import cudf\n",
    "# import talib as ta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import jpx_tokyo_market_prediction\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.polynomial.hermite as Herm\n",
    "import math\n",
    "from tensorflow.python.ops import math_ops\n",
    "from scipy import stats\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "import random\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title GroupTimeSeriesSplit { display-mode: \"form\" }\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class GroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_size : int, default=None\n",
    "        Maximum size for a single training set.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.model_selection import GroupTimeSeriesSplit\n",
    "    >>> groups = np.array(['a', 'a', 'a', 'a', 'a', 'a',\\\n",
    "                           'b', 'b', 'b', 'b', 'b',\\\n",
    "                           'c', 'c', 'c', 'c',\\\n",
    "                           'd', 'd', 'd'])\n",
    "    >>> gtss = GroupTimeSeriesSplit(n_splits=3)\n",
    "    >>> for train_idx, test_idx in gtss.split(groups, groups=groups):\n",
    "    ...     print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
    "    ...     print(\"TRAIN GROUP:\", groups[train_idx],\\\n",
    "                  \"TEST GROUP:\", groups[test_idx])\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5] TEST: [6, 7, 8, 9, 10]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a']\\\n",
    "    TEST GROUP: ['b' 'b' 'b' 'b' 'b']\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] TEST: [11, 12, 13, 14]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b']\\\n",
    "    TEST GROUP: ['c' 'c' 'c' 'c']\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\\\n",
    "    TEST: [15, 16, 17]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b' 'c' 'c' 'c' 'c']\\\n",
    "    TEST GROUP: ['d' 'd' 'd']\n",
    "    \"\"\"\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_size=None\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_size = max_train_size\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "        group_test_size = n_groups // n_folds\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "            for train_group_idx in unique_groups[:group_test_start]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "            train_end = train_array.size\n",
    "            if self.max_train_size and self.max_train_size < train_end:\n",
    "                train_array = train_array[train_end -\n",
    "                                          self.max_train_size:train_end]\n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "\n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    "\n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "            \n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "    jet     = plt.cm.get_cmap('jet', 256)\n",
    "    seq     = np.linspace(0, 1, 256)\n",
    "    _       = np.random.shuffle(seq)   # inplace\n",
    "    cmap_data = ListedColormap(jet(seq))    \n",
    "    for ii, (tr, tt) in enumerate(list(cv.split(X=X, y=y, groups=group))):\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0        \n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices), c=indices, marker='_', lw=lw, cmap=cmap_cv, vmin=-.2, vmax=1.2)\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X), c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n",
    "    ax.scatter(range(len(X)), [ii + 2.5] * len(X), c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "    yticklabels = list(range(n_splits)) + ['target', 'day']\n",
    "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels, xlabel='Sample index', ylabel=\"CV iteration\", ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"CPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"TPU\":\n",
    "    print(\"connecting to TPU...\")\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        tpu = None\n",
    "    if tpu:\n",
    "        try:\n",
    "            print(\"initializing  TPU ...\")\n",
    "            tf.config.experimental_connect_to_cluster(tpu)\n",
    "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "            strategy = tf.distribute.TPUStrategy(tpu)\n",
    "            print(\"TPU initialized\")\n",
    "        except: print(\"failed to initialize TPU\")\n",
    "    else: device = \"GPU\"\n",
    "\n",
    "if device != \"TPU\": strategy = tf.distribute.get_strategy()\n",
    "if device == \"GPU\": print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "AUTO     = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED = 2025\n",
    "# set_all_seeds(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_use  = [\n",
    "    # 'day', 'Volume',\n",
    "    'ScaledAdjustedClose',\n",
    "    # 'close_arccos_deg',\n",
    "    'trend_psa_indicator',\n",
    "    'trend_aroon_ind_diff1',\n",
    "    # 'volume_pct_change_ror_1',\n",
    "    'sma_5_25', 'sma_25_30',\n",
    "    'd_atr',\n",
    "    'ror_1', 'ror_5', 'ror_10',\n",
    "    'TradedAmount_1', 'TradedAmount_5',\n",
    "    # 'ror1_ror2', 'ror1_ror3', 'ror1_ror4', 'ror1_ror5',\n",
    "    'ror_1_shift1', 'ror_1_shift2', 'ror_1_shift3', 'ror_1_shift4', 'ror_1_shift5',\n",
    "    'ror_1_shift6', 'ror_1_shift7', 'ror_1_shift8', 'ror_1_shift9',\n",
    "    'd_Amount',\n",
    "    'range_1', 'range_5',\n",
    "    'gap_range_1', 'gap_range_5',\n",
    "    'day_range_1', 'day_range_5',\n",
    "    'hig_range_1', 'hig_range_5',\n",
    "    'mi_1', 'mi_5',\n",
    "    'vola_10',\n",
    "    'hl_5', 'hl_10',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_cols = col_use + ['Date', 'SecuritiesCode', 'Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_parquet('../Output/train_scaling.parquet', columns=read_cols)\n",
    "groups = pd.factorize(pd.to_datetime(X['Date']).dt.strftime('%d').astype(str) + '_' + pd.to_datetime(X['Date']).dt.strftime('%m').astype(str) + '_' +pd.to_datetime(X['Date']).dt.strftime('%Y').astype(str))\n",
    "y = X.Target\n",
    "# X = X.drop(['RowId','Target','AdjustmentFactor','ExpectedDividend','SupervisionFlag','Date'],axis=1)\n",
    "X = X[col_use]\n",
    "# valid = pd.read_parquet('../input/scaling/valid_scaling.parquet')\n",
    "# test = pd.read_parquet('../input/scaling/test_scaling.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV PARAMS\n",
    "FOLDS                = 3\n",
    "GROUP_GAP            = 14\n",
    "# MAX_TEST_GROUP_SIZE  = 180  \n",
    "# MAX_TRAIN_GROUP_SIZE = 485\n",
    "\n",
    "# USE VERBOSE=0 for silent, VERBOSE=1 for interactive, VERBOSE=2 for commit\n",
    "VERBOSE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2286531, 34), (2286531,))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import backend as K\n",
    "def e_swish(beta=0.25):\n",
    "    def beta_swish(x): return x*K.sigmoid(x)*(1+beta)\n",
    "    return beta_swish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlationLoss(x,y, axis=-2):\n",
    "    \n",
    "    \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n",
    "    while trying to have the same mean and variance\"\"\"\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    y = math_ops.cast(y, x.dtype)\n",
    "    n = tf.cast(tf.shape(x)[axis], x.dtype)\n",
    "    xsum = tf.reduce_sum(x, axis=axis)\n",
    "    ysum = tf.reduce_sum(y, axis=axis)\n",
    "    xmean = xsum / n\n",
    "    ymean = ysum / n\n",
    "    xsqsum = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n",
    "    ysqsum = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n",
    "    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n",
    "    corr = cov / tf.sqrt(xsqsum * ysqsum)\n",
    "    return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr ) , dtype=tf.float32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(x, y, axis=-2):\n",
    "    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    y = math_ops.cast(y, x.dtype)\n",
    "    n = tf.cast(tf.shape(x)[axis], x.dtype)\n",
    "    xsum = tf.reduce_sum(x, axis=axis)\n",
    "    ysum = tf.reduce_sum(y, axis=axis)\n",
    "    xmean = xsum / n\n",
    "    ymean = ysum / n\n",
    "    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n",
    "    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n",
    "    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n",
    "    corr = cov / tf.sqrt(xvar * yvar)\n",
    "    return tf.constant(1.0, dtype=x.dtype) - corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_loss(X_train,y_pred):\n",
    "    y_pred = tf.Variable(y_pred,dtype=tf.float64)\n",
    "    port_ret = tf.reduce_sum(tf.multiply(_,y_pred),axis=1)\n",
    "    s_ratio = K.mean(port_ret)/K.std(port_ret)\n",
    "    \n",
    "    return tf.math.exp(-s_ratio,  name='sharpe_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp, dim = 128, fold=0):\n",
    "    # SEED = 2025\n",
    "    SEED = 1\n",
    "    set_all_seeds(SEED)\n",
    "    \n",
    "    features_inputs = tf.keras.layers.Input(shape = (dim, ))\n",
    "    x0      =  tf.keras.layers.BatchNormalization()(features_inputs)\n",
    "    \n",
    "    weight = tf.Variable(tf.keras.backend.random_normal((dim, 1), stddev=hp.Float(f'weight_{fold}',1e-10, 0.09), dtype=tf.float32))\n",
    "    var    = tf.Variable(tf.zeros((1,1), dtype=tf.float32))\n",
    "   \n",
    "    encoder = tf.keras.layers.GaussianNoise(0.4)(x0)\n",
    "    encoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_en0',32, 1024))(encoder)\n",
    "    encoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_en1',32, 1024))(encoder)\n",
    "    encoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_en2',32, 1024))(encoder)\n",
    "    encoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_en2',16, 256))(encoder)\n",
    "    encoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_en2',16, 256))(encoder)\n",
    "    encoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_en2',4, 64))(encoder)\n",
    "    encoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_en2',4, 64))(encoder)\n",
    "    encoder = tf.keras.layers.BatchNormalization()(encoder)\n",
    "    encoder = tf.keras.layers.Activation(e_swish(beta=hp.Float(f'e{fold}_en0',0.001, 1 )))(encoder)\n",
    "    \n",
    "    decoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_de0',4, 64))(encoder)\n",
    "    decoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_de1',32, 1024), name='decoder')(decoder)\n",
    "#     decoder = tf.keras.layers.Dropout(hp.Float(f'dropout{fold}_de0',0.001, 0.8))(encoder)\n",
    "#     decoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_de0',32, 1024), name='decoder')(decoder)\n",
    "    \n",
    "    x_ae = tf.keras.layers.Dense(hp.Int(f'layers{fold}_ae0',32, 1024))(decoder)\n",
    "    x_ae = tf.keras.layers.BatchNormalization()(x_ae)\n",
    "    x_ae = tf.keras.layers.Activation(e_swish(beta=hp.Float(f'e{fold}_ae0',0.001, 1 )))(x_ae)\n",
    "#     x_ae = tf.keras.layers.Dropout(hp.Float(f'dropout{fold}_ae0',0.001, 0.8))(x_ae) \n",
    "    \n",
    "    feature_x = tf.keras.layers.Concatenate()([x0, encoder])\n",
    "    feature_x = tf.keras.layers.BatchNormalization()(feature_x)\n",
    "    feature_x = tf.keras.layers.Dense(hp.Int(f'layers{fold}_fx0',32, 1024))(feature_x)\n",
    "    feature_x = tf.keras.layers.Activation(e_swish(beta=hp.Float(f'e_fx0',0.001, 1 )))(feature_x)\n",
    "#     feature_x = tf.keras.layers.Dropout(hp.Float(f'dropout{fold}_fx0',0.001, 0.8))(feature_x)\n",
    "\n",
    "    x = layers.Dense(hp.Int(f'layers{fold}_x0',32, 1024), activation= e_swish(beta=hp.Float(f'e{fold}_x0',0.001, 1 )), kernel_regularizer=\"l2\")(feature_x)\n",
    "    x = layers.Dense(hp.Int(f'layers{fold}_x1',32, 1024), activation= e_swish(beta=hp.Float(f'e{fold}_x1',0.001, 1 )), kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dense(hp.Int(f'layers{fold}_x2',32, 1024), activation= e_swish(beta=hp.Float(f'e{fold}_x2',0.001, 1 )), kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dense(hp.Int(f'layers{fold}_x3',32, 1024), activation= e_swish(beta=hp.Float(f'e{fold}_x3',0.001, 1 )), kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dense(hp.Int(f'layers{fold}_x3',16, 256), activation= e_swish(beta=hp.Float(f'e{fold}_x4',0.001, 1 )), kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dense(hp.Int(f'layers{fold}_x3',16, 256), activation= e_swish(beta=hp.Float(f'e{fold}_x5',0.001, 1 )), kernel_regularizer=\"l2\")(x)\n",
    "#     x = tf.keras.layers.Dropout(hp.Float(f'dropout{fold}_x0',0.001, 0.8))(x)\n",
    "\n",
    "    mlp_out = layers.Dense(1, name ='mlp_out')(x)\n",
    "\n",
    "    model  = tf.keras.Model(inputs=[features_inputs], outputs=[decoder, mlp_out])\n",
    "    \n",
    "    loss_out = tf.add(tf.matmul(features_inputs,weight), tf.math.reduce_sum(weight*var))\n",
    "    tf.compat.v1.losses.add_loss(loss_out)\n",
    "  \n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Float(f'lr_adam{fold}',1e-3, 1e-5)),\n",
    "                  loss = {'decoder': [tf.keras.losses.CosineSimilarity(axis=-2), \n",
    "                                      tf.keras.losses.MeanSquaredError(), \n",
    "                                      correlationLoss],         \n",
    "                          \n",
    "                          'mlp_out' : [sharpe_loss],\n",
    "                         },\n",
    "                  metrics = {'decoder': [tf.keras.metrics.CosineSimilarity(name='cosine'),\n",
    "                                         tf.keras.metrics.MeanAbsoluteError(name=\"mae\"), \n",
    "                                         correlation, \n",
    "                                         tf.keras.metrics.RootMeanSquaredError(name='rmse')], \n",
    "                             \n",
    "                             'mlp_out' : [tf.keras.metrics.CosineSimilarity(name='cosine'),\n",
    "                                          tf.keras.metrics.MeanAbsoluteError(name=\"mae\"), \n",
    "                                          correlation, \n",
    "                                          tf.keras.metrics.RootMeanSquaredError(name='rmse')],\n",
    "                            },\n",
    "                 ) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = pd.read_pickle(f'../Output/hp-jpx-aemlp/best_hp_ae_jpx_3gkf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(build_model(hp, fold=0), show_shapes=True, expand_nested=True, show_dtype=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [4096*4,4096,4096*8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> AEMLP_FOLD:0\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_14), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(34, 1) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "Epoch 1/200\n",
      "33/33 - 38s - loss: 7.4191 - decoder_loss: -1.6052e-02 - mlp_out_loss: 0.0131 - decoder_cosine: 0.0493 - decoder_mae: 0.4479 - decoder_correlation: 0.9900 - decoder_rmse: 0.7990 - mlp_out_cosine: 0.0078 - mlp_out_mae: 0.0333 - mlp_out_correlation: 0.9956 - mlp_out_rmse: 0.1145 - val_loss: 3.8747 - val_decoder_loss: -1.2320e-02 - val_mlp_out_loss: 0.0054 - val_decoder_cosine: 0.0558 - val_decoder_mae: 0.6109 - val_decoder_correlation: 0.9912 - val_decoder_rmse: 1.5068 - val_mlp_out_cosine: -2.8031e-02 - val_mlp_out_mae: 0.0229 - val_mlp_out_correlation: 1.0018 - val_mlp_out_rmse: 0.0736 - 38s/epoch - 1s/step\n",
      "Epoch 2/200\n",
      "33/33 - 32s - loss: 2.4388 - decoder_loss: -5.9293e-02 - mlp_out_loss: 5.2857e-04 - decoder_cosine: 0.1754 - decoder_mae: 0.5166 - decoder_correlation: 0.9536 - decoder_rmse: 0.8015 - mlp_out_cosine: 0.0128 - mlp_out_mae: 0.0142 - mlp_out_correlation: 0.9948 - mlp_out_rmse: 0.0230 - val_loss: 1.5077 - val_decoder_loss: -3.1444e-02 - val_mlp_out_loss: 0.0015 - val_decoder_cosine: 0.1267 - val_decoder_mae: 0.5256 - val_decoder_correlation: 0.9747 - val_decoder_rmse: 1.1130 - val_mlp_out_cosine: -2.7411e-02 - val_mlp_out_mae: 0.0176 - val_mlp_out_correlation: 1.0041 - val_mlp_out_rmse: 0.0388 - 32s/epoch - 970ms/step\n",
      "Epoch 3/200\n",
      "33/33 - 33s - loss: 1.0650 - decoder_loss: -8.6417e-02 - mlp_out_loss: 4.1809e-04 - decoder_cosine: 0.2402 - decoder_mae: 0.6760 - decoder_correlation: 0.9287 - decoder_rmse: 0.9856 - mlp_out_cosine: 0.0236 - mlp_out_mae: 0.0130 - mlp_out_correlation: 0.9983 - mlp_out_rmse: 0.0204 - val_loss: 0.8062 - val_decoder_loss: -3.6552e-02 - val_mlp_out_loss: 6.4346e-04 - val_decoder_cosine: 0.0948 - val_decoder_mae: 0.4839 - val_decoder_correlation: 0.9649 - val_decoder_rmse: 0.8724 - val_mlp_out_cosine: -2.7313e-02 - val_mlp_out_mae: 0.0164 - val_mlp_out_correlation: 1.0056 - val_mlp_out_rmse: 0.0254 - 33s/epoch - 990ms/step\n",
      "Epoch 4/200\n",
      "33/33 - 34s - loss: 0.5804 - decoder_loss: -9.3446e-02 - mlp_out_loss: 4.0238e-04 - decoder_cosine: 0.2626 - decoder_mae: 0.7180 - decoder_correlation: 0.9228 - decoder_rmse: 1.0321 - mlp_out_cosine: 0.0306 - mlp_out_mae: 0.0127 - mlp_out_correlation: 1.0097 - mlp_out_rmse: 0.0201 - val_loss: 0.4874 - val_decoder_loss: -3.6861e-02 - val_mlp_out_loss: 5.7542e-04 - val_decoder_cosine: 0.0642 - val_decoder_mae: 0.4569 - val_decoder_correlation: 0.9593 - val_decoder_rmse: 0.6525 - val_mlp_out_cosine: -2.4600e-02 - val_mlp_out_mae: 0.0161 - val_mlp_out_correlation: 1.0051 - val_mlp_out_rmse: 0.0240 - 34s/epoch - 1s/step\n",
      "Epoch 5/200\n",
      "33/33 - 36s - loss: 0.3302 - decoder_loss: -9.9969e-02 - mlp_out_loss: 3.9951e-04 - decoder_cosine: 0.2772 - decoder_mae: 0.7315 - decoder_correlation: 0.9153 - decoder_rmse: 0.9961 - mlp_out_cosine: 0.0335 - mlp_out_mae: 0.0127 - mlp_out_correlation: 1.0148 - mlp_out_rmse: 0.0200 - val_loss: 0.3090 - val_decoder_loss: -3.4499e-02 - val_mlp_out_loss: 5.7276e-04 - val_decoder_cosine: 0.0500 - val_decoder_mae: 0.5274 - val_decoder_correlation: 0.9587 - val_decoder_rmse: 0.7557 - val_mlp_out_cosine: -2.2033e-02 - val_mlp_out_mae: 0.0161 - val_mlp_out_correlation: 1.0058 - val_mlp_out_rmse: 0.0239 - 36s/epoch - 1s/step\n",
      "Epoch 6/200\n",
      "33/33 - 34s - loss: 0.1819 - decoder_loss: -1.0374e-01 - mlp_out_loss: 3.9888e-04 - decoder_cosine: 0.2881 - decoder_mae: 0.7469 - decoder_correlation: 0.9110 - decoder_rmse: 0.9886 - mlp_out_cosine: 0.0326 - mlp_out_mae: 0.0127 - mlp_out_correlation: 1.0151 - mlp_out_rmse: 0.0200 - val_loss: 0.1981 - val_decoder_loss: -3.3366e-02 - val_mlp_out_loss: 5.7258e-04 - val_decoder_cosine: 0.0469 - val_decoder_mae: 0.5795 - val_decoder_correlation: 0.9616 - val_decoder_rmse: 0.8317 - val_mlp_out_cosine: -2.1988e-02 - val_mlp_out_mae: 0.0161 - val_mlp_out_correlation: 1.0144 - val_mlp_out_rmse: 0.0239 - 34s/epoch - 1s/step\n",
      "Epoch 7/200\n",
      "33/33 - 34s - loss: 0.0890 - decoder_loss: -1.0522e-01 - mlp_out_loss: 3.9879e-04 - decoder_cosine: 0.2932 - decoder_mae: 0.7283 - decoder_correlation: 0.9095 - decoder_rmse: 0.9465 - mlp_out_cosine: 0.0326 - mlp_out_mae: 0.0127 - mlp_out_correlation: 1.0087 - mlp_out_rmse: 0.0200 - val_loss: 0.1240 - val_decoder_loss: -3.5018e-02 - val_mlp_out_loss: 5.7231e-04 - val_decoder_cosine: 0.0360 - val_decoder_mae: 0.5318 - val_decoder_correlation: 0.9544 - val_decoder_rmse: 0.6708 - val_mlp_out_cosine: -2.1988e-02 - val_mlp_out_mae: 0.0160 - val_mlp_out_correlation: 1.0299 - val_mlp_out_rmse: 0.0239 - 34s/epoch - 1s/step\n",
      "Epoch 8/200\n",
      "33/33 - 34s - loss: 0.0250 - decoder_loss: -1.0938e-01 - mlp_out_loss: 3.9878e-04 - decoder_cosine: 0.3051 - decoder_mae: 0.7134 - decoder_correlation: 0.9047 - decoder_rmse: 0.9232 - mlp_out_cosine: 0.0326 - mlp_out_mae: 0.0127 - mlp_out_correlation: 0.9889 - mlp_out_rmse: 0.0200 - val_loss: 0.0811 - val_decoder_loss: -3.0032e-02 - val_mlp_out_loss: 5.7223e-04 - val_decoder_cosine: 0.0366 - val_decoder_mae: 0.6465 - val_decoder_correlation: 0.9654 - val_decoder_rmse: 1.0492 - val_mlp_out_cosine: -2.1988e-02 - val_mlp_out_mae: 0.0160 - val_mlp_out_correlation: 1.0208 - val_mlp_out_rmse: 0.0239 - 34s/epoch - 1s/step\n",
      "Epoch 9/200\n",
      "33/33 - 41s - loss: -1.5987e-02 - decoder_loss: -1.1055e-01 - mlp_out_loss: 3.9878e-04 - decoder_cosine: 0.3055 - decoder_mae: 0.7249 - decoder_correlation: 0.9037 - decoder_rmse: 0.9404 - mlp_out_cosine: 0.0326 - mlp_out_mae: 0.0127 - mlp_out_correlation: 0.9794 - mlp_out_rmse: 0.0200 - val_loss: 0.0441 - val_decoder_loss: -3.4851e-02 - val_mlp_out_loss: 5.7223e-04 - val_decoder_cosine: 0.0447 - val_decoder_mae: 0.6042 - val_decoder_correlation: 0.9592 - val_decoder_rmse: 0.8565 - val_mlp_out_cosine: -2.1988e-02 - val_mlp_out_mae: 0.0160 - val_mlp_out_correlation: 1.0099 - val_mlp_out_rmse: 0.0239 - 41s/epoch - 1s/step\n",
      "Epoch 10/200\n",
      "33/33 - 38s - loss: -4.4763e-02 - decoder_loss: -1.1235e-01 - mlp_out_loss: 3.9878e-04 - decoder_cosine: 0.3017 - decoder_mae: 0.7035 - decoder_correlation: 0.9012 - decoder_rmse: 0.9228 - mlp_out_cosine: 0.0326 - mlp_out_mae: 0.0127 - mlp_out_correlation: 0.9758 - mlp_out_rmse: 0.0200 - val_loss: 0.0214 - val_decoder_loss: -3.5592e-02 - val_mlp_out_loss: 5.7238e-04 - val_decoder_cosine: 0.0445 - val_decoder_mae: 0.5974 - val_decoder_correlation: 0.9579 - val_decoder_rmse: 0.8288 - val_mlp_out_cosine: -2.1988e-02 - val_mlp_out_mae: 0.0160 - val_mlp_out_correlation: 0.9994 - val_mlp_out_rmse: 0.0239 - 38s/epoch - 1s/step\n",
      "Epoch 11/200\n",
      "33/33 - 37s - loss: -6.3847e-02 - decoder_loss: -1.1294e-01 - mlp_out_loss: 3.9878e-04 - decoder_cosine: 0.3074 - decoder_mae: 0.7046 - decoder_correlation: 0.9004 - decoder_rmse: 0.9281 - mlp_out_cosine: 0.0326 - mlp_out_mae: 0.0127 - mlp_out_correlation: 0.9729 - mlp_out_rmse: 0.0200 - val_loss: 0.0034 - val_decoder_loss: -3.8425e-02 - val_mlp_out_loss: 5.7247e-04 - val_decoder_cosine: 0.0555 - val_decoder_mae: 0.5817 - val_decoder_correlation: 0.9573 - val_decoder_rmse: 0.7602 - val_mlp_out_cosine: -2.1988e-02 - val_mlp_out_mae: 0.0160 - val_mlp_out_correlation: 0.9909 - val_mlp_out_rmse: 0.0239 - 37s/epoch - 1s/step\n",
      "Epoch 12/200\n",
      "33/33 - 39s - loss: -7.7089e-02 - decoder_loss: -1.1332e-01 - mlp_out_loss: 3.9878e-04 - decoder_cosine: 0.3074 - decoder_mae: 0.7026 - decoder_correlation: 0.8993 - decoder_rmse: 0.9361 - mlp_out_cosine: 0.0326 - mlp_out_mae: 0.0127 - mlp_out_correlation: 0.9689 - mlp_out_rmse: 0.0200 - val_loss: -6.9026e-03 - val_decoder_loss: -3.8045e-02 - val_mlp_out_loss: 5.7256e-04 - val_decoder_cosine: 0.0540 - val_decoder_mae: 0.5796 - val_decoder_correlation: 0.9574 - val_decoder_rmse: 0.7783 - val_mlp_out_cosine: -2.1988e-02 - val_mlp_out_mae: 0.0160 - val_mlp_out_correlation: 0.9801 - val_mlp_out_rmse: 0.0239 - 39s/epoch - 1s/step\n",
      "Epoch 13/200\n",
      "33/33 - 38s - loss: -8.7380e-02 - decoder_loss: -1.1452e-01 - mlp_out_loss: 3.9877e-04 - decoder_cosine: 0.3194 - decoder_mae: 0.6760 - decoder_correlation: 0.8989 - decoder_rmse: 0.9057 - mlp_out_cosine: 0.0326 - mlp_out_mae: 0.0127 - mlp_out_correlation: 0.9667 - mlp_out_rmse: 0.0200 - val_loss: -1.8881e-02 - val_decoder_loss: -4.2441e-02 - val_mlp_out_loss: 5.7249e-04 - val_decoder_cosine: 0.0606 - val_decoder_mae: 0.5613 - val_decoder_correlation: 0.9525 - val_decoder_rmse: 0.7197 - val_mlp_out_cosine: -2.1988e-02 - val_mlp_out_mae: 0.0160 - val_mlp_out_correlation: 0.9713 - val_mlp_out_rmse: 0.0239 - 38s/epoch - 1s/step\n",
      "Epoch 14/200\n",
      "33/33 - 37s - loss: -9.6529e-02 - decoder_loss: -1.1715e-01 - mlp_out_loss: 3.9879e-04 - decoder_cosine: 0.3206 - decoder_mae: 0.6492 - decoder_correlation: 0.8958 - decoder_rmse: 0.8658 - mlp_out_cosine: 0.0326 - mlp_out_mae: 0.0127 - mlp_out_correlation: 0.9639 - mlp_out_rmse: 0.0200 - val_loss: -1.8508e-02 - val_decoder_loss: -3.6585e-02 - val_mlp_out_loss: 5.7266e-04 - val_decoder_cosine: 0.0464 - val_decoder_mae: 0.5709 - val_decoder_correlation: 0.9578 - val_decoder_rmse: 0.7771 - val_mlp_out_cosine: -2.1988e-02 - val_mlp_out_mae: 0.0161 - val_mlp_out_correlation: 0.9751 - val_mlp_out_rmse: 0.0239 - 37s/epoch - 1s/step\n",
      "Epoch 15/200\n",
      "33/33 - 37s - loss: -9.9713e-02 - decoder_loss: -1.1559e-01 - mlp_out_loss: 3.9878e-04 - decoder_cosine: 0.3200 - decoder_mae: 0.6835 - decoder_correlation: 0.8974 - decoder_rmse: 0.9056 - mlp_out_cosine: 0.0326 - mlp_out_mae: 0.0127 - mlp_out_correlation: 0.9629 - mlp_out_rmse: 0.0200 - val_loss: -1.7800e-02 - val_decoder_loss: -3.1850e-02 - val_mlp_out_loss: 5.7227e-04 - val_decoder_cosine: 0.0290 - val_decoder_mae: 0.5576 - val_decoder_correlation: 0.9644 - val_decoder_rmse: 0.7866 - val_mlp_out_cosine: -2.1988e-02 - val_mlp_out_mae: 0.0160 - val_mlp_out_correlation: 0.9679 - val_mlp_out_rmse: 0.0239 - 37s/epoch - 1s/step\n",
      "Epoch 16/200\n",
      "33/33 - 36s - loss: -1.0374e-01 - decoder_loss: -1.1611e-01 - mlp_out_loss: 3.9878e-04 - decoder_cosine: 0.3160 - decoder_mae: 0.6408 - decoder_correlation: 0.8967 - decoder_rmse: 0.8724 - mlp_out_cosine: 0.0326 - mlp_out_mae: 0.0127 - mlp_out_correlation: 0.9695 - mlp_out_rmse: 0.0200 - val_loss: -2.2465e-02 - val_decoder_loss: -3.3515e-02 - val_mlp_out_loss: 5.7272e-04 - val_decoder_cosine: 0.0340 - val_decoder_mae: 0.6346 - val_decoder_correlation: 0.9597 - val_decoder_rmse: 0.8710 - val_mlp_out_cosine: -2.1988e-02 - val_mlp_out_mae: 0.0161 - val_mlp_out_correlation: 0.9724 - val_mlp_out_rmse: 0.0239 - 36s/epoch - 1s/step\n",
      "Epoch 17/200\n",
      "33/33 - 37s - loss: -1.0896e-01 - decoder_loss: -1.1871e-01 - mlp_out_loss: 3.9879e-04 - decoder_cosine: 0.3215 - decoder_mae: 0.6459 - decoder_correlation: 0.8933 - decoder_rmse: 0.8703 - mlp_out_cosine: 0.0326 - mlp_out_mae: 0.0127 - mlp_out_correlation: 0.9802 - mlp_out_rmse: 0.0200 - val_loss: -2.2129e-02 - val_decoder_loss: -3.0914e-02 - val_mlp_out_loss: 5.7244e-04 - val_decoder_cosine: 0.0148 - val_decoder_mae: 0.5566 - val_decoder_correlation: 0.9620 - val_decoder_rmse: 0.7509 - val_mlp_out_cosine: -2.1988e-02 - val_mlp_out_mae: 0.0160 - val_mlp_out_correlation: 0.9859 - val_mlp_out_rmse: 0.0239 - 37s/epoch - 1s/step\n",
      "Epoch 18/200\n",
      "33/33 - 37s - loss: -1.1083e-01 - decoder_loss: -1.1858e-01 - mlp_out_loss: 3.9878e-04 - decoder_cosine: 0.3262 - decoder_mae: 0.5771 - decoder_correlation: 0.8944 - decoder_rmse: 0.7812 - mlp_out_cosine: 0.0326 - mlp_out_mae: 0.0127 - mlp_out_correlation: 0.9845 - mlp_out_rmse: 0.0200 - val_loss: -2.3163e-02 - val_decoder_loss: -3.0218e-02 - val_mlp_out_loss: 5.7247e-04 - val_decoder_cosine: 0.0318 - val_decoder_mae: 0.5596 - val_decoder_correlation: 0.9651 - val_decoder_rmse: 0.8135 - val_mlp_out_cosine: -2.1988e-02 - val_mlp_out_mae: 0.0160 - val_mlp_out_correlation: 0.9960 - val_mlp_out_rmse: 0.0239 - 37s/epoch - 1s/step\n",
      "Epoch 19/200\n",
      "33/33 - 39s - loss: -1.1273e-01 - decoder_loss: -1.1895e-01 - mlp_out_loss: 3.9878e-04 - decoder_cosine: 0.3204 - decoder_mae: 0.5462 - decoder_correlation: 0.8936 - decoder_rmse: 0.7405 - mlp_out_cosine: 0.0326 - mlp_out_mae: 0.0127 - mlp_out_correlation: 0.9966 - mlp_out_rmse: 0.0200 - val_loss: -2.6610e-02 - val_decoder_loss: -3.2329e-02 - val_mlp_out_loss: 5.7285e-04 - val_decoder_cosine: 0.0307 - val_decoder_mae: 0.5237 - val_decoder_correlation: 0.9641 - val_decoder_rmse: 0.7105 - val_mlp_out_cosine: -2.1988e-02 - val_mlp_out_mae: 0.0161 - val_mlp_out_correlation: 0.9992 - val_mlp_out_rmse: 0.0239 - 39s/epoch - 1s/step\n",
      "Epoch 20/200\n",
      "33/33 - 38s - loss: -1.1434e-01 - decoder_loss: -1.1937e-01 - mlp_out_loss: 3.9878e-04 - decoder_cosine: 0.3268 - decoder_mae: 0.5434 - decoder_correlation: 0.8930 - decoder_rmse: 0.7335 - mlp_out_cosine: 0.0326 - mlp_out_mae: 0.0127 - mlp_out_correlation: 0.9964 - mlp_out_rmse: 0.0200 - val_loss: -2.2123e-02 - val_decoder_loss: -2.6802e-02 - val_mlp_out_loss: 5.7265e-04 - val_decoder_cosine: 0.0034 - val_decoder_mae: 0.5332 - val_decoder_correlation: 0.9671 - val_decoder_rmse: 0.7283 - val_mlp_out_cosine: -2.1988e-02 - val_mlp_out_mae: 0.0161 - val_mlp_out_correlation: 0.9993 - val_mlp_out_rmse: 0.0239 - 38s/epoch - 1s/step\n",
      "Epoch 21/200\n",
      "33/33 - 43s - loss: -1.1818e-01 - decoder_loss: -1.2228e-01 - mlp_out_loss: 3.9879e-04 - decoder_cosine: 0.3239 - decoder_mae: 0.5141 - decoder_correlation: 0.8902 - decoder_rmse: 0.6953 - mlp_out_cosine: 0.0326 - mlp_out_mae: 0.0127 - mlp_out_correlation: 0.9998 - mlp_out_rmse: 0.0200 - val_loss: -2.1698e-02 - val_decoder_loss: -2.5559e-02 - val_mlp_out_loss: 5.7294e-04 - val_decoder_cosine: 0.0170 - val_decoder_mae: 0.5055 - val_decoder_correlation: 0.9717 - val_decoder_rmse: 0.7444 - val_mlp_out_cosine: -2.1988e-02 - val_mlp_out_mae: 0.0161 - val_mlp_out_correlation: 0.9983 - val_mlp_out_rmse: 0.0239 - 43s/epoch - 1s/step\n",
      "Epoch 22/200\n",
      "33/33 - 41s - loss: -1.1838e-01 - decoder_loss: -1.2175e-01 - mlp_out_loss: 3.9879e-04 - decoder_cosine: 0.3237 - decoder_mae: 0.5257 - decoder_correlation: 0.8904 - decoder_rmse: 0.7182 - mlp_out_cosine: 0.0326 - mlp_out_mae: 0.0127 - mlp_out_correlation: 1.0002 - mlp_out_rmse: 0.0200 - val_loss: -2.0747e-02 - val_decoder_loss: -2.3961e-02 - val_mlp_out_loss: 5.7263e-04 - val_decoder_cosine: 0.0039 - val_decoder_mae: 0.5379 - val_decoder_correlation: 0.9711 - val_decoder_rmse: 0.7894 - val_mlp_out_cosine: -2.1988e-02 - val_mlp_out_mae: 0.0161 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0239 - 41s/epoch - 1s/step\n",
      "Epoch 23/200\n",
      "33/33 - 35s - loss: -1.2119e-01 - decoder_loss: -1.2398e-01 - mlp_out_loss: 3.9878e-04 - decoder_cosine: 0.3252 - decoder_mae: 0.5019 - decoder_correlation: 0.8878 - decoder_rmse: 0.6912 - mlp_out_cosine: 0.0326 - mlp_out_mae: 0.0127 - mlp_out_correlation: 0.9999 - mlp_out_rmse: 0.0200 - val_loss: -1.7374e-02 - val_decoder_loss: -2.0074e-02 - val_mlp_out_loss: 5.7244e-04 - val_decoder_cosine: -1.6451e-02 - val_decoder_mae: 0.5343 - val_decoder_correlation: 0.9744 - val_decoder_rmse: 0.7638 - val_mlp_out_cosine: -2.1988e-02 - val_mlp_out_mae: 0.0160 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0239 - 35s/epoch - 1s/step\n",
      "================================================================================================\n",
      ">>> AEMLP_FOLD:1\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_15), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(34, 1) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "Epoch 1/200\n",
      "268/268 - 16s - loss: 0.6498 - decoder_loss: -6.6320e-02 - mlp_out_loss: 0.0011 - decoder_cosine: 0.1673 - decoder_mae: 0.7953 - decoder_correlation: 0.9347 - decoder_rmse: 1.1353 - mlp_out_cosine: -3.3608e-03 - mlp_out_mae: 0.0162 - mlp_out_correlation: 1.0084 - mlp_out_rmse: 0.0338 - val_loss: 0.0737 - val_decoder_loss: -5.7761e-02 - val_mlp_out_loss: 7.1084e-04 - val_decoder_cosine: 0.1535 - val_decoder_mae: 0.7315 - val_decoder_correlation: 0.9545 - val_decoder_rmse: 0.9635 - val_mlp_out_cosine: -5.5855e-03 - val_mlp_out_mae: 0.0178 - val_mlp_out_correlation: 1.0056 - val_mlp_out_rmse: 0.0267 - 16s/epoch - 59ms/step\n",
      "Epoch 2/200\n",
      "268/268 - 12s - loss: -4.4438e-02 - decoder_loss: -8.6146e-02 - mlp_out_loss: 4.8775e-04 - decoder_cosine: 0.2106 - decoder_mae: 0.8343 - decoder_correlation: 0.9152 - decoder_rmse: 1.1919 - mlp_out_cosine: 0.0023 - mlp_out_mae: 0.0144 - mlp_out_correlation: 1.0176 - mlp_out_rmse: 0.0221 - val_loss: -4.6201e-02 - val_decoder_loss: -5.3360e-02 - val_mlp_out_loss: 7.1079e-04 - val_decoder_cosine: 0.1143 - val_decoder_mae: 0.8469 - val_decoder_correlation: 0.9529 - val_decoder_rmse: 1.3133 - val_mlp_out_cosine: -7.2266e-03 - val_mlp_out_mae: 0.0178 - val_mlp_out_correlation: 1.0069 - val_mlp_out_rmse: 0.0267 - 12s/epoch - 44ms/step\n",
      "Epoch 3/200\n",
      "268/268 - 11s - loss: -8.5467e-02 - decoder_loss: -8.8205e-02 - mlp_out_loss: 4.8776e-04 - decoder_cosine: 0.2140 - decoder_mae: 0.8651 - decoder_correlation: 0.9130 - decoder_rmse: 1.2601 - mlp_out_cosine: 0.0022 - mlp_out_mae: 0.0144 - mlp_out_correlation: 1.0089 - mlp_out_rmse: 0.0221 - val_loss: -4.5850e-02 - val_decoder_loss: -4.7048e-02 - val_mlp_out_loss: 7.1077e-04 - val_decoder_cosine: 0.0846 - val_decoder_mae: 1.0688 - val_decoder_correlation: 0.9599 - val_decoder_rmse: 1.8554 - val_mlp_out_cosine: -7.2266e-03 - val_mlp_out_mae: 0.0178 - val_mlp_out_correlation: 1.0023 - val_mlp_out_rmse: 0.0267 - 11s/epoch - 41ms/step\n",
      "Epoch 4/200\n",
      "268/268 - 10s - loss: -9.0278e-02 - decoder_loss: -9.0949e-02 - mlp_out_loss: 4.8774e-04 - decoder_cosine: 0.2144 - decoder_mae: 0.8132 - decoder_correlation: 0.9102 - decoder_rmse: 1.2061 - mlp_out_cosine: 0.0019 - mlp_out_mae: 0.0144 - mlp_out_correlation: nan - mlp_out_rmse: 0.0221 - val_loss: -4.8224e-02 - val_decoder_loss: -4.8976e-02 - val_mlp_out_loss: 7.1091e-04 - val_decoder_cosine: 0.0907 - val_decoder_mae: 1.0499 - val_decoder_correlation: 0.9579 - val_decoder_rmse: 1.7713 - val_mlp_out_cosine: -7.2266e-03 - val_mlp_out_mae: 0.0178 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0267 - 10s/epoch - 35ms/step\n",
      "Epoch 5/200\n",
      "268/268 - 13s - loss: -9.1974e-02 - decoder_loss: -9.2477e-02 - mlp_out_loss: 4.8777e-04 - decoder_cosine: 0.2186 - decoder_mae: 0.7804 - decoder_correlation: 0.9087 - decoder_rmse: 1.1708 - mlp_out_cosine: 0.0022 - mlp_out_mae: 0.0144 - mlp_out_correlation: nan - mlp_out_rmse: 0.0221 - val_loss: -4.6827e-02 - val_decoder_loss: -4.7540e-02 - val_mlp_out_loss: 7.1085e-04 - val_decoder_cosine: 0.0836 - val_decoder_mae: 1.0387 - val_decoder_correlation: 0.9598 - val_decoder_rmse: 1.8182 - val_mlp_out_cosine: -7.2266e-03 - val_mlp_out_mae: 0.0178 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0267 - 13s/epoch - 50ms/step\n",
      "Epoch 6/200\n",
      "268/268 - 13s - loss: -9.2959e-02 - decoder_loss: -9.3448e-02 - mlp_out_loss: 4.8778e-04 - decoder_cosine: 0.2138 - decoder_mae: 0.7215 - decoder_correlation: 0.9078 - decoder_rmse: 1.1004 - mlp_out_cosine: 0.0030 - mlp_out_mae: 0.0144 - mlp_out_correlation: nan - mlp_out_rmse: 0.0221 - val_loss: -4.8876e-02 - val_decoder_loss: -4.9587e-02 - val_mlp_out_loss: 7.1095e-04 - val_decoder_cosine: 0.0879 - val_decoder_mae: 1.0098 - val_decoder_correlation: 0.9579 - val_decoder_rmse: 1.6957 - val_mlp_out_cosine: -7.2266e-03 - val_mlp_out_mae: 0.0178 - val_mlp_out_correlation: nan - val_mlp_out_rmse: 0.0267 - 13s/epoch - 49ms/step\n",
      "Epoch 7/200\n",
      "268/268 - 13s - loss: -9.4816e-02 - decoder_loss: -9.5304e-02 - mlp_out_loss: 4.8779e-04 - decoder_cosine: 0.2184 - decoder_mae: 0.6614 - decoder_correlation: 0.9057 - decoder_rmse: 1.0080 - mlp_out_cosine: 0.0020 - mlp_out_mae: 0.0144 - mlp_out_correlation: nan - mlp_out_rmse: 0.0221 - val_loss: -4.9716e-02 - val_decoder_loss: -5.0426e-02 - val_mlp_out_loss: 7.1087e-04 - val_decoder_cosine: 0.0989 - val_decoder_mae: 0.8349 - val_decoder_correlation: 0.9581 - val_decoder_rmse: 1.4240 - val_mlp_out_cosine: -7.2266e-03 - val_mlp_out_mae: 0.0178 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0267 - 13s/epoch - 48ms/step\n",
      "Epoch 8/200\n",
      "268/268 - 13s - loss: -9.7863e-02 - decoder_loss: -9.8351e-02 - mlp_out_loss: 4.8778e-04 - decoder_cosine: 0.2214 - decoder_mae: 0.6208 - decoder_correlation: 0.9026 - decoder_rmse: 0.9392 - mlp_out_cosine: 0.0032 - mlp_out_mae: 0.0144 - mlp_out_correlation: nan - mlp_out_rmse: 0.0221 - val_loss: -4.8542e-02 - val_decoder_loss: -4.9253e-02 - val_mlp_out_loss: 7.1084e-04 - val_decoder_cosine: 0.1079 - val_decoder_mae: 0.7099 - val_decoder_correlation: 0.9608 - val_decoder_rmse: 1.1843 - val_mlp_out_cosine: -7.2266e-03 - val_mlp_out_mae: 0.0178 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0267 - 13s/epoch - 48ms/step\n",
      "Epoch 9/200\n",
      "268/268 - 12s - loss: -1.0105e-01 - decoder_loss: -1.0153e-01 - mlp_out_loss: 4.8801e-04 - decoder_cosine: 0.2300 - decoder_mae: 0.6330 - decoder_correlation: 0.8995 - decoder_rmse: 0.9522 - mlp_out_cosine: 0.0011 - mlp_out_mae: 0.0144 - mlp_out_correlation: nan - mlp_out_rmse: 0.0221 - val_loss: -4.5619e-02 - val_decoder_loss: -4.6330e-02 - val_mlp_out_loss: 7.1120e-04 - val_decoder_cosine: 0.1053 - val_decoder_mae: 0.7858 - val_decoder_correlation: 0.9642 - val_decoder_rmse: 1.3812 - val_mlp_out_cosine: 0.0072 - val_mlp_out_mae: 0.0178 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0267 - 12s/epoch - 44ms/step\n",
      "Epoch 10/200\n",
      "268/268 - 13s - loss: -1.0305e-01 - decoder_loss: -1.0354e-01 - mlp_out_loss: 4.8791e-04 - decoder_cosine: 0.2377 - decoder_mae: 0.6566 - decoder_correlation: 0.8974 - decoder_rmse: 0.9996 - mlp_out_cosine: 0.0023 - mlp_out_mae: 0.0144 - mlp_out_correlation: nan - mlp_out_rmse: 0.0221 - val_loss: -4.8373e-02 - val_decoder_loss: -4.9084e-02 - val_mlp_out_loss: 7.1078e-04 - val_decoder_cosine: 0.1127 - val_decoder_mae: 0.7851 - val_decoder_correlation: 0.9603 - val_decoder_rmse: 1.3653 - val_mlp_out_cosine: -7.2266e-03 - val_mlp_out_mae: 0.0178 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0267 - 13s/epoch - 49ms/step\n",
      "Epoch 11/200\n",
      "268/268 - 13s - loss: -1.0439e-01 - decoder_loss: -1.0488e-01 - mlp_out_loss: 4.8796e-04 - decoder_cosine: 0.2374 - decoder_mae: 0.6407 - decoder_correlation: 0.8960 - decoder_rmse: 0.9807 - mlp_out_cosine: 0.0015 - mlp_out_mae: 0.0144 - mlp_out_correlation: nan - mlp_out_rmse: 0.0221 - val_loss: -5.3743e-02 - val_decoder_loss: -5.4454e-02 - val_mlp_out_loss: 7.1103e-04 - val_decoder_cosine: 0.1310 - val_decoder_mae: 0.8492 - val_decoder_correlation: 0.9581 - val_decoder_rmse: 1.3721 - val_mlp_out_cosine: 0.0072 - val_mlp_out_mae: 0.0178 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0267 - 13s/epoch - 47ms/step\n",
      "Epoch 12/200\n",
      "268/268 - 12s - loss: -1.0607e-01 - decoder_loss: -1.0655e-01 - mlp_out_loss: 4.8797e-04 - decoder_cosine: 0.2474 - decoder_mae: 0.6241 - decoder_correlation: 0.8944 - decoder_rmse: 0.9500 - mlp_out_cosine: 0.0026 - mlp_out_mae: 0.0144 - mlp_out_correlation: nan - mlp_out_rmse: 0.0221 - val_loss: -4.4428e-02 - val_decoder_loss: -4.5139e-02 - val_mlp_out_loss: 7.1101e-04 - val_decoder_cosine: 0.0920 - val_decoder_mae: 0.7444 - val_decoder_correlation: 0.9658 - val_decoder_rmse: 1.2968 - val_mlp_out_cosine: 0.0072 - val_mlp_out_mae: 0.0178 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0267 - 12s/epoch - 47ms/step\n",
      "Epoch 13/200\n",
      "268/268 - 15s - loss: -1.0836e-01 - decoder_loss: -1.0885e-01 - mlp_out_loss: 4.8804e-04 - decoder_cosine: 0.2452 - decoder_mae: 0.6051 - decoder_correlation: 0.8920 - decoder_rmse: 0.9301 - mlp_out_cosine: -1.5300e-04 - mlp_out_mae: 0.0144 - mlp_out_correlation: nan - mlp_out_rmse: 0.0221 - val_loss: -4.9948e-02 - val_decoder_loss: -5.0658e-02 - val_mlp_out_loss: 7.1083e-04 - val_decoder_cosine: 0.1152 - val_decoder_mae: 0.7583 - val_decoder_correlation: 0.9624 - val_decoder_rmse: 1.2934 - val_mlp_out_cosine: -7.2266e-03 - val_mlp_out_mae: 0.0178 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0267 - 15s/epoch - 58ms/step\n",
      "Epoch 14/200\n",
      "268/268 - 14s - loss: -1.1047e-01 - decoder_loss: -1.1096e-01 - mlp_out_loss: 4.8813e-04 - decoder_cosine: 0.2513 - decoder_mae: 0.5899 - decoder_correlation: 0.8899 - decoder_rmse: 0.9127 - mlp_out_cosine: -1.1499e-04 - mlp_out_mae: 0.0144 - mlp_out_correlation: nan - mlp_out_rmse: 0.0221 - val_loss: -5.0563e-02 - val_decoder_loss: -5.1274e-02 - val_mlp_out_loss: 7.1078e-04 - val_decoder_cosine: 0.1035 - val_decoder_mae: 0.8371 - val_decoder_correlation: 0.9599 - val_decoder_rmse: 1.3704 - val_mlp_out_cosine: -7.2266e-03 - val_mlp_out_mae: 0.0178 - val_mlp_out_correlation: nan - val_mlp_out_rmse: 0.0267 - 14s/epoch - 51ms/step\n",
      "Epoch 15/200\n",
      "268/268 - 11s - loss: -1.1225e-01 - decoder_loss: -1.1274e-01 - mlp_out_loss: 4.8820e-04 - decoder_cosine: 0.2524 - decoder_mae: 0.6230 - decoder_correlation: 0.8881 - decoder_rmse: 0.9593 - mlp_out_cosine: 0.0016 - mlp_out_mae: 0.0144 - mlp_out_correlation: nan - mlp_out_rmse: 0.0221 - val_loss: -4.3479e-02 - val_decoder_loss: -4.4191e-02 - val_mlp_out_loss: 7.1175e-04 - val_decoder_cosine: 0.0955 - val_decoder_mae: 0.7680 - val_decoder_correlation: 0.9629 - val_decoder_rmse: 1.4195 - val_mlp_out_cosine: -7.2266e-03 - val_mlp_out_mae: 0.0178 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0267 - 11s/epoch - 42ms/step\n",
      "Epoch 16/200\n",
      "268/268 - 11s - loss: -1.1298e-01 - decoder_loss: -1.1346e-01 - mlp_out_loss: 4.8788e-04 - decoder_cosine: 0.2577 - decoder_mae: 0.6039 - decoder_correlation: 0.8874 - decoder_rmse: 0.9372 - mlp_out_cosine: 0.0017 - mlp_out_mae: 0.0144 - mlp_out_correlation: nan - mlp_out_rmse: 0.0221 - val_loss: -4.8930e-02 - val_decoder_loss: -4.9641e-02 - val_mlp_out_loss: 7.1098e-04 - val_decoder_cosine: 0.1093 - val_decoder_mae: 0.7658 - val_decoder_correlation: 0.9637 - val_decoder_rmse: 1.2900 - val_mlp_out_cosine: -7.2266e-03 - val_mlp_out_mae: 0.0178 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0267 - 11s/epoch - 41ms/step\n",
      "Epoch 17/200\n",
      "268/268 - 11s - loss: -1.1311e-01 - decoder_loss: -1.1360e-01 - mlp_out_loss: 4.8798e-04 - decoder_cosine: 0.2600 - decoder_mae: 0.6117 - decoder_correlation: 0.8873 - decoder_rmse: 0.9461 - mlp_out_cosine: 0.0017 - mlp_out_mae: 0.0144 - mlp_out_correlation: nan - mlp_out_rmse: 0.0221 - val_loss: -4.5963e-02 - val_decoder_loss: -4.6675e-02 - val_mlp_out_loss: 7.1151e-04 - val_decoder_cosine: 0.1068 - val_decoder_mae: 0.7424 - val_decoder_correlation: 0.9626 - val_decoder_rmse: 1.3141 - val_mlp_out_cosine: -7.2266e-03 - val_mlp_out_mae: 0.0178 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0267 - 11s/epoch - 41ms/step\n",
      "Epoch 18/200\n",
      "268/268 - 11s - loss: -1.1415e-01 - decoder_loss: -1.1464e-01 - mlp_out_loss: 4.8815e-04 - decoder_cosine: 0.2561 - decoder_mae: 0.5954 - decoder_correlation: 0.8861 - decoder_rmse: 0.9266 - mlp_out_cosine: 0.0018 - mlp_out_mae: 0.0144 - mlp_out_correlation: nan - mlp_out_rmse: 0.0221 - val_loss: -4.4858e-02 - val_decoder_loss: -4.5569e-02 - val_mlp_out_loss: 7.1123e-04 - val_decoder_cosine: 0.0991 - val_decoder_mae: 0.7700 - val_decoder_correlation: 0.9653 - val_decoder_rmse: 1.4155 - val_mlp_out_cosine: 0.0072 - val_mlp_out_mae: 0.0178 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0267 - 11s/epoch - 41ms/step\n",
      "================================================================================================\n",
      ">>> AEMLP_FOLD:2\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_16), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(34, 1) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "Epoch 1/200\n",
      "52/52 - 16s - loss: 1.7389 - decoder_loss: -3.7302e-02 - mlp_out_loss: 9.7881e-04 - decoder_cosine: 0.0725 - decoder_mae: 0.4667 - decoder_correlation: 0.9648 - decoder_rmse: 0.7164 - mlp_out_cosine: 0.0083 - mlp_out_mae: 0.0182 - mlp_out_correlation: 0.9891 - mlp_out_rmse: 0.0313 - val_loss: 1.1732 - val_decoder_loss: -1.7808e-02 - val_mlp_out_loss: 5.0859e-04 - val_decoder_cosine: 0.0618 - val_decoder_mae: 0.3915 - val_decoder_correlation: 0.9706 - val_decoder_rmse: 0.6031 - val_mlp_out_cosine: -9.1554e-03 - val_mlp_out_mae: 0.0155 - val_mlp_out_correlation: 0.9880 - val_mlp_out_rmse: 0.0226 - 16s/epoch - 316ms/step\n",
      "Epoch 2/200\n",
      "52/52 - 17s - loss: 0.7825 - decoder_loss: -7.4857e-02 - mlp_out_loss: 5.5994e-04 - decoder_cosine: 0.1295 - decoder_mae: 0.5360 - decoder_correlation: 0.9267 - decoder_rmse: 0.8273 - mlp_out_cosine: 0.0169 - mlp_out_mae: 0.0155 - mlp_out_correlation: 0.9704 - mlp_out_rmse: 0.0237 - val_loss: 0.5502 - val_decoder_loss: -2.6827e-02 - val_mlp_out_loss: 4.9685e-04 - val_decoder_cosine: 0.0866 - val_decoder_mae: 0.3696 - val_decoder_correlation: 0.9637 - val_decoder_rmse: 0.4929 - val_mlp_out_cosine: 0.0109 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 0.9823 - val_mlp_out_rmse: 0.0223 - 17s/epoch - 319ms/step\n",
      "Epoch 3/200\n",
      "52/52 - 16s - loss: 0.3303 - decoder_loss: -8.7922e-02 - mlp_out_loss: 5.5890e-04 - decoder_cosine: 0.1524 - decoder_mae: 0.4241 - decoder_correlation: 0.9142 - decoder_rmse: 0.6829 - mlp_out_cosine: 0.0013 - mlp_out_mae: 0.0155 - mlp_out_correlation: 0.9585 - mlp_out_rmse: 0.0236 - val_loss: 0.2571 - val_decoder_loss: -2.6949e-02 - val_mlp_out_loss: 4.9736e-04 - val_decoder_cosine: 0.0776 - val_decoder_mae: 0.2339 - val_decoder_correlation: 0.9630 - val_decoder_rmse: 0.3124 - val_mlp_out_cosine: -9.9530e-03 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 0.9885 - val_mlp_out_rmse: 0.0223 - 16s/epoch - 315ms/step\n",
      "Epoch 4/200\n",
      "52/52 - 16s - loss: 0.1086 - decoder_loss: -9.8330e-02 - mlp_out_loss: 5.5913e-04 - decoder_cosine: 0.1674 - decoder_mae: 0.3332 - decoder_correlation: 0.8981 - decoder_rmse: 0.5234 - mlp_out_cosine: -2.0949e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: 0.9540 - mlp_out_rmse: 0.0236 - val_loss: 0.1086 - val_decoder_loss: -3.2658e-02 - val_mlp_out_loss: 4.9664e-04 - val_decoder_cosine: 0.0891 - val_decoder_mae: 0.1700 - val_decoder_correlation: 0.9638 - val_decoder_rmse: 0.2232 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 0.9874 - val_mlp_out_rmse: 0.0223 - 16s/epoch - 301ms/step\n",
      "Epoch 5/200\n",
      "52/52 - 19s - loss: -3.1050e-03 - decoder_loss: -1.0608e-01 - mlp_out_loss: 5.5918e-04 - decoder_cosine: 0.1821 - decoder_mae: 0.2702 - decoder_correlation: 0.8942 - decoder_rmse: 0.4246 - mlp_out_cosine: -3.2656e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: 0.9562 - mlp_out_rmse: 0.0236 - val_loss: 0.0348 - val_decoder_loss: -3.5201e-02 - val_mlp_out_loss: 4.9687e-04 - val_decoder_cosine: 0.1007 - val_decoder_mae: 0.1460 - val_decoder_correlation: 0.9639 - val_decoder_rmse: 0.1892 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 0.9861 - val_mlp_out_rmse: 0.0223 - 19s/epoch - 374ms/step\n",
      "Epoch 6/200\n",
      "52/52 - 25s - loss: -5.9128e-02 - decoder_loss: -1.0986e-01 - mlp_out_loss: 5.5913e-04 - decoder_cosine: 0.1891 - decoder_mae: 0.2168 - decoder_correlation: 0.8913 - decoder_rmse: 0.3379 - mlp_out_cosine: -2.8203e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: 0.9607 - mlp_out_rmse: 0.0236 - val_loss: -6.2000e-03 - val_decoder_loss: -4.0282e-02 - val_mlp_out_loss: 4.9662e-04 - val_decoder_cosine: 0.1089 - val_decoder_mae: 0.1509 - val_decoder_correlation: 0.9637 - val_decoder_rmse: 0.1948 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 0.9859 - val_mlp_out_rmse: 0.0223 - 25s/epoch - 481ms/step\n",
      "Epoch 7/200\n",
      "52/52 - 21s - loss: -8.8921e-02 - decoder_loss: -1.1337e-01 - mlp_out_loss: 5.5918e-04 - decoder_cosine: 0.1951 - decoder_mae: 0.1910 - decoder_correlation: 0.8849 - decoder_rmse: 0.3008 - mlp_out_cosine: -1.9650e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: 0.9674 - mlp_out_rmse: 0.0236 - val_loss: -2.1116e-02 - val_decoder_loss: -3.7265e-02 - val_mlp_out_loss: 4.9686e-04 - val_decoder_cosine: 0.1012 - val_decoder_mae: 0.1207 - val_decoder_correlation: 0.9628 - val_decoder_rmse: 0.1567 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 0.9991 - val_mlp_out_rmse: 0.0223 - 21s/epoch - 412ms/step\n",
      "Epoch 8/200\n",
      "52/52 - 17s - loss: -1.0384e-01 - decoder_loss: -1.1534e-01 - mlp_out_loss: 5.5913e-04 - decoder_cosine: 0.1914 - decoder_mae: 0.1570 - decoder_correlation: 0.8845 - decoder_rmse: 0.2536 - mlp_out_cosine: -3.4747e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: 0.9900 - mlp_out_rmse: 0.0236 - val_loss: -2.4957e-02 - val_decoder_loss: -3.2440e-02 - val_mlp_out_loss: 4.9678e-04 - val_decoder_cosine: 0.0908 - val_decoder_mae: 0.0966 - val_decoder_correlation: 0.9645 - val_decoder_rmse: 0.1251 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0013 - val_mlp_out_rmse: 0.0223 - 17s/epoch - 325ms/step\n",
      "Epoch 9/200\n",
      "52/52 - 16s - loss: -1.1507e-01 - decoder_loss: -1.2041e-01 - mlp_out_loss: 5.5915e-04 - decoder_cosine: 0.1937 - decoder_mae: 0.1212 - decoder_correlation: 0.8786 - decoder_rmse: 0.1997 - mlp_out_cosine: -2.9982e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: 1.0004 - mlp_out_rmse: 0.0236 - val_loss: 2.6333e-04 - val_decoder_loss: -3.2083e-03 - val_mlp_out_loss: 4.9676e-04 - val_decoder_cosine: 0.0622 - val_decoder_mae: 0.0656 - val_decoder_correlation: 0.9881 - val_decoder_rmse: 0.0949 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 16s/epoch - 303ms/step\n",
      "Epoch 10/200\n",
      "52/52 - 15s - loss: -1.1248e-01 - decoder_loss: -1.1504e-01 - mlp_out_loss: 5.5912e-04 - decoder_cosine: 0.1924 - decoder_mae: 0.1945 - decoder_correlation: 0.8849 - decoder_rmse: 0.3201 - mlp_out_cosine: -3.4747e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: nan - mlp_out_rmse: 0.0236 - val_loss: -2.3448e-02 - val_decoder_loss: -2.5151e-02 - val_mlp_out_loss: 4.9702e-04 - val_decoder_cosine: 0.0947 - val_decoder_mae: 0.0915 - val_decoder_correlation: 0.9721 - val_decoder_rmse: 0.1236 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 15s/epoch - 288ms/step\n",
      "Epoch 11/200\n",
      "52/52 - 17s - loss: -1.2441e-01 - decoder_loss: -1.2576e-01 - mlp_out_loss: 5.5913e-04 - decoder_cosine: 0.1953 - decoder_mae: 0.0952 - decoder_correlation: 0.8763 - decoder_rmse: 0.1615 - mlp_out_cosine: -3.3742e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: 1.0000 - mlp_out_rmse: 0.0236 - val_loss: -4.4461e-02 - val_decoder_loss: -4.5424e-02 - val_mlp_out_loss: 4.9712e-04 - val_decoder_cosine: 0.1213 - val_decoder_mae: 0.0696 - val_decoder_correlation: 0.9655 - val_decoder_rmse: 0.0939 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 17s/epoch - 323ms/step\n",
      "Epoch 12/200\n",
      "52/52 - 17s - loss: -1.2989e-01 - decoder_loss: -1.3075e-01 - mlp_out_loss: 5.5915e-04 - decoder_cosine: 0.1996 - decoder_mae: 0.0811 - decoder_correlation: 0.8676 - decoder_rmse: 0.1411 - mlp_out_cosine: -3.2785e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: nan - mlp_out_rmse: 0.0236 - val_loss: -3.4593e-02 - val_decoder_loss: -3.5260e-02 - val_mlp_out_loss: 4.9683e-04 - val_decoder_cosine: 0.1067 - val_decoder_mae: 0.0489 - val_decoder_correlation: 0.9734 - val_decoder_rmse: 0.0682 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 17s/epoch - 323ms/step\n",
      "Epoch 13/200\n",
      "52/52 - 15s - loss: -1.3260e-01 - decoder_loss: -1.3327e-01 - mlp_out_loss: 5.5912e-04 - decoder_cosine: 0.2008 - decoder_mae: 0.0725 - decoder_correlation: 0.8698 - decoder_rmse: 0.1301 - mlp_out_cosine: -3.4747e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: 1.0000 - mlp_out_rmse: 0.0236 - val_loss: -2.2514e-02 - val_decoder_loss: -2.3070e-02 - val_mlp_out_loss: 4.9712e-04 - val_decoder_cosine: 0.1106 - val_decoder_mae: 0.0578 - val_decoder_correlation: 0.9759 - val_decoder_rmse: 0.0819 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 15s/epoch - 294ms/step\n",
      "Epoch 14/200\n",
      "52/52 - 17s - loss: -1.3216e-01 - decoder_loss: -1.3275e-01 - mlp_out_loss: 5.5914e-04 - decoder_cosine: 0.1950 - decoder_mae: 0.0913 - decoder_correlation: 0.8678 - decoder_rmse: 0.1574 - mlp_out_cosine: -2.6744e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: 1.0000 - mlp_out_rmse: 0.0236 - val_loss: -1.6161e-02 - val_decoder_loss: -1.6678e-02 - val_mlp_out_loss: 4.9682e-04 - val_decoder_cosine: 0.0781 - val_decoder_mae: 0.0531 - val_decoder_correlation: 0.9860 - val_decoder_rmse: 0.0783 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 17s/epoch - 318ms/step\n",
      "Epoch 15/200\n",
      "52/52 - 16s - loss: -1.3708e-01 - decoder_loss: -1.3765e-01 - mlp_out_loss: 5.5914e-04 - decoder_cosine: 0.1974 - decoder_mae: 0.0739 - decoder_correlation: 0.8652 - decoder_rmse: 0.1351 - mlp_out_cosine: -3.4448e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: nan - mlp_out_rmse: 0.0236 - val_loss: -3.7899e-02 - val_decoder_loss: -3.8401e-02 - val_mlp_out_loss: 4.9648e-04 - val_decoder_cosine: 0.1241 - val_decoder_mae: 0.0583 - val_decoder_correlation: 0.9699 - val_decoder_rmse: 0.0809 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 16s/epoch - 313ms/step\n",
      "Epoch 16/200\n",
      "52/52 - 17s - loss: -1.4013e-01 - decoder_loss: -1.4069e-01 - mlp_out_loss: 5.5914e-04 - decoder_cosine: 0.2001 - decoder_mae: 0.0677 - decoder_correlation: 0.8596 - decoder_rmse: 0.1236 - mlp_out_cosine: -3.4747e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: nan - mlp_out_rmse: 0.0236 - val_loss: -4.1271e-02 - val_decoder_loss: -4.1770e-02 - val_mlp_out_loss: 4.9648e-04 - val_decoder_cosine: 0.1102 - val_decoder_mae: 0.0624 - val_decoder_correlation: 0.9680 - val_decoder_rmse: 0.0880 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 17s/epoch - 326ms/step\n",
      "Epoch 17/200\n",
      "52/52 - 18s - loss: -1.4003e-01 - decoder_loss: -1.4059e-01 - mlp_out_loss: 5.5927e-04 - decoder_cosine: 0.1966 - decoder_mae: 0.0700 - decoder_correlation: 0.8605 - decoder_rmse: 0.1268 - mlp_out_cosine: -1.7616e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: nan - mlp_out_rmse: 0.0236 - val_loss: -2.5963e-02 - val_decoder_loss: -2.6461e-02 - val_mlp_out_loss: 4.9730e-04 - val_decoder_cosine: 0.0930 - val_decoder_mae: 0.0508 - val_decoder_correlation: 0.9792 - val_decoder_rmse: 0.0747 - val_mlp_out_cosine: -9.8621e-03 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 18s/epoch - 337ms/step\n",
      "Epoch 18/200\n",
      "52/52 - 17s - loss: -1.4325e-01 - decoder_loss: -1.4381e-01 - mlp_out_loss: 5.5914e-04 - decoder_cosine: 0.2088 - decoder_mae: 0.0645 - decoder_correlation: 0.8572 - decoder_rmse: 0.1183 - mlp_out_cosine: -3.4831e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: nan - mlp_out_rmse: 0.0236 - val_loss: -2.4904e-02 - val_decoder_loss: -2.5401e-02 - val_mlp_out_loss: 4.9647e-04 - val_decoder_cosine: 0.0989 - val_decoder_mae: 0.0509 - val_decoder_correlation: 0.9828 - val_decoder_rmse: 0.0733 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 17s/epoch - 334ms/step\n",
      "Epoch 19/200\n",
      "52/52 - 20s - loss: -1.4259e-01 - decoder_loss: -1.4314e-01 - mlp_out_loss: 5.5921e-04 - decoder_cosine: 0.2050 - decoder_mae: 0.0672 - decoder_correlation: 0.8579 - decoder_rmse: 0.1219 - mlp_out_cosine: -1.8728e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: nan - mlp_out_rmse: 0.0236 - val_loss: -2.9492e-02 - val_decoder_loss: -2.9989e-02 - val_mlp_out_loss: 4.9703e-04 - val_decoder_cosine: 0.1027 - val_decoder_mae: 0.0510 - val_decoder_correlation: 0.9739 - val_decoder_rmse: 0.0741 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 20s/epoch - 375ms/step\n",
      "Epoch 20/200\n",
      "52/52 - 17s - loss: -1.4611e-01 - decoder_loss: -1.4667e-01 - mlp_out_loss: 5.5913e-04 - decoder_cosine: 0.2109 - decoder_mae: 0.0661 - decoder_correlation: 0.8540 - decoder_rmse: 0.1217 - mlp_out_cosine: -3.2103e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: nan - mlp_out_rmse: 0.0236 - val_loss: -3.1629e-02 - val_decoder_loss: -3.2126e-02 - val_mlp_out_loss: 4.9693e-04 - val_decoder_cosine: 0.1103 - val_decoder_mae: 0.0540 - val_decoder_correlation: 0.9714 - val_decoder_rmse: 0.0775 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 17s/epoch - 336ms/step\n",
      "Epoch 21/200\n",
      "52/52 - 17s - loss: -1.4556e-01 - decoder_loss: -1.4611e-01 - mlp_out_loss: 5.5912e-04 - decoder_cosine: 0.2069 - decoder_mae: 0.0631 - decoder_correlation: 0.8538 - decoder_rmse: 0.1168 - mlp_out_cosine: -2.7820e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: 1.0000 - mlp_out_rmse: 0.0236 - val_loss: -2.7005e-02 - val_decoder_loss: -2.7503e-02 - val_mlp_out_loss: 4.9755e-04 - val_decoder_cosine: 0.1038 - val_decoder_mae: 0.0656 - val_decoder_correlation: 0.9694 - val_decoder_rmse: 0.0929 - val_mlp_out_cosine: -9.8621e-03 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 17s/epoch - 332ms/step\n",
      "Epoch 22/200\n",
      "52/52 - 18s - loss: -1.4638e-01 - decoder_loss: -1.4694e-01 - mlp_out_loss: 5.5922e-04 - decoder_cosine: 0.2090 - decoder_mae: 0.0679 - decoder_correlation: 0.8552 - decoder_rmse: 0.1241 - mlp_out_cosine: -1.8046e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: nan - mlp_out_rmse: 0.0236 - val_loss: -2.2283e-02 - val_decoder_loss: -2.2780e-02 - val_mlp_out_loss: 4.9690e-04 - val_decoder_cosine: 0.0799 - val_decoder_mae: 0.0465 - val_decoder_correlation: 0.9852 - val_decoder_rmse: 0.0686 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 18s/epoch - 339ms/step\n",
      "Epoch 23/200\n",
      "52/52 - 18s - loss: -1.4651e-01 - decoder_loss: -1.4707e-01 - mlp_out_loss: 5.5914e-04 - decoder_cosine: 0.2055 - decoder_mae: 0.0642 - decoder_correlation: 0.8546 - decoder_rmse: 0.1189 - mlp_out_cosine: -3.4747e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: nan - mlp_out_rmse: 0.0236 - val_loss: -2.4816e-02 - val_decoder_loss: -2.5314e-02 - val_mlp_out_loss: 4.9776e-04 - val_decoder_cosine: 0.0788 - val_decoder_mae: 0.0500 - val_decoder_correlation: 0.9764 - val_decoder_rmse: 0.0732 - val_mlp_out_cosine: -9.8621e-03 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 18s/epoch - 342ms/step\n",
      "Epoch 24/200\n",
      "52/52 - 17s - loss: -1.4676e-01 - decoder_loss: -1.4732e-01 - mlp_out_loss: 5.5917e-04 - decoder_cosine: 0.2154 - decoder_mae: 0.0667 - decoder_correlation: 0.8521 - decoder_rmse: 0.1220 - mlp_out_cosine: -2.9702e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: nan - mlp_out_rmse: 0.0236 - val_loss: -3.1238e-02 - val_decoder_loss: -3.1735e-02 - val_mlp_out_loss: 4.9651e-04 - val_decoder_cosine: 0.1048 - val_decoder_mae: 0.0509 - val_decoder_correlation: 0.9780 - val_decoder_rmse: 0.0732 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 17s/epoch - 318ms/step\n",
      "Epoch 25/200\n",
      "52/52 - 18s - loss: -1.4514e-01 - decoder_loss: -1.4570e-01 - mlp_out_loss: 5.5917e-04 - decoder_cosine: 0.2137 - decoder_mae: 0.0703 - decoder_correlation: 0.8554 - decoder_rmse: 0.1285 - mlp_out_cosine: -3.3455e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: nan - mlp_out_rmse: 0.0236 - val_loss: -3.6935e-02 - val_decoder_loss: -3.7432e-02 - val_mlp_out_loss: 4.9714e-04 - val_decoder_cosine: 0.1033 - val_decoder_mae: 0.0649 - val_decoder_correlation: 0.9688 - val_decoder_rmse: 0.0913 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 18s/epoch - 354ms/step\n",
      "Epoch 26/200\n",
      "52/52 - 18s - loss: -1.4013e-01 - decoder_loss: -1.4069e-01 - mlp_out_loss: 5.5914e-04 - decoder_cosine: 0.2000 - decoder_mae: 0.0948 - decoder_correlation: 0.8601 - decoder_rmse: 0.1700 - mlp_out_cosine: -1.9195e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: nan - mlp_out_rmse: 0.0236 - val_loss: -1.7706e-02 - val_decoder_loss: -1.8204e-02 - val_mlp_out_loss: 4.9732e-04 - val_decoder_cosine: 0.0737 - val_decoder_mae: 0.0543 - val_decoder_correlation: 0.9843 - val_decoder_rmse: 0.0834 - val_mlp_out_cosine: -9.8621e-03 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 18s/epoch - 345ms/step\n",
      "Epoch 27/200\n",
      "52/52 - 15s - loss: -1.4621e-01 - decoder_loss: -1.4676e-01 - mlp_out_loss: 5.5916e-04 - decoder_cosine: 0.2107 - decoder_mae: 0.0758 - decoder_correlation: 0.8525 - decoder_rmse: 0.1381 - mlp_out_cosine: -2.3394e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: nan - mlp_out_rmse: 0.0236 - val_loss: -2.8614e-02 - val_decoder_loss: -2.9111e-02 - val_mlp_out_loss: 4.9698e-04 - val_decoder_cosine: 0.0804 - val_decoder_mae: 0.0512 - val_decoder_correlation: 0.9758 - val_decoder_rmse: 0.0786 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 15s/epoch - 293ms/step\n",
      "Epoch 28/200\n",
      "52/52 - 15s - loss: -1.4704e-01 - decoder_loss: -1.4760e-01 - mlp_out_loss: 5.5921e-04 - decoder_cosine: 0.2129 - decoder_mae: 0.0727 - decoder_correlation: 0.8496 - decoder_rmse: 0.1359 - mlp_out_cosine: -2.6600e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: nan - mlp_out_rmse: 0.0236 - val_loss: -2.4523e-02 - val_decoder_loss: -2.5019e-02 - val_mlp_out_loss: 4.9668e-04 - val_decoder_cosine: 0.0859 - val_decoder_mae: 0.0531 - val_decoder_correlation: 0.9789 - val_decoder_rmse: 0.0791 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 15s/epoch - 288ms/step\n",
      "Epoch 29/200\n",
      "52/52 - 15s - loss: -1.4740e-01 - decoder_loss: -1.4796e-01 - mlp_out_loss: 5.5924e-04 - decoder_cosine: 0.2144 - decoder_mae: 0.0732 - decoder_correlation: 0.8510 - decoder_rmse: 0.1355 - mlp_out_cosine: -1.4338e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: nan - mlp_out_rmse: 0.0236 - val_loss: -3.3391e-02 - val_decoder_loss: -3.3887e-02 - val_mlp_out_loss: 4.9696e-04 - val_decoder_cosine: 0.0968 - val_decoder_mae: 0.0503 - val_decoder_correlation: 0.9769 - val_decoder_rmse: 0.0758 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 15s/epoch - 283ms/step\n",
      "Epoch 30/200\n",
      "52/52 - 16s - loss: -1.5091e-01 - decoder_loss: -1.5147e-01 - mlp_out_loss: 5.5916e-04 - decoder_cosine: 0.2134 - decoder_mae: 0.0668 - decoder_correlation: 0.8491 - decoder_rmse: 0.1255 - mlp_out_cosine: -2.2760e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: nan - mlp_out_rmse: 0.0236 - val_loss: -2.3557e-02 - val_decoder_loss: -2.4056e-02 - val_mlp_out_loss: 4.9875e-04 - val_decoder_cosine: 0.0768 - val_decoder_mae: 0.0484 - val_decoder_correlation: 0.9806 - val_decoder_rmse: 0.0707 - val_mlp_out_cosine: -9.8621e-03 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 16s/epoch - 299ms/step\n",
      "Epoch 31/200\n",
      "52/52 - 18s - loss: -1.4688e-01 - decoder_loss: -1.4743e-01 - mlp_out_loss: 5.5921e-04 - decoder_cosine: 0.2082 - decoder_mae: 0.0749 - decoder_correlation: 0.8534 - decoder_rmse: 0.1370 - mlp_out_cosine: -2.6365e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: nan - mlp_out_rmse: 0.0236 - val_loss: -8.9949e-03 - val_decoder_loss: -9.4914e-03 - val_mlp_out_loss: 4.9655e-04 - val_decoder_cosine: 0.0589 - val_decoder_mae: 0.0630 - val_decoder_correlation: 0.9888 - val_decoder_rmse: 0.0996 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 18s/epoch - 347ms/step\n",
      "Epoch 32/200\n",
      "52/52 - 21s - loss: -1.3091e-01 - decoder_loss: -1.3147e-01 - mlp_out_loss: 5.5918e-04 - decoder_cosine: 0.1766 - decoder_mae: 0.1299 - decoder_correlation: 0.8704 - decoder_rmse: 0.2291 - mlp_out_cosine: -1.8109e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: nan - mlp_out_rmse: 0.0236 - val_loss: -2.6308e-02 - val_decoder_loss: -2.6806e-02 - val_mlp_out_loss: 4.9794e-04 - val_decoder_cosine: 0.0874 - val_decoder_mae: 0.0661 - val_decoder_correlation: 0.9796 - val_decoder_rmse: 0.0987 - val_mlp_out_cosine: -9.8621e-03 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 21s/epoch - 413ms/step\n",
      "Epoch 33/200\n",
      "52/52 - 18s - loss: -1.4967e-01 - decoder_loss: -1.5023e-01 - mlp_out_loss: 5.5932e-04 - decoder_cosine: 0.2087 - decoder_mae: 0.0746 - decoder_correlation: 0.8529 - decoder_rmse: 0.1393 - mlp_out_cosine: -1.0430e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: nan - mlp_out_rmse: 0.0236 - val_loss: -2.7060e-02 - val_decoder_loss: -2.7559e-02 - val_mlp_out_loss: 4.9819e-04 - val_decoder_cosine: 0.0931 - val_decoder_mae: 0.0556 - val_decoder_correlation: 0.9781 - val_decoder_rmse: 0.0832 - val_mlp_out_cosine: -9.8621e-03 - val_mlp_out_mae: 0.0153 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0223 - 18s/epoch - 349ms/step\n",
      "================================================================================================\n"
     ]
    }
   ],
   "source": [
    "gkf = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, \n",
    "                                 group_gap = GROUP_GAP, \n",
    "                                #  max_train_group_size = MAX_TRAIN_GROUP_SIZE, \n",
    "                                #  max_test_group_size  = MAX_TEST_GROUP_SIZE\n",
    "                                 ).split(X, y, groups[0])\n",
    "models = []\n",
    "for fold, (train_idx, val_idx) in enumerate(list(gkf)):\n",
    "    x_train, x_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    print(f'>>> AEMLP_FOLD:{fold}')\n",
    "    K.clear_session()\n",
    "    with strategy.scope(): model = build_model(hp, dim = x_train.shape[1], fold=fold)\n",
    "    model_save = tf.keras.callbacks.ModelCheckpoint('./fold-%i.hdf5' %(fold), \n",
    "                                                         monitor = 'val_mlp_out_rmse', verbose = 0, \n",
    "                                                         save_best_only = True, save_weights_only = True,\n",
    "                                                         mode = 'min', save_freq = 'epoch')\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_mlp_out_rmse', patience=15, mode='min', restore_best_weights=True)\n",
    "    history = model.fit(x_train, y_train ,\n",
    "                        epochs          = 200, \n",
    "                        callbacks       = [model_save, early_stop], \n",
    "                        validation_data = (x_val, y_val), \n",
    "                        batch_size      = batch_size[fold],\n",
    "                        verbose         = 2) \n",
    "    print('='*96)\n",
    "    models.append(model)\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_parquet('../Output/valid_scaling.parquet')\n",
    "test = pd.read_parquet('../Output/test_scaling.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_rank(df):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df (pd.DataFrame): including predict column\n",
    "    Returns:\n",
    "        df (pd.DataFrame): df with Rank\n",
    "    \"\"\"\n",
    "    # sort records to set Rank\n",
    "    df = df.sort_values(\"predict\", ascending=False)\n",
    "    # set Rank starting from 0\n",
    "    df.loc[:, \"Rank\"] = np.arange(len(df[\"predict\"]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2, rank_col='Rank') -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df (pd.DataFrame): predicted results\n",
    "        portfolio_size (int): # of equities to buy/sell\n",
    "        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n",
    "    Returns:\n",
    "        (float): sharpe ratio\n",
    "    \"\"\"\n",
    "\n",
    "    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): predicted results\n",
    "            portfolio_size (int): # of equities to buy/sell\n",
    "            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n",
    "        Returns:\n",
    "            (float): spread return\n",
    "        \"\"\"\n",
    "        assert df[rank_col].min() == 0\n",
    "        assert df[rank_col].max() == len(df[rank_col]) - 1\n",
    "        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n",
    "        purchase = (df.sort_values(by=rank_col)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n",
    "        short = (df.sort_values(by=rank_col, ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n",
    "        return purchase - short\n",
    "\n",
    "    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n",
    "    sharpe_ratio = buf.mean() / buf.std()\n",
    "    return sharpe_ratio, buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ScaledAdjustedClose</th>\n",
       "      <th>trend_psa_indicator</th>\n",
       "      <th>trend_aroon_ind_diff1</th>\n",
       "      <th>sma_5_25</th>\n",
       "      <th>sma_25_30</th>\n",
       "      <th>d_atr</th>\n",
       "      <th>ror_1</th>\n",
       "      <th>ror_5</th>\n",
       "      <th>ror_10</th>\n",
       "      <th>TradedAmount_1</th>\n",
       "      <th>TradedAmount_5</th>\n",
       "      <th>ror_1_shift1</th>\n",
       "      <th>ror_1_shift2</th>\n",
       "      <th>ror_1_shift3</th>\n",
       "      <th>ror_1_shift4</th>\n",
       "      <th>ror_1_shift5</th>\n",
       "      <th>ror_1_shift6</th>\n",
       "      <th>ror_1_shift7</th>\n",
       "      <th>ror_1_shift8</th>\n",
       "      <th>ror_1_shift9</th>\n",
       "      <th>d_Amount</th>\n",
       "      <th>range_1</th>\n",
       "      <th>range_5</th>\n",
       "      <th>gap_range_1</th>\n",
       "      <th>gap_range_5</th>\n",
       "      <th>day_range_1</th>\n",
       "      <th>day_range_5</th>\n",
       "      <th>hig_range_1</th>\n",
       "      <th>hig_range_5</th>\n",
       "      <th>mi_1</th>\n",
       "      <th>mi_5</th>\n",
       "      <th>vola_10</th>\n",
       "      <th>hl_5</th>\n",
       "      <th>hl_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2288402</th>\n",
       "      <td>-0.969892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076375</td>\n",
       "      <td>0.090829</td>\n",
       "      <td>4.706147</td>\n",
       "      <td>-0.076190</td>\n",
       "      <td>-0.116397</td>\n",
       "      <td>-0.121730</td>\n",
       "      <td>7.289201e+09</td>\n",
       "      <td>3.646480e+09</td>\n",
       "      <td>-0.011506</td>\n",
       "      <td>-0.017472</td>\n",
       "      <td>-0.017172</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.005035</td>\n",
       "      <td>-0.003012</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>3.865548</td>\n",
       "      <td>0.087831</td>\n",
       "      <td>0.039321</td>\n",
       "      <td>0.052910</td>\n",
       "      <td>0.013879</td>\n",
       "      <td>0.034921</td>\n",
       "      <td>0.027917</td>\n",
       "      <td>0.011640</td>\n",
       "      <td>0.011974</td>\n",
       "      <td>1.204943e-11</td>\n",
       "      <td>1.143334e-11</td>\n",
       "      <td>0.007177</td>\n",
       "      <td>16.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288523</th>\n",
       "      <td>0.245653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.040451</td>\n",
       "      <td>-0.040275</td>\n",
       "      <td>0.923833</td>\n",
       "      <td>0.026768</td>\n",
       "      <td>-0.009081</td>\n",
       "      <td>0.014588</td>\n",
       "      <td>8.768386e+10</td>\n",
       "      <td>1.504537e+11</td>\n",
       "      <td>0.008508</td>\n",
       "      <td>-0.026907</td>\n",
       "      <td>-0.033692</td>\n",
       "      <td>0.017692</td>\n",
       "      <td>-0.033737</td>\n",
       "      <td>0.005476</td>\n",
       "      <td>-0.020560</td>\n",
       "      <td>0.044019</td>\n",
       "      <td>0.030619</td>\n",
       "      <td>0.763711</td>\n",
       "      <td>0.033095</td>\n",
       "      <td>0.034460</td>\n",
       "      <td>0.017034</td>\n",
       "      <td>0.011593</td>\n",
       "      <td>0.018170</td>\n",
       "      <td>0.026869</td>\n",
       "      <td>0.008436</td>\n",
       "      <td>0.013331</td>\n",
       "      <td>3.774399e-13</td>\n",
       "      <td>2.919821e-13</td>\n",
       "      <td>0.011041</td>\n",
       "      <td>31.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287372</th>\n",
       "      <td>-0.519343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>0.051674</td>\n",
       "      <td>0.053797</td>\n",
       "      <td>5.053454</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>-0.096986</td>\n",
       "      <td>3.008863e+08</td>\n",
       "      <td>1.058135e+08</td>\n",
       "      <td>-0.012016</td>\n",
       "      <td>0.013532</td>\n",
       "      <td>-0.015979</td>\n",
       "      <td>0.012129</td>\n",
       "      <td>-0.006693</td>\n",
       "      <td>-0.007968</td>\n",
       "      <td>-0.009211</td>\n",
       "      <td>-0.011704</td>\n",
       "      <td>0.007864</td>\n",
       "      <td>4.300527</td>\n",
       "      <td>0.139189</td>\n",
       "      <td>0.045818</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>0.006204</td>\n",
       "      <td>0.139189</td>\n",
       "      <td>0.045279</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.019414</td>\n",
       "      <td>4.625973e-10</td>\n",
       "      <td>5.030325e-10</td>\n",
       "      <td>0.027868</td>\n",
       "      <td>42.0</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288388</th>\n",
       "      <td>0.802228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.121066</td>\n",
       "      <td>-0.155476</td>\n",
       "      <td>1.314994</td>\n",
       "      <td>0.016082</td>\n",
       "      <td>0.040514</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>1.897406e+10</td>\n",
       "      <td>6.092362e+10</td>\n",
       "      <td>0.004523</td>\n",
       "      <td>0.002916</td>\n",
       "      <td>0.004884</td>\n",
       "      <td>0.011528</td>\n",
       "      <td>-0.008167</td>\n",
       "      <td>-0.000653</td>\n",
       "      <td>-0.001304</td>\n",
       "      <td>0.009214</td>\n",
       "      <td>-0.000658</td>\n",
       "      <td>0.688261</td>\n",
       "      <td>0.016404</td>\n",
       "      <td>0.015455</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.005867</td>\n",
       "      <td>0.014796</td>\n",
       "      <td>0.013618</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.007612</td>\n",
       "      <td>8.645482e-13</td>\n",
       "      <td>5.510298e-13</td>\n",
       "      <td>0.009210</td>\n",
       "      <td>24.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288181</th>\n",
       "      <td>-0.933359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.026280</td>\n",
       "      <td>0.038176</td>\n",
       "      <td>2.447600</td>\n",
       "      <td>0.038579</td>\n",
       "      <td>0.021072</td>\n",
       "      <td>0.023537</td>\n",
       "      <td>1.426336e+08</td>\n",
       "      <td>9.625044e+07</td>\n",
       "      <td>-0.006086</td>\n",
       "      <td>-0.022024</td>\n",
       "      <td>0.019417</td>\n",
       "      <td>-0.007827</td>\n",
       "      <td>-0.006579</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.004225</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>-0.001811</td>\n",
       "      <td>2.528974</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.024599</td>\n",
       "      <td>0.022045</td>\n",
       "      <td>0.008837</td>\n",
       "      <td>0.026944</td>\n",
       "      <td>0.019626</td>\n",
       "      <td>0.010410</td>\n",
       "      <td>0.008703</td>\n",
       "      <td>3.048248e-10</td>\n",
       "      <td>2.504696e-10</td>\n",
       "      <td>0.021658</td>\n",
       "      <td>57.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2369992</th>\n",
       "      <td>0.061290</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.313069</td>\n",
       "      <td>-0.295134</td>\n",
       "      <td>1.450737</td>\n",
       "      <td>-0.075838</td>\n",
       "      <td>0.013540</td>\n",
       "      <td>0.459610</td>\n",
       "      <td>5.396728e+09</td>\n",
       "      <td>5.738070e+09</td>\n",
       "      <td>0.164271</td>\n",
       "      <td>-0.170358</td>\n",
       "      <td>0.087037</td>\n",
       "      <td>0.044487</td>\n",
       "      <td>0.109442</td>\n",
       "      <td>0.071264</td>\n",
       "      <td>0.058394</td>\n",
       "      <td>0.084433</td>\n",
       "      <td>0.055710</td>\n",
       "      <td>2.047311</td>\n",
       "      <td>0.156966</td>\n",
       "      <td>0.164847</td>\n",
       "      <td>0.022928</td>\n",
       "      <td>0.020691</td>\n",
       "      <td>0.156966</td>\n",
       "      <td>0.163615</td>\n",
       "      <td>0.058201</td>\n",
       "      <td>0.045203</td>\n",
       "      <td>2.908549e-11</td>\n",
       "      <td>2.925303e-11</td>\n",
       "      <td>0.011594</td>\n",
       "      <td>89.0</td>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2369865</th>\n",
       "      <td>0.798726</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.197518</td>\n",
       "      <td>0.184281</td>\n",
       "      <td>1.272239</td>\n",
       "      <td>0.034179</td>\n",
       "      <td>0.227749</td>\n",
       "      <td>0.532680</td>\n",
       "      <td>2.999349e+09</td>\n",
       "      <td>1.786231e+09</td>\n",
       "      <td>0.129514</td>\n",
       "      <td>0.021628</td>\n",
       "      <td>-0.033210</td>\n",
       "      <td>0.064136</td>\n",
       "      <td>-0.015464</td>\n",
       "      <td>0.086835</td>\n",
       "      <td>0.060921</td>\n",
       "      <td>-0.054775</td>\n",
       "      <td>0.163399</td>\n",
       "      <td>2.907139</td>\n",
       "      <td>0.127894</td>\n",
       "      <td>0.124434</td>\n",
       "      <td>0.007718</td>\n",
       "      <td>0.011465</td>\n",
       "      <td>0.127894</td>\n",
       "      <td>0.124434</td>\n",
       "      <td>0.085998</td>\n",
       "      <td>0.062521</td>\n",
       "      <td>4.264064e-11</td>\n",
       "      <td>7.882136e-11</td>\n",
       "      <td>0.017156</td>\n",
       "      <td>66.0</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368917</th>\n",
       "      <td>0.507060</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.023935</td>\n",
       "      <td>-0.023789</td>\n",
       "      <td>2.670614</td>\n",
       "      <td>-0.112158</td>\n",
       "      <td>-0.036245</td>\n",
       "      <td>-0.042475</td>\n",
       "      <td>1.088228e+09</td>\n",
       "      <td>4.011016e+08</td>\n",
       "      <td>0.044723</td>\n",
       "      <td>0.056711</td>\n",
       "      <td>-0.031136</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>-0.014652</td>\n",
       "      <td>0.025352</td>\n",
       "      <td>0.030978</td>\n",
       "      <td>-0.030047</td>\n",
       "      <td>-0.016620</td>\n",
       "      <td>3.986210</td>\n",
       "      <td>0.145548</td>\n",
       "      <td>0.064465</td>\n",
       "      <td>0.011986</td>\n",
       "      <td>0.007327</td>\n",
       "      <td>0.145548</td>\n",
       "      <td>0.062648</td>\n",
       "      <td>0.021404</td>\n",
       "      <td>0.008866</td>\n",
       "      <td>1.337477e-10</td>\n",
       "      <td>2.071032e-10</td>\n",
       "      <td>0.014817</td>\n",
       "      <td>360.0</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2370104</th>\n",
       "      <td>0.644526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.043922</td>\n",
       "      <td>0.041250</td>\n",
       "      <td>3.676971</td>\n",
       "      <td>0.086057</td>\n",
       "      <td>0.152601</td>\n",
       "      <td>0.147296</td>\n",
       "      <td>2.909146e+09</td>\n",
       "      <td>1.060751e+09</td>\n",
       "      <td>0.037288</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.006865</td>\n",
       "      <td>0.010405</td>\n",
       "      <td>0.002317</td>\n",
       "      <td>0.029833</td>\n",
       "      <td>-0.008284</td>\n",
       "      <td>-0.036488</td>\n",
       "      <td>0.009206</td>\n",
       "      <td>4.536994</td>\n",
       "      <td>0.087146</td>\n",
       "      <td>0.033348</td>\n",
       "      <td>0.023965</td>\n",
       "      <td>0.006631</td>\n",
       "      <td>0.068627</td>\n",
       "      <td>0.029644</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>0.007016</td>\n",
       "      <td>2.995586e-11</td>\n",
       "      <td>3.186361e-11</td>\n",
       "      <td>0.013241</td>\n",
       "      <td>50.0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2370223</th>\n",
       "      <td>0.012016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.105799</td>\n",
       "      <td>-0.103341</td>\n",
       "      <td>4.281735</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.228723</td>\n",
       "      <td>0.228723</td>\n",
       "      <td>7.455756e+08</td>\n",
       "      <td>2.328300e+08</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.016043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.005319</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>-0.005348</td>\n",
       "      <td>0.016304</td>\n",
       "      <td>-0.041667</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>8.369056</td>\n",
       "      <td>0.123810</td>\n",
       "      <td>0.058598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004267</td>\n",
       "      <td>0.123810</td>\n",
       "      <td>0.058598</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.009006</td>\n",
       "      <td>1.660590e-10</td>\n",
       "      <td>3.930147e-10</td>\n",
       "      <td>0.023246</td>\n",
       "      <td>76.0</td>\n",
       "      <td>215.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84000 rows  34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ScaledAdjustedClose  trend_psa_indicator  trend_aroon_ind_diff1  \\\n",
       "2288402            -0.969892                  0.0                    0.0   \n",
       "2288523             0.245653                  0.0                    0.0   \n",
       "2287372            -0.519343                  0.0                  -64.0   \n",
       "2288388             0.802228                  0.0                    4.0   \n",
       "2288181            -0.933359                  1.0                    0.0   \n",
       "...                      ...                  ...                    ...   \n",
       "2369992             0.061290                  0.0                    0.0   \n",
       "2369865             0.798726                  0.0                    4.0   \n",
       "2368917             0.507060                 -1.0                    0.0   \n",
       "2370104             0.644526                  0.0                    4.0   \n",
       "2370223             0.012016                  0.0                    4.0   \n",
       "\n",
       "         sma_5_25  sma_25_30     d_atr     ror_1     ror_5    ror_10  \\\n",
       "2288402  0.076375   0.090829  4.706147 -0.076190 -0.116397 -0.121730   \n",
       "2288523 -0.040451  -0.040275  0.923833  0.026768 -0.009081  0.014588   \n",
       "2287372  0.051674   0.053797  5.053454 -0.068919 -0.071429 -0.096986   \n",
       "2288388  0.121066  -0.155476  1.314994  0.016082  0.040514  0.038803   \n",
       "2288181 -0.026280   0.038176  2.447600  0.038579  0.021072  0.023537   \n",
       "...           ...        ...       ...       ...       ...       ...   \n",
       "2369992  0.313069  -0.295134  1.450737 -0.075838  0.013540  0.459610   \n",
       "2369865  0.197518   0.184281  1.272239  0.034179  0.227749  0.532680   \n",
       "2368917 -0.023935  -0.023789  2.670614 -0.112158 -0.036245 -0.042475   \n",
       "2370104  0.043922   0.041250  3.676971  0.086057  0.152601  0.147296   \n",
       "2370223 -0.105799  -0.103341  4.281735  0.100000  0.228723  0.228723   \n",
       "\n",
       "         TradedAmount_1  TradedAmount_5  ror_1_shift1  ror_1_shift2  \\\n",
       "2288402    7.289201e+09    3.646480e+09     -0.011506     -0.017472   \n",
       "2288523    8.768386e+10    1.504537e+11      0.008508     -0.026907   \n",
       "2287372    3.008863e+08    1.058135e+08     -0.012016      0.013532   \n",
       "2288388    1.897406e+10    6.092362e+10      0.004523      0.002916   \n",
       "2288181    1.426336e+08    9.625044e+07     -0.006086     -0.022024   \n",
       "...                 ...             ...           ...           ...   \n",
       "2369992    5.396728e+09    5.738070e+09      0.164271     -0.170358   \n",
       "2369865    2.999349e+09    1.786231e+09      0.129514      0.021628   \n",
       "2368917    1.088228e+09    4.011016e+08      0.044723      0.056711   \n",
       "2370104    2.909146e+09    1.060751e+09      0.037288      0.005682   \n",
       "2370223    7.455756e+08    2.328300e+08      0.105263      0.016043   \n",
       "\n",
       "         ror_1_shift3  ror_1_shift4  ror_1_shift5  ror_1_shift6  ror_1_shift7  \\\n",
       "2288402     -0.017172      0.002024      0.000000     -0.005035     -0.003012   \n",
       "2288523     -0.033692      0.017692     -0.033737      0.005476     -0.020560   \n",
       "2287372     -0.015979      0.012129     -0.006693     -0.007968     -0.009211   \n",
       "2288388      0.004884      0.011528     -0.008167     -0.000653     -0.001304   \n",
       "2288181      0.019417     -0.007827     -0.006579      0.004808      0.004225   \n",
       "...               ...           ...           ...           ...           ...   \n",
       "2369992      0.087037      0.044487      0.109442      0.071264      0.058394   \n",
       "2369865     -0.033210      0.064136     -0.015464      0.086835      0.060921   \n",
       "2368917     -0.031136      0.014870     -0.014652      0.025352      0.030978   \n",
       "2370104      0.006865      0.010405      0.002317      0.029833     -0.008284   \n",
       "2370223      0.000000     -0.005319      0.010753     -0.005348      0.016304   \n",
       "\n",
       "         ror_1_shift8  ror_1_shift9  d_Amount   range_1   range_5  \\\n",
       "2288402      0.001005      0.001006  3.865548  0.087831  0.039321   \n",
       "2288523      0.044019      0.030619  0.763711  0.033095  0.034460   \n",
       "2287372     -0.011704      0.007864  4.300527  0.139189  0.045818   \n",
       "2288388      0.009214     -0.000658  0.688261  0.016404  0.015455   \n",
       "2288181      0.001814     -0.001811  2.528974  0.043478  0.024599   \n",
       "...               ...           ...       ...       ...       ...   \n",
       "2369992      0.084433      0.055710  2.047311  0.156966  0.164847   \n",
       "2369865     -0.054775      0.163399  2.907139  0.127894  0.124434   \n",
       "2368917     -0.030047     -0.016620  3.986210  0.145548  0.064465   \n",
       "2370104     -0.036488      0.009206  4.536994  0.087146  0.033348   \n",
       "2370223     -0.041667      0.021277  8.369056  0.123810  0.058598   \n",
       "\n",
       "         gap_range_1  gap_range_5  day_range_1  day_range_5  hig_range_1  \\\n",
       "2288402     0.052910     0.013879     0.034921     0.027917     0.011640   \n",
       "2288523     0.017034     0.011593     0.018170     0.026869     0.008436   \n",
       "2287372     0.016216     0.006204     0.139189     0.045279     0.054054   \n",
       "2288388     0.003538     0.005867     0.014796     0.013618     0.002252   \n",
       "2288181     0.022045     0.008837     0.026944     0.019626     0.010410   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "2369992     0.022928     0.020691     0.156966     0.163615     0.058201   \n",
       "2369865     0.007718     0.011465     0.127894     0.124434     0.085998   \n",
       "2368917     0.011986     0.007327     0.145548     0.062648     0.021404   \n",
       "2370104     0.023965     0.006631     0.068627     0.029644     0.006536   \n",
       "2370223     0.000000     0.004267     0.123810     0.058598     0.023810   \n",
       "\n",
       "         hig_range_5          mi_1          mi_5   vola_10   hl_5  hl_10  \n",
       "2288402     0.011974  1.204943e-11  1.143334e-11  0.007177   16.0   30.0  \n",
       "2288523     0.013331  3.774399e-13  2.919821e-13  0.011041   31.0   65.0  \n",
       "2287372     0.019414  4.625973e-10  5.030325e-10  0.027868   42.0  120.0  \n",
       "2288388     0.007612  8.645482e-13  5.510298e-13  0.009210   24.0   54.0  \n",
       "2288181     0.008703  3.048248e-10  2.504696e-10  0.021658   57.0   69.0  \n",
       "...              ...           ...           ...       ...    ...    ...  \n",
       "2369992     0.045203  2.908549e-11  2.925303e-11  0.011594   89.0  123.0  \n",
       "2369865     0.062521  4.264064e-11  7.882136e-11  0.017156   66.0  159.0  \n",
       "2368917     0.008866  1.337477e-10  2.071032e-10  0.014817  360.0  550.0  \n",
       "2370104     0.007016  2.995586e-11  3.186361e-11  0.013241   50.0   87.0  \n",
       "2370223     0.009006  1.660590e-10  3.930147e-10  0.023246   76.0  215.0  \n",
       "\n",
       "[84000 rows x 34 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid[col_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2625/2625 [==============================] - 5s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "84000"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(models[i].predict(valid[col_use])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2625/2625 [==============================] - 18s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-0.00027696, -0.00022373, -0.00021226, ...,  0.00110479,\n",
       "         0.00115732,  0.00122536], dtype=float32),\n",
       " array([1, 1, 1, ..., 1, 1, 1]))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(models[0].predict(valid[col_use])[-1] * ap[0], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2625/2625 [==============================] - 14s 5ms/step\n",
      "2625/2625 [==============================] - 3s 1ms/step\n",
      "2625/2625 [==============================] - 3s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "ap = [0.10,0.10,0.80]\n",
    "model_x = list()\n",
    "for i in range(FOLDS):\n",
    "    prediction_x = models[i].predict(valid[col_use])[-1] * ap[i]\n",
    "    model_x.append(prediction_x)\n",
    "model_x = np.mean(model_x, axis = 0)\n",
    "valid['predict'] = model_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson: 0.017824402718611\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "pearson_score = stats.pearsonr(valid['predict'], valid.Target)[0]\n",
    "print('Pearson:', pearson_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = valid.sort_values([\"Date\", \"predict\"], ascending=[True, False])\n",
    "ranking = valid.groupby(\"Date\").apply(set_rank).reset_index(drop=True)\n",
    "sharp_ratio, _ = calc_spread_return_sharpe(ranking, portfolio_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.07574970389415503"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sharp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2313/2313 [==============================] - 17s 7ms/step\n",
      "2313/2313 [==============================] - 4s 2ms/step\n",
      "2313/2313 [==============================] - 4s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "ap = [0.50,0.70,0.00]\n",
    "model_x = list()\n",
    "for i in range(FOLDS):\n",
    "    prediction_x = models[i].predict(test[col_use])[-1] * ap[i]\n",
    "    model_x.append(prediction_x)\n",
    "model_x = np.mean(model_x, axis = 0)\n",
    "test['predict'] = model_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson: 0.036311828734799385\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "pearson_score = stats.pearsonr(test['predict'], test.Target)[0]\n",
    "print('Pearson:', pearson_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.sort_values([\"Date\", \"predict\"], ascending=[True, False])\n",
    "ranking = test.groupby(\"Date\").apply(set_rank).reset_index(drop=True)\n",
    "sharp_ratio, _ = calc_spread_return_sharpe(ranking, portfolio_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1479013168850063"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sharp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2022-01-04    0.944416\n",
       "2022-01-05    0.725218\n",
       "2022-01-06    1.544686\n",
       "2022-01-07   -0.058226\n",
       "2022-01-11   -1.304273\n",
       "2022-01-12   -0.395666\n",
       "2022-01-13    0.725902\n",
       "2022-01-14    2.188461\n",
       "2022-01-17   -2.537617\n",
       "2022-01-18    3.016354\n",
       "2022-01-19    0.963744\n",
       "2022-01-20   -0.678302\n",
       "2022-01-21    1.363827\n",
       "2022-01-24    0.066028\n",
       "2022-01-25   -4.680783\n",
       "2022-01-26   -1.008589\n",
       "2022-01-27    1.886614\n",
       "2022-01-28    2.388035\n",
       "2022-01-31    3.193490\n",
       "2022-02-01   -0.385363\n",
       "2022-02-02   -0.510959\n",
       "2022-02-03   -0.571943\n",
       "2022-02-04   -0.791617\n",
       "2022-02-07    0.129298\n",
       "2022-02-08    1.672243\n",
       "2022-02-09   -3.298243\n",
       "2022-02-10    0.550891\n",
       "2022-02-14   -0.029040\n",
       "2022-02-15   -2.051278\n",
       "2022-02-16    0.125233\n",
       "2022-02-17   -1.531243\n",
       "2022-02-18   -0.470706\n",
       "2022-02-21   -2.637605\n",
       "2022-02-22    7.368525\n",
       "2022-02-24    1.464058\n",
       "2022-02-25    6.983547\n",
       "2022-02-28   -1.342039\n",
       "dtype: float64"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid\n",
    "0.02019443429209232\n",
    "\n",
    "test\n",
    "0.010806184456573602"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid\n",
    "-0.21702511460140766\n",
    "\n",
    "test\n",
    "-0.1842915053496203"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12543d8aedefe43b1af9739f504be95c6574cbf3bd96045c58756e8a60ca5178"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
