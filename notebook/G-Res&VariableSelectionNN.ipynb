{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datatable as dt\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eval_results(scores, n_splits):\n",
    "    cols = 5\n",
    "    rows = int(np.ceil(n_splits/cols))\n",
    "    \n",
    "    fig, ax = plt.subplots(rows, cols, tight_layout=True, figsize=(20,2.5))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for fold in range(len(scores)):\n",
    "        df_eval = pd.DataFrame({'train_loss': scores[fold]['loss'], 'valid_loss': scores[fold]['val_loss']})\n",
    "\n",
    "        sns.lineplot(\n",
    "            x=df_eval.index,\n",
    "            y=df_eval['train_loss'],\n",
    "            label='train_loss',\n",
    "            ax=ax[fold]\n",
    "        )\n",
    "\n",
    "        sns.lineplot(\n",
    "            x=df_eval.index,\n",
    "            y=df_eval['valid_loss'],\n",
    "            label='valid_loss',\n",
    "            ax=ax[fold]\n",
    "        )\n",
    "\n",
    "        ax[fold].set_ylabel('')\n",
    "\n",
    "    sns.despine()\n",
    "\n",
    "def plot_cm(cm):\n",
    "    metrics = {\n",
    "        'accuracy': cm / cm.sum(),\n",
    "        'recall' : cm / cm.sum(axis=1),\n",
    "        'precision': cm / cm.sum(axis=0)\n",
    "    }\n",
    "    \n",
    "    fig, ax = plt.subplots(1,3, tight_layout=True, figsize=(15,5))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    mask = (np.eye(cm.shape[0]) == 0) * 1\n",
    "\n",
    "    for idx, (name, matrix) in enumerate(metrics.items()):\n",
    "\n",
    "        ax[idx].set_title(name)\n",
    "\n",
    "        sns.heatmap(\n",
    "            data=matrix,\n",
    "            cmap=sns.dark_palette(\"#69d\", reverse=True, as_cmap=True),\n",
    "            cbar=False,\n",
    "            mask=mask,\n",
    "            lw=0.25,\n",
    "            annot=True,\n",
    "            fmt='.2f',\n",
    "            ax=ax[idx]\n",
    "        )\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks\n",
    "lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", \n",
    "    factor=0.5, \n",
    "    patience=5, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "es = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_mlp_out_rmse\", \n",
    "    patience=10, \n",
    "    verbose=True, \n",
    "    mode=\"min\", \n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedLinearUnit(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(GatedLinearUnit, self).__init__()\n",
    "        self.linear = layers.Dense(units)\n",
    "        self.sigmoid = layers.Dense(units, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.linear(inputs) * self.sigmoid(inputs)\n",
    "\n",
    "class GatedResidualNetwork(layers.Layer):\n",
    "    def __init__(self, units, dropout_rate):\n",
    "        super(GatedResidualNetwork, self).__init__()\n",
    "        self.units = units\n",
    "        self.elu_dense = layers.Dense(units, activation=\"elu\")\n",
    "        self.linear_dense = layers.Dense(units)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.gated_linear_unit = GatedLinearUnit(units)\n",
    "        self.layer_norm = layers.LayerNormalization()\n",
    "        self.project = layers.Dense(units)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.elu_dense(inputs)\n",
    "        x = self.linear_dense(x)\n",
    "        x = self.dropout(x)\n",
    "        if inputs.shape[-1] != self.units:\n",
    "            inputs = self.project(inputs)\n",
    "        x = inputs + self.gated_linear_unit(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "class VariableSelection(layers.Layer):\n",
    "    def __init__(self, num_features, units, dropout_rate):\n",
    "        super(VariableSelection, self).__init__()\n",
    "        self.grns = list()\n",
    "        for idx in range(num_features):\n",
    "            grn = GatedResidualNetwork(units, dropout_rate)\n",
    "            self.grns.append(grn)\n",
    "        self.grn_concat = GatedResidualNetwork(units, dropout_rate)\n",
    "        self.activation = layers.Dense(units=num_features, activation=layers.ReLU())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        v = layers.concatenate(inputs)\n",
    "        v = self.grn_concat(v)\n",
    "        v = tf.expand_dims(self.activation(v), axis=-1)\n",
    "\n",
    "        x = []\n",
    "        for idx, input in enumerate(inputs):\n",
    "            x.append(self.grns[idx](input))\n",
    "        x = tf.stack(x, axis=1)\n",
    "\n",
    "        outputs = tf.squeeze(tf.matmul(v, x, transpose_a=True), axis=1)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in X.columns:\n",
    "        inputs[feature_name] = layers.Input(\n",
    "            name=feature_name, shape=(), dtype=tf.float64\n",
    "        )\n",
    "    return inputs\n",
    "\n",
    "def encode_inputs(inputs, encoding_size):\n",
    "    encoded_features = []\n",
    "    for col in range(inputs.shape[1]):\n",
    "        encoded_feature = tf.expand_dims(inputs[:, col], -1)\n",
    "        encoded_feature = layers.Dense(units=encoding_size)(encoded_feature)\n",
    "        encoded_features.append(encoded_feature)\n",
    "    return encoded_features\n",
    "\n",
    "def create_model(encoding_size, dropout_rate=0.15):\n",
    "    inputs = layers.Input(len(X.columns))\n",
    "    feature_list = encode_inputs(inputs, encoding_size)\n",
    "    num_features = len(feature_list)\n",
    "\n",
    "    features = VariableSelection(num_features, encoding_size, dropout_rate)(\n",
    "        feature_list\n",
    "    )\n",
    "\n",
    "    outputs = layers.Dense(units=y.shape[-1], activation=layers.ReLU())(features)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "    tf_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    print(\"Running on TPU:\", tpu.master())\n",
    "except:\n",
    "    tf_strategy = tf.distribute.get_strategy()\n",
    "    print(f\"Running on {tf_strategy.num_replicas_in_sync} replicas\")\n",
    "    print(\"Number of GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_use  = [\n",
    "#     'day',\n",
    "#     'Volume',\n",
    "    'ScaledAdjustedClose',\n",
    "    # 'close_arccos_deg',\n",
    "    'trend_psa_indicator',\n",
    "#     'trend_aroon_ind_diff1',\n",
    "    # 'volume_pct_change_ror_1',\n",
    "    'sma_5_25', 'sma_25_30',\n",
    "    'd_atr',\n",
    "    'ror_1', 'ror_5', 'ror_10',\n",
    "#     'TradedAmount_1', 'TradedAmount_5',\n",
    "    'ror1_ror2', 'ror1_ror3', 'ror1_ror4', 'ror1_ror5',\n",
    "#     'ror_1_shift1', 'ror_1_shift2', 'ror_1_shift3', 'ror_1_shift4', 'ror_1_shift5',\n",
    "#     'ror_1_shift6', 'ror_1_shift7', 'ror_1_shift8', 'ror_1_shift9',\n",
    "    'd_Amount',\n",
    "    'range_1', 'range_5',\n",
    "    'gap_range_1', 'gap_range_5',\n",
    "    'day_range_1', 'day_range_5',\n",
    "    'hig_range_1', 'hig_range_5',\n",
    "    'mi_1', 'mi_5',\n",
    "    'vola_10',\n",
    "#     'hl_5', 'hl_10',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_cols = col_use + ['Date', 'SecuritiesCode', 'Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_parquet('../input/scaling/train_scaling.parquet', columns=read_cols)\n",
    "groups = pd.factorize(pd.to_datetime(X['Date']).dt.strftime('%d').astype(str) + '_' + pd.to_datetime(X['Date']).dt.strftime('%m').astype(str) + '_' +pd.to_datetime(X['Date']).dt.strftime('%Y').astype(str))\n",
    "max_train_size = X['Date'].value_counts().max()\n",
    "y = X.Target\n",
    "X = X[col_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.polynomial.hermite as Herm\n",
    "import math\n",
    "from tensorflow.python.ops import math_ops\n",
    "from scipy import stats\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlationLoss(x,y, axis=-2):\n",
    "    \n",
    "    \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n",
    "    while trying to have the same mean and variance\"\"\"\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    y = math_ops.cast(y, x.dtype)\n",
    "    n = tf.cast(tf.shape(x)[axis], x.dtype)\n",
    "    xsum = tf.reduce_sum(x, axis=axis)\n",
    "    ysum = tf.reduce_sum(y, axis=axis)\n",
    "    xmean = xsum / n\n",
    "    ymean = ysum / n\n",
    "    xsqsum = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n",
    "    ysqsum = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n",
    "    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n",
    "    corr = cov / tf.sqrt(xsqsum * ysqsum)\n",
    "    return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr ) , dtype=tf.float64 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(x, y, axis=-2):\n",
    "    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    y = math_ops.cast(y, x.dtype)\n",
    "    n = tf.cast(tf.shape(x)[axis], x.dtype)\n",
    "    xsum = tf.reduce_sum(x, axis=axis)\n",
    "    ysum = tf.reduce_sum(y, axis=axis)\n",
    "    xmean = xsum / n\n",
    "    ymean = ysum / n\n",
    "    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n",
    "    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n",
    "    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n",
    "    corr = cov / tf.sqrt(xvar * yvar)\n",
    "    return tf.constant(1.0, dtype=x.dtype) - corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_loss(X_train,y_pred):\n",
    "    y_pred = tf.Variable(y_pred,dtype=tf.float64)\n",
    "    port_ret = tf.reduce_sum(tf.multiply(_,y_pred),axis=1)\n",
    "    s_ratio = K.mean(port_ret)/K.std(port_ret)\n",
    "    \n",
    "    return tf.math.exp(-s_ratio,  name='sharpe_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2021\n",
    "set_seeds(seed)\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=5, max_train_size=max_train_size, gap=14)\n",
    "\n",
    "predictions = []\n",
    "oof_preds = {'y_valid': list(), 'y_hat': list()}\n",
    "scores_nn = {fold:None for fold in range(cv.n_splits)}\n",
    "\n",
    "for fold, (idx_train, idx_valid) in enumerate(cv.split(X,y)):\n",
    "    X_train, y_train = X.iloc[idx_train], y[idx_train]\n",
    "    X_valid, y_valid = X.iloc[idx_valid], y[idx_valid]\n",
    "    \n",
    "    scl = RobustScaler()\n",
    "    X_train = scl.fit_transform(X_train)\n",
    "    X_valid = scl.transform(X_valid)\n",
    "    \n",
    "    with tf_strategy.scope():\n",
    "        model = create_model(encoding_size=128)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "            loss=[\n",
    "                tf.keras.losses.CosineSimilarity(axis=-2), \n",
    "                tf.keras.losses.MeanSquaredError(), \n",
    "                correlationLoss,\n",
    "                sharpe_loss\n",
    "            ],\n",
    "            metrics=[\n",
    "                tf.keras.losses.CosineSimilarity(axis=-2), \n",
    "                tf.keras.losses.MeanSquaredError(), \n",
    "                correlationLoss,\n",
    "                sharpe_loss\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        epochs=90,\n",
    "        batch_size=4096,\n",
    "        verbose=False,\n",
    "        callbacks=[lr,es]\n",
    "    )\n",
    "    \n",
    "    scores_nn[fold] = history.history\n",
    "    \n",
    "    oof_preds['y_valid'].extend(y.iloc[idx_valid])\n",
    "    oof_preds['y_hat'].extend(model.predict(X_valid, batch_size=4096))\n",
    "    \n",
    "    prediction = model.predict(scl.transform(X_test), batch_size=4096) \n",
    "    predictions.append(prediction)\n",
    "    \n",
    "    del model, prediction\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    \n",
    "    print('_'*65)\n",
    "    print(f\"Fold {fold+1} || Min Val Loss: {np.min(scores_nn[fold]['val_loss'])}\")\n",
    "    print('_'*65)\n",
    "    \n",
    "print('_'*65)\n",
    "overall_score = [np.min(scores_nn[fold]['val_loss']) for fold in range(cv.n_splits)]\n",
    "print(f\"Overall Mean Validation Loss: {np.mean(overall_score)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
