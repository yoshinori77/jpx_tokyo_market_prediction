{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, os\n",
    "# import cudf\n",
    "# import talib as ta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import jpx_tokyo_market_prediction\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.polynomial.hermite as Herm\n",
    "import math\n",
    "from tensorflow.python.ops import math_ops\n",
    "from scipy import stats\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "import random\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title GroupTimeSeriesSplit { display-mode: \"form\" }\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class GroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_size : int, default=None\n",
    "        Maximum size for a single training set.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.model_selection import GroupTimeSeriesSplit\n",
    "    >>> groups = np.array(['a', 'a', 'a', 'a', 'a', 'a',\\\n",
    "                           'b', 'b', 'b', 'b', 'b',\\\n",
    "                           'c', 'c', 'c', 'c',\\\n",
    "                           'd', 'd', 'd'])\n",
    "    >>> gtss = GroupTimeSeriesSplit(n_splits=3)\n",
    "    >>> for train_idx, test_idx in gtss.split(groups, groups=groups):\n",
    "    ...     print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
    "    ...     print(\"TRAIN GROUP:\", groups[train_idx],\\\n",
    "                  \"TEST GROUP:\", groups[test_idx])\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5] TEST: [6, 7, 8, 9, 10]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a']\\\n",
    "    TEST GROUP: ['b' 'b' 'b' 'b' 'b']\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] TEST: [11, 12, 13, 14]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b']\\\n",
    "    TEST GROUP: ['c' 'c' 'c' 'c']\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\\\n",
    "    TEST: [15, 16, 17]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b' 'c' 'c' 'c' 'c']\\\n",
    "    TEST GROUP: ['d' 'd' 'd']\n",
    "    \"\"\"\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_size=None\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_size = max_train_size\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "        group_test_size = n_groups // n_folds\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "            for train_group_idx in unique_groups[:group_test_start]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "            train_end = train_array.size\n",
    "            if self.max_train_size and self.max_train_size < train_end:\n",
    "                train_array = train_array[train_end -\n",
    "                                          self.max_train_size:train_end]\n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "\n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    "\n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "            \n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "    jet     = plt.cm.get_cmap('jet', 256)\n",
    "    seq     = np.linspace(0, 1, 256)\n",
    "    _       = np.random.shuffle(seq)   # inplace\n",
    "    cmap_data = ListedColormap(jet(seq))    \n",
    "    for ii, (tr, tt) in enumerate(list(cv.split(X=X, y=y, groups=group))):\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0        \n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices), c=indices, marker='_', lw=lw, cmap=cmap_cv, vmin=-.2, vmax=1.2)\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X), c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n",
    "    ax.scatter(range(len(X)), [ii + 2.5] * len(X), c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "    yticklabels = list(range(n_splits)) + ['target', 'day']\n",
    "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels, xlabel='Sample index', ylabel=\"CV iteration\", ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"CPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"TPU\":\n",
    "    print(\"connecting to TPU...\")\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        tpu = None\n",
    "    if tpu:\n",
    "        try:\n",
    "            print(\"initializing  TPU ...\")\n",
    "            tf.config.experimental_connect_to_cluster(tpu)\n",
    "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "            strategy = tf.distribute.TPUStrategy(tpu)\n",
    "            print(\"TPU initialized\")\n",
    "        except: print(\"failed to initialize TPU\")\n",
    "    else: device = \"GPU\"\n",
    "\n",
    "if device != \"TPU\": strategy = tf.distribute.get_strategy()\n",
    "if device == \"GPU\": print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "AUTO     = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2025\n",
    "set_all_seeds(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_use  = [\n",
    "    # 'day', 'Volume',\n",
    "    'ScaledAdjustedClose',\n",
    "    # 'close_arccos_deg',\n",
    "    'trend_psa_indicator',\n",
    "    'trend_aroon_ind_diff1',\n",
    "    # 'volume_pct_change_ror_1',\n",
    "    'sma_5_25', 'sma_25_30',\n",
    "    'd_atr',\n",
    "    'ror_1', 'ror_5', 'ror_10',\n",
    "    'TradedAmount_1', 'TradedAmount_5',\n",
    "    # 'ror1_ror2', 'ror1_ror3', 'ror1_ror4', 'ror1_ror5',\n",
    "    'ror_1_shift1', 'ror_1_shift2', 'ror_1_shift3', 'ror_1_shift4', 'ror_1_shift5',\n",
    "    'ror_1_shift6', 'ror_1_shift7', 'ror_1_shift8', 'ror_1_shift9',\n",
    "    'd_Amount',\n",
    "    'range_1', 'range_5',\n",
    "    'gap_range_1', 'gap_range_5',\n",
    "    'day_range_1', 'day_range_5',\n",
    "    'hig_range_1', 'hig_range_5',\n",
    "    'mi_1', 'mi_5',\n",
    "    'vola_10',\n",
    "    'hl_5', 'hl_10',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_use = [\n",
    "    'day', 'Volume',\n",
    "    'ScaledAdjustedClose',\n",
    "    'sma_5_25', 'sma_25_30',\n",
    "    'trend_psa_indicator',\n",
    "    'trend_aroon_ind_diff1',\n",
    "    'volume_pct_change_ror_1',\n",
    "    'ror_1', 'ror_5', 'ror_10', 'ror_20', 'ror_40',\n",
    "    'TradedAmount_1', 'TradedAmount_5', 'TradedAmount_10',\n",
    "    'TradedAmount_20', 'TradedAmount_40',\n",
    "    'd_Amount', 'd_atr',\n",
    "    'range_1', 'range_5', 'range_10', 'range_20', 'range_40',\n",
    "    'gap_range_1', 'gap_range_5', 'gap_range_10', 'gap_range_20', 'gap_range_40',\n",
    "    'day_range_1', 'day_range_5', 'day_range_10', 'day_range_20', 'day_range_40',\n",
    "    'hig_range_1', 'hig_range_5', 'hig_range_10', 'hig_range_20', 'hig_range_40',\n",
    "    'mi_1', 'mi_5', 'mi_10', 'mi_20', 'mi_40',\n",
    "    'vola_5', 'vola_10', 'vola_20', 'vola_40',\n",
    "    'hl_5', 'hl_10', 'hl_20', 'hl_40',\n",
    "    'ror_1_shift1', 'ror_1_shift2', 'ror_1_shift3', 'ror_1_shift4', 'ror_1_shift5',\n",
    "    'ror_1_shift6', 'ror_1_shift7', 'ror_1_shift8', 'ror_1_shift9'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_use = [\n",
    "#     'ror_1', 'ror_5', 'ror_10', 'ror_20', 'ror_40', 'TradedAmount_1', 'TradedAmount_5', 'TradedAmount_10',\n",
    "#     'TradedAmount_20', 'TradedAmount_40', 'd_Amount', 'range_1', 'range_5',\n",
    "#     'range_10', 'range_20', 'range_40', 'd_atr', 'gap_range', 'gap_range_1', 'gap_range_5', 'gap_range_10', 'gap_range_20',\n",
    "#     'gap_range_40', 'day_range_1', 'day_range_5', 'day_range_10', 'day_range_20', 'day_range_40',\n",
    "#     'hig_range_1', 'hig_range_5', 'hig_range_10', 'hig_range_20', 'hig_range_40',\n",
    "#     'mi_1', 'mi_5', 'mi_10', 'mi_20', 'mi_40', 'vola_5', 'vola_10', 'vola_20', 'vola_40',\n",
    "#     'hl_5', 'hl_10', 'hl_20', 'hl_40', 'ror_1_shift1', 'ror_1_shift2', 'ror_1_shift3', 'ror_1_shift4', 'ror_1_shift5',\n",
    "#     'ror_1_shift6', 'ror_1_shift7', 'ror_1_shift8', 'ror_1_shift9'\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_cols = col_use + ['Date', 'Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_parquet('../Output/train_scaling.parquet', columns=read_cols)\n",
    "groups = pd.factorize(pd.to_datetime(X['Date']).dt.strftime('%d').astype(str) + '_' + pd.to_datetime(X['Date']).dt.strftime('%m').astype(str) + '_' +pd.to_datetime(X['Date']).dt.strftime('%Y').astype(str))\n",
    "y = X.Target\n",
    "# X = X.drop(['RowId','Target','AdjustmentFactor','ExpectedDividend','SupervisionFlag','Date'],axis=1)\n",
    "X = X[col_use]\n",
    "# valid = pd.read_parquet('../input/scaling/valid_scaling.parquet')\n",
    "# test = pd.read_parquet('../input/scaling/test_scaling.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['day',\n",
       " 'Volume',\n",
       " 'ScaledAdjustedClose',\n",
       " 'sma_5_25',\n",
       " 'sma_25_30',\n",
       " 'trend_psa_indicator',\n",
       " 'trend_aroon_ind_diff1',\n",
       " 'volume_pct_change_ror_1',\n",
       " 'ror_1',\n",
       " 'ror_5',\n",
       " 'ror_10',\n",
       " 'ror_20',\n",
       " 'ror_40',\n",
       " 'TradedAmount_1',\n",
       " 'TradedAmount_5',\n",
       " 'TradedAmount_10',\n",
       " 'TradedAmount_20',\n",
       " 'TradedAmount_40',\n",
       " 'd_Amount',\n",
       " 'd_atr',\n",
       " 'range_1',\n",
       " 'range_5',\n",
       " 'range_10',\n",
       " 'range_20',\n",
       " 'range_40',\n",
       " 'gap_range_1',\n",
       " 'gap_range_5',\n",
       " 'gap_range_10',\n",
       " 'gap_range_20',\n",
       " 'gap_range_40',\n",
       " 'day_range_1',\n",
       " 'day_range_5',\n",
       " 'day_range_10',\n",
       " 'day_range_20',\n",
       " 'day_range_40',\n",
       " 'hig_range_1',\n",
       " 'hig_range_5',\n",
       " 'hig_range_10',\n",
       " 'hig_range_20',\n",
       " 'hig_range_40',\n",
       " 'mi_1',\n",
       " 'mi_5',\n",
       " 'mi_10',\n",
       " 'mi_20',\n",
       " 'mi_40',\n",
       " 'vola_5',\n",
       " 'vola_10',\n",
       " 'vola_20',\n",
       " 'vola_40',\n",
       " 'hl_5',\n",
       " 'hl_10',\n",
       " 'hl_20',\n",
       " 'hl_40',\n",
       " 'ror_1_shift1',\n",
       " 'ror_1_shift2',\n",
       " 'ror_1_shift3',\n",
       " 'ror_1_shift4',\n",
       " 'ror_1_shift5',\n",
       " 'ror_1_shift6',\n",
       " 'ror_1_shift7',\n",
       " 'ror_1_shift8',\n",
       " 'ror_1_shift9']"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c for c in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV PARAMS\n",
    "FOLDS                = 3\n",
    "GROUP_GAP            = 14\n",
    "MAX_TEST_GROUP_SIZE  = 180  \n",
    "MAX_TRAIN_GROUP_SIZE = 485\n",
    "\n",
    "# USE VERBOSE=0 for silent, VERBOSE=1 for interactive, VERBOSE=2 for commit\n",
    "VERBOSE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.colors import ListedColormap\n",
    "# fig, ax = plt.subplots(figsize = (12, 6))\n",
    "# cv = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap = GROUP_GAP, max_train_group_size=MAX_TRAIN_GROUP_SIZE, max_test_group_size=MAX_TEST_GROUP_SIZE)\n",
    "# plot_cv_indices(cv, X, y, groups[0], ax, FOLDS, lw=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2286531, 62), (2286531,))"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import backend as K\n",
    "def e_swish(beta=0.25):\n",
    "    def beta_swish(x): return x*K.sigmoid(x)*(1+beta)\n",
    "    return beta_swish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlationLoss(x,y, axis=-2):\n",
    "    \n",
    "    \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n",
    "    while trying to have the same mean and variance\"\"\"\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    y = math_ops.cast(y, x.dtype)\n",
    "    n = tf.cast(tf.shape(x)[axis], x.dtype)\n",
    "    xsum = tf.reduce_sum(x, axis=axis)\n",
    "    ysum = tf.reduce_sum(y, axis=axis)\n",
    "    xmean = xsum / n\n",
    "    ymean = ysum / n\n",
    "    xsqsum = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n",
    "    ysqsum = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n",
    "    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n",
    "    corr = cov / tf.sqrt(xsqsum * ysqsum)\n",
    "    return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr ) , dtype=tf.float32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(x, y, axis=-2):\n",
    "    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    y = math_ops.cast(y, x.dtype)\n",
    "    n = tf.cast(tf.shape(x)[axis], x.dtype)\n",
    "    xsum = tf.reduce_sum(x, axis=axis)\n",
    "    ysum = tf.reduce_sum(y, axis=axis)\n",
    "    xmean = xsum / n\n",
    "    ymean = ysum / n\n",
    "    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n",
    "    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n",
    "    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n",
    "    corr = cov / tf.sqrt(xvar * yvar)\n",
    "    return tf.constant(1.0, dtype=x.dtype) - corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_loss(X_train,y_pred):\n",
    "    y_pred = tf.Variable(y_pred,dtype=tf.float64)\n",
    "    port_ret = tf.reduce_sum(tf.multiply(_,y_pred),axis=1)\n",
    "    s_ratio = K.mean(port_ret)/K.std(port_ret)\n",
    "    \n",
    "    return tf.math.exp(-s_ratio,  name='sharpe_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp, dim = 128, fold=0):\n",
    "\n",
    "    features_inputs = tf.keras.layers.Input(shape = (dim, ))\n",
    "    x0      =  tf.keras.layers.BatchNormalization()(features_inputs)\n",
    "    \n",
    "    weight = tf.Variable(tf.keras.backend.random_normal((dim, 1), stddev=hp.Float(f'weight_{fold}',1e-10, 0.09), dtype=tf.float32))\n",
    "    var    = tf.Variable(tf.zeros((1,1), dtype=tf.float32))\n",
    "   \n",
    "    encoder = tf.keras.layers.GaussianNoise(0.4)(x0)\n",
    "    encoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_en0',32, 1024))(encoder)\n",
    "    encoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_en1',32, 1024))(encoder)\n",
    "    encoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_en2',32, 1024))(encoder)\n",
    "    encoder = tf.keras.layers.BatchNormalization()(encoder)\n",
    "    encoder = tf.keras.layers.Activation(e_swish(beta=hp.Float(f'e{fold}_en0',0.001, 1 )))(encoder)\n",
    "    \n",
    "    decoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_de0',32, 1024), name='decoder')(encoder)\n",
    "#     decoder = tf.keras.layers.Dropout(hp.Float(f'dropout{fold}_de0',0.001, 0.8))(encoder)\n",
    "#     decoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_de0',32, 1024), name='decoder')(decoder)\n",
    "    \n",
    "    x_ae = tf.keras.layers.Dense(hp.Int(f'layers{fold}_ae0',32, 1024))(decoder)\n",
    "    x_ae = tf.keras.layers.BatchNormalization()(x_ae)\n",
    "    x_ae = tf.keras.layers.Activation(e_swish(beta=hp.Float(f'e{fold}_ae0',0.001, 1 )))(x_ae)\n",
    "#     x_ae = tf.keras.layers.Dropout(hp.Float(f'dropout{fold}_ae0',0.001, 0.8))(x_ae) \n",
    "    \n",
    "    feature_x = tf.keras.layers.Concatenate()([x0, encoder])\n",
    "    feature_x = tf.keras.layers.BatchNormalization()(feature_x)\n",
    "    feature_x = tf.keras.layers.Dense(hp.Int(f'layers{fold}_fx0',32, 1024))(feature_x)\n",
    "    feature_x = tf.keras.layers.Activation(e_swish(beta=hp.Float(f'e_fx0',0.001, 1 )))(feature_x)\n",
    "#     feature_x = tf.keras.layers.Dropout(hp.Float(f'dropout{fold}_fx0',0.001, 0.8))(feature_x)\n",
    "\n",
    "    x = layers.Dense(hp.Int(f'layers{fold}_x0',32, 1024), activation= e_swish(beta=hp.Float(f'e{fold}_x0',0.001, 1 )), kernel_regularizer=\"l2\")(feature_x)\n",
    "    x = layers.Dense(hp.Int(f'layers{fold}_x1',32, 1024), activation= e_swish(beta=hp.Float(f'e{fold}_x1',0.001, 1 )), kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dense(hp.Int(f'layers{fold}_x2',32, 1024), activation= e_swish(beta=hp.Float(f'e{fold}_x2',0.001, 1 )), kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dense(hp.Int(f'layers{fold}_x3',32, 1024), activation= e_swish(beta=hp.Float(f'e{fold}_x3',0.001, 1 )), kernel_regularizer=\"l2\")(x)\n",
    "#     x = tf.keras.layers.Dropout(hp.Float(f'dropout{fold}_x0',0.001, 0.8))(x)\n",
    "\n",
    "    mlp_out = layers.Dense(1, name ='mlp_out')(x)\n",
    "\n",
    "    model  = tf.keras.Model(inputs=[features_inputs], outputs=[decoder, mlp_out])\n",
    "    \n",
    "    loss_out = tf.add(tf.matmul(features_inputs,weight), tf.math.reduce_sum(weight*var))\n",
    "    tf.compat.v1.losses.add_loss(loss_out)\n",
    "  \n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Float(f'lr_adam{fold}',1e-3, 1e-5)),\n",
    "                  loss = {'decoder': [tf.keras.losses.CosineSimilarity(axis=-2), \n",
    "                                      tf.keras.losses.MeanSquaredError(), \n",
    "                                      correlationLoss],         \n",
    "                          \n",
    "                          'mlp_out' : [sharpe_loss],\n",
    "                         },\n",
    "                  metrics = {'decoder': [tf.keras.metrics.CosineSimilarity(name='cosine'),\n",
    "                                         tf.keras.metrics.MeanAbsoluteError(name=\"mae\"), \n",
    "                                         correlation, \n",
    "                                         tf.keras.metrics.RootMeanSquaredError(name='rmse')], \n",
    "                             \n",
    "                             'mlp_out' : [tf.keras.metrics.CosineSimilarity(name='cosine'),\n",
    "                                          tf.keras.metrics.MeanAbsoluteError(name=\"mae\"), \n",
    "                                          correlation, \n",
    "                                          tf.keras.metrics.RootMeanSquaredError(name='rmse')],\n",
    "                            },\n",
    "                 ) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = pd.read_pickle(f'../Output/hp-jpx-aemlp/best_hp_ae_jpx_3gkf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_80), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(128, 1) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(build_model(hp, fold=0), show_shapes=True, expand_nested=True, show_dtype=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [4096*4,4096,4096*8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> AEMLP_FOLD:0\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_81), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(62, 1) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "Epoch 1/200\n",
      "57/57 - 44s - loss: 9.5085 - decoder_loss: -1.0532e-02 - mlp_out_loss: 0.3912 - decoder_cosine: 0.0365 - decoder_mae: 0.3669 - decoder_correlation: 0.9894 - decoder_rmse: 0.6752 - mlp_out_cosine: 0.0021 - mlp_out_mae: 0.2248 - mlp_out_correlation: 1.0018 - mlp_out_rmse: 0.6255 - val_loss: 7.2487 - val_decoder_loss: -4.5662e-03 - val_mlp_out_loss: 0.0028 - val_decoder_cosine: -3.5205e-02 - val_decoder_mae: 0.3828 - val_decoder_correlation: 1.0038 - val_decoder_rmse: 0.9502 - val_mlp_out_cosine: -2.5734e-02 - val_mlp_out_mae: 0.0350 - val_mlp_out_correlation: 1.0091 - val_mlp_out_rmse: 0.0531 - 44s/epoch - 773ms/step\n",
      "Epoch 2/200\n",
      "57/57 - 43s - loss: 5.8165 - decoder_loss: -4.6635e-02 - mlp_out_loss: 0.0022 - decoder_cosine: 0.1289 - decoder_mae: 0.4146 - decoder_correlation: 0.9530 - decoder_rmse: 0.6761 - mlp_out_cosine: 0.0061 - mlp_out_mae: 0.0325 - mlp_out_correlation: 0.9926 - mlp_out_rmse: 0.0471 - val_loss: 4.6489 - val_decoder_loss: -1.2990e-02 - val_mlp_out_loss: 0.0015 - val_decoder_cosine: -5.3381e-02 - val_decoder_mae: 0.4639 - val_decoder_correlation: 0.9968 - val_decoder_rmse: 0.6786 - val_mlp_out_cosine: -3.3780e-04 - val_mlp_out_mae: 0.0271 - val_mlp_out_correlation: 1.0134 - val_mlp_out_rmse: 0.0390 - 43s/epoch - 761ms/step\n",
      "Epoch 3/200\n",
      "57/57 - 46s - loss: 3.8013 - decoder_loss: -7.9639e-02 - mlp_out_loss: 0.0011 - decoder_cosine: 0.2020 - decoder_mae: 0.5350 - decoder_correlation: 0.9201 - decoder_rmse: 0.7489 - mlp_out_cosine: 0.0049 - mlp_out_mae: 0.0232 - mlp_out_correlation: 0.9871 - mlp_out_rmse: 0.0338 - val_loss: 3.1744 - val_decoder_loss: -2.0003e-02 - val_mlp_out_loss: 9.9545e-04 - val_decoder_cosine: -2.2580e-02 - val_decoder_mae: 0.5507 - val_decoder_correlation: 0.9845 - val_decoder_rmse: 0.7151 - val_mlp_out_cosine: -4.2488e-03 - val_mlp_out_mae: 0.0220 - val_mlp_out_correlation: 1.0104 - val_mlp_out_rmse: 0.0316 - 46s/epoch - 801ms/step\n",
      "Epoch 4/200\n",
      "57/57 - 42s - loss: 2.6340 - decoder_loss: -9.1585e-02 - mlp_out_loss: 8.2711e-04 - decoder_cosine: 0.2174 - decoder_mae: 0.5784 - decoder_correlation: 0.9084 - decoder_rmse: 0.7887 - mlp_out_cosine: 0.0071 - mlp_out_mae: 0.0195 - mlp_out_correlation: 0.9844 - mlp_out_rmse: 0.0288 - val_loss: 2.2778 - val_decoder_loss: -2.5507e-02 - val_mlp_out_loss: 8.6600e-04 - val_decoder_cosine: -8.1102e-04 - val_decoder_mae: 0.5144 - val_decoder_correlation: 0.9736 - val_decoder_rmse: 0.6686 - val_mlp_out_cosine: -2.3488e-02 - val_mlp_out_mae: 0.0197 - val_mlp_out_correlation: 1.0056 - val_mlp_out_rmse: 0.0294 - 42s/epoch - 739ms/step\n",
      "Epoch 5/200\n",
      "57/57 - 43s - loss: 1.9050 - decoder_loss: -9.6423e-02 - mlp_out_loss: 7.0659e-04 - decoder_cosine: 0.2262 - decoder_mae: 0.5521 - decoder_correlation: 0.9038 - decoder_rmse: 0.7722 - mlp_out_cosine: 0.0038 - mlp_out_mae: 0.0179 - mlp_out_correlation: 0.9851 - mlp_out_rmse: 0.0266 - val_loss: 1.6944 - val_decoder_loss: -2.9712e-02 - val_mlp_out_loss: 8.3336e-04 - val_decoder_cosine: 0.0230 - val_decoder_mae: 0.4686 - val_decoder_correlation: 0.9703 - val_decoder_rmse: 0.6145 - val_mlp_out_cosine: -2.6124e-02 - val_mlp_out_mae: 0.0192 - val_mlp_out_correlation: 1.0024 - val_mlp_out_rmse: 0.0289 - 43s/epoch - 750ms/step\n",
      "Epoch 6/200\n",
      "57/57 - 46s - loss: 1.4170 - decoder_loss: -1.0164e-01 - mlp_out_loss: 6.5043e-04 - decoder_cosine: 0.2304 - decoder_mae: 0.5044 - decoder_correlation: 0.8986 - decoder_rmse: 0.7120 - mlp_out_cosine: 9.0134e-04 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9893 - mlp_out_rmse: 0.0255 - val_loss: 1.2980 - val_decoder_loss: -2.9066e-02 - val_mlp_out_loss: 8.2256e-04 - val_decoder_cosine: 0.0202 - val_decoder_mae: 0.4170 - val_decoder_correlation: 0.9659 - val_decoder_rmse: 0.5562 - val_mlp_out_cosine: -1.2211e-02 - val_mlp_out_mae: 0.0190 - val_mlp_out_correlation: 1.0045 - val_mlp_out_rmse: 0.0287 - 46s/epoch - 805ms/step\n",
      "Epoch 7/200\n",
      "57/57 - 44s - loss: 1.0753 - decoder_loss: -1.0577e-01 - mlp_out_loss: 6.1942e-04 - decoder_cosine: 0.2342 - decoder_mae: 0.4430 - decoder_correlation: 0.8943 - decoder_rmse: 0.6381 - mlp_out_cosine: -1.9624e-04 - mlp_out_mae: 0.0166 - mlp_out_correlation: 0.9956 - mlp_out_rmse: 0.0249 - val_loss: 1.0130 - val_decoder_loss: -3.0443e-02 - val_mlp_out_loss: 8.0813e-04 - val_decoder_cosine: 0.0291 - val_decoder_mae: 0.3595 - val_decoder_correlation: 0.9612 - val_decoder_rmse: 0.4961 - val_mlp_out_cosine: -6.3775e-03 - val_mlp_out_mae: 0.0189 - val_mlp_out_correlation: 1.0057 - val_mlp_out_rmse: 0.0284 - 44s/epoch - 779ms/step\n",
      "Epoch 8/200\n",
      "57/57 - 43s - loss: 0.8248 - decoder_loss: -1.1130e-01 - mlp_out_loss: 5.9773e-04 - decoder_cosine: 0.2431 - decoder_mae: 0.3836 - decoder_correlation: 0.8889 - decoder_rmse: 0.5606 - mlp_out_cosine: -1.9133e-03 - mlp_out_mae: 0.0162 - mlp_out_correlation: 1.0026 - mlp_out_rmse: 0.0244 - val_loss: 0.8063 - val_decoder_loss: -2.7929e-02 - val_mlp_out_loss: 7.9806e-04 - val_decoder_cosine: 0.0348 - val_decoder_mae: 0.3042 - val_decoder_correlation: 0.9632 - val_decoder_rmse: 0.4281 - val_mlp_out_cosine: -3.4153e-03 - val_mlp_out_mae: 0.0187 - val_mlp_out_correlation: 1.0082 - val_mlp_out_rmse: 0.0282 - 43s/epoch - 757ms/step\n",
      "Epoch 9/200\n",
      "57/57 - 43s - loss: 0.6373 - decoder_loss: -1.1600e-01 - mlp_out_loss: 5.7869e-04 - decoder_cosine: 0.2478 - decoder_mae: 0.3386 - decoder_correlation: 0.8841 - decoder_rmse: 0.4981 - mlp_out_cosine: -2.7800e-03 - mlp_out_mae: 0.0160 - mlp_out_correlation: 1.0052 - mlp_out_rmse: 0.0241 - val_loss: 0.6483 - val_decoder_loss: -2.7673e-02 - val_mlp_out_loss: 7.9412e-04 - val_decoder_cosine: 0.0333 - val_decoder_mae: 0.2905 - val_decoder_correlation: 0.9619 - val_decoder_rmse: 0.4129 - val_mlp_out_cosine: -3.0811e-03 - val_mlp_out_mae: 0.0187 - val_mlp_out_correlation: 1.0081 - val_mlp_out_rmse: 0.0282 - 43s/epoch - 754ms/step\n",
      "Epoch 10/200\n",
      "57/57 - 43s - loss: 0.4915 - decoder_loss: -1.2214e-01 - mlp_out_loss: 5.6428e-04 - decoder_cosine: 0.2657 - decoder_mae: 0.2985 - decoder_correlation: 0.8780 - decoder_rmse: 0.4396 - mlp_out_cosine: -3.3786e-03 - mlp_out_mae: 0.0158 - mlp_out_correlation: 1.0080 - mlp_out_rmse: 0.0238 - val_loss: 0.5284 - val_decoder_loss: -2.5394e-02 - val_mlp_out_loss: 7.9214e-04 - val_decoder_cosine: -1.9176e-02 - val_decoder_mae: 0.2679 - val_decoder_correlation: 0.9653 - val_decoder_rmse: 0.4142 - val_mlp_out_cosine: -1.4085e-03 - val_mlp_out_mae: 0.0186 - val_mlp_out_correlation: 1.0065 - val_mlp_out_rmse: 0.0281 - 43s/epoch - 753ms/step\n",
      "Epoch 11/200\n",
      "57/57 - 43s - loss: 0.3786 - decoder_loss: -1.2626e-01 - mlp_out_loss: 5.5384e-04 - decoder_cosine: 0.2799 - decoder_mae: 0.2732 - decoder_correlation: 0.8741 - decoder_rmse: 0.4052 - mlp_out_cosine: -2.6300e-03 - mlp_out_mae: 0.0156 - mlp_out_correlation: 1.0066 - mlp_out_rmse: 0.0235 - val_loss: 0.4218 - val_decoder_loss: -3.6048e-02 - val_mlp_out_loss: 7.9262e-04 - val_decoder_cosine: 0.0354 - val_decoder_mae: 0.2696 - val_decoder_correlation: 0.9590 - val_decoder_rmse: 0.4165 - val_mlp_out_cosine: -2.9983e-03 - val_mlp_out_mae: 0.0186 - val_mlp_out_correlation: 1.0036 - val_mlp_out_rmse: 0.0282 - 43s/epoch - 756ms/step\n",
      "Epoch 12/200\n",
      "57/57 - 43s - loss: 0.2885 - decoder_loss: -1.3046e-01 - mlp_out_loss: 5.4661e-04 - decoder_cosine: 0.2866 - decoder_mae: 0.2632 - decoder_correlation: 0.8698 - decoder_rmse: 0.3878 - mlp_out_cosine: -1.1909e-03 - mlp_out_mae: 0.0155 - mlp_out_correlation: 1.0056 - mlp_out_rmse: 0.0234 - val_loss: 0.3579 - val_decoder_loss: -2.3566e-02 - val_mlp_out_loss: 7.9032e-04 - val_decoder_cosine: -2.0163e-02 - val_decoder_mae: 0.2707 - val_decoder_correlation: 0.9743 - val_decoder_rmse: 0.4310 - val_mlp_out_cosine: 1.9994e-04 - val_mlp_out_mae: 0.0185 - val_mlp_out_correlation: 1.0015 - val_mlp_out_rmse: 0.0281 - 43s/epoch - 750ms/step\n",
      "Epoch 13/200\n",
      "57/57 - 43s - loss: 0.2161 - decoder_loss: -1.3408e-01 - mlp_out_loss: 5.4401e-04 - decoder_cosine: 0.3003 - decoder_mae: 0.2552 - decoder_correlation: 0.8661 - decoder_rmse: 0.3757 - mlp_out_cosine: -2.2557e-03 - mlp_out_mae: 0.0154 - mlp_out_correlation: 1.0064 - mlp_out_rmse: 0.0233 - val_loss: 0.2956 - val_decoder_loss: -2.4418e-02 - val_mlp_out_loss: 7.8982e-04 - val_decoder_cosine: -1.3006e-02 - val_decoder_mae: 0.2966 - val_decoder_correlation: 0.9769 - val_decoder_rmse: 0.4602 - val_mlp_out_cosine: 4.0920e-04 - val_mlp_out_mae: 0.0185 - val_mlp_out_correlation: 1.0024 - val_mlp_out_rmse: 0.0281 - 43s/epoch - 751ms/step\n",
      "Epoch 14/200\n",
      "57/57 - 43s - loss: 0.1567 - decoder_loss: -1.3785e-01 - mlp_out_loss: 5.3732e-04 - decoder_cosine: 0.3045 - decoder_mae: 0.2452 - decoder_correlation: 0.8622 - decoder_rmse: 0.3614 - mlp_out_cosine: 1.2782e-04 - mlp_out_mae: 0.0153 - mlp_out_correlation: 1.0058 - mlp_out_rmse: 0.0232 - val_loss: 0.2381 - val_decoder_loss: -3.1926e-02 - val_mlp_out_loss: 7.8670e-04 - val_decoder_cosine: 0.0426 - val_decoder_mae: 0.2670 - val_decoder_correlation: 0.9745 - val_decoder_rmse: 0.4129 - val_mlp_out_cosine: 0.0029 - val_mlp_out_mae: 0.0185 - val_mlp_out_correlation: 1.0017 - val_mlp_out_rmse: 0.0280 - 43s/epoch - 761ms/step\n",
      "Epoch 15/200\n",
      "57/57 - 43s - loss: 0.1092 - decoder_loss: -1.3981e-01 - mlp_out_loss: 5.3224e-04 - decoder_cosine: 0.3059 - decoder_mae: 0.2375 - decoder_correlation: 0.8602 - decoder_rmse: 0.3504 - mlp_out_cosine: -9.4127e-06 - mlp_out_mae: 0.0152 - mlp_out_correlation: 1.0053 - mlp_out_rmse: 0.0231 - val_loss: 0.2012 - val_decoder_loss: -2.7766e-02 - val_mlp_out_loss: 7.8342e-04 - val_decoder_cosine: -7.7137e-03 - val_decoder_mae: 0.2739 - val_decoder_correlation: 0.9699 - val_decoder_rmse: 0.4454 - val_mlp_out_cosine: 0.0049 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0009 - val_mlp_out_rmse: 0.0280 - 43s/epoch - 759ms/step\n",
      "Epoch 16/200\n",
      "57/57 - 43s - loss: 0.0695 - decoder_loss: -1.4208e-01 - mlp_out_loss: 5.2772e-04 - decoder_cosine: 0.3164 - decoder_mae: 0.2299 - decoder_correlation: 0.8578 - decoder_rmse: 0.3393 - mlp_out_cosine: 2.0151e-04 - mlp_out_mae: 0.0151 - mlp_out_correlation: 1.0040 - mlp_out_rmse: 0.0230 - val_loss: 0.1638 - val_decoder_loss: -3.1157e-02 - val_mlp_out_loss: 7.8139e-04 - val_decoder_cosine: -7.7189e-03 - val_decoder_mae: 0.2778 - val_decoder_correlation: 0.9733 - val_decoder_rmse: 0.4563 - val_mlp_out_cosine: 0.0059 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0024 - val_mlp_out_rmse: 0.0280 - 43s/epoch - 754ms/step\n",
      "Epoch 17/200\n",
      "57/57 - 43s - loss: 0.0386 - decoder_loss: -1.4189e-01 - mlp_out_loss: 5.2576e-04 - decoder_cosine: 0.3163 - decoder_mae: 0.2259 - decoder_correlation: 0.8582 - decoder_rmse: 0.3330 - mlp_out_cosine: 0.0018 - mlp_out_mae: 0.0151 - mlp_out_correlation: 1.0029 - mlp_out_rmse: 0.0229 - val_loss: 0.1360 - val_decoder_loss: -3.0668e-02 - val_mlp_out_loss: 7.7968e-04 - val_decoder_cosine: 0.0106 - val_decoder_mae: 0.2762 - val_decoder_correlation: 0.9769 - val_decoder_rmse: 0.4406 - val_mlp_out_cosine: 0.0052 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0016 - val_mlp_out_rmse: 0.0279 - 43s/epoch - 756ms/step\n",
      "Epoch 18/200\n",
      "57/57 - 43s - loss: 0.0097 - decoder_loss: -1.4484e-01 - mlp_out_loss: 5.2420e-04 - decoder_cosine: 0.3225 - decoder_mae: 0.2199 - decoder_correlation: 0.8554 - decoder_rmse: 0.3254 - mlp_out_cosine: 0.0023 - mlp_out_mae: 0.0151 - mlp_out_correlation: 1.0006 - mlp_out_rmse: 0.0229 - val_loss: 0.1154 - val_decoder_loss: -2.7561e-02 - val_mlp_out_loss: 7.7902e-04 - val_decoder_cosine: -1.4938e-02 - val_decoder_mae: 0.2522 - val_decoder_correlation: 0.9756 - val_decoder_rmse: 0.4183 - val_mlp_out_cosine: 0.0048 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0042 - val_mlp_out_rmse: 0.0279 - 43s/epoch - 757ms/step\n",
      "Epoch 19/200\n",
      "57/57 - 43s - loss: -1.4793e-02 - decoder_loss: -1.4750e-01 - mlp_out_loss: 5.2347e-04 - decoder_cosine: 0.3263 - decoder_mae: 0.2087 - decoder_correlation: 0.8527 - decoder_rmse: 0.3094 - mlp_out_cosine: 0.0033 - mlp_out_mae: 0.0151 - mlp_out_correlation: 1.0000 - mlp_out_rmse: 0.0229 - val_loss: 0.0953 - val_decoder_loss: -2.7734e-02 - val_mlp_out_loss: 7.7876e-04 - val_decoder_cosine: -7.4353e-04 - val_decoder_mae: 0.2549 - val_decoder_correlation: 0.9760 - val_decoder_rmse: 0.4188 - val_mlp_out_cosine: 0.0035 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0058 - val_mlp_out_rmse: 0.0279 - 43s/epoch - 758ms/step\n",
      "Epoch 20/200\n",
      "57/57 - 43s - loss: -3.3583e-02 - decoder_loss: -1.4786e-01 - mlp_out_loss: 5.2295e-04 - decoder_cosine: 0.3299 - decoder_mae: 0.2025 - decoder_correlation: 0.8525 - decoder_rmse: 0.2998 - mlp_out_cosine: 0.0043 - mlp_out_mae: 0.0151 - mlp_out_correlation: 0.9990 - mlp_out_rmse: 0.0229 - val_loss: 0.0730 - val_decoder_loss: -3.3073e-02 - val_mlp_out_loss: 7.7836e-04 - val_decoder_cosine: 0.0114 - val_decoder_mae: 0.2536 - val_decoder_correlation: 0.9676 - val_decoder_rmse: 0.4212 - val_mlp_out_cosine: 0.0014 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0070 - val_mlp_out_rmse: 0.0279 - 43s/epoch - 758ms/step\n",
      "Epoch 21/200\n",
      "57/57 - 43s - loss: -5.0762e-02 - decoder_loss: -1.4940e-01 - mlp_out_loss: 5.2257e-04 - decoder_cosine: 0.3314 - decoder_mae: 0.1901 - decoder_correlation: 0.8509 - decoder_rmse: 0.2826 - mlp_out_cosine: 0.0042 - mlp_out_mae: 0.0151 - mlp_out_correlation: 0.9997 - mlp_out_rmse: 0.0229 - val_loss: 0.0597 - val_decoder_loss: -3.2014e-02 - val_mlp_out_loss: 7.7822e-04 - val_decoder_cosine: -2.4040e-03 - val_decoder_mae: 0.2568 - val_decoder_correlation: 0.9649 - val_decoder_rmse: 0.4303 - val_mlp_out_cosine: 2.7050e-04 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0087 - val_mlp_out_rmse: 0.0279 - 43s/epoch - 751ms/step\n",
      "Epoch 22/200\n",
      "57/57 - 43s - loss: -6.5703e-02 - decoder_loss: -1.5102e-01 - mlp_out_loss: 5.2231e-04 - decoder_cosine: 0.3328 - decoder_mae: 0.1855 - decoder_correlation: 0.8491 - decoder_rmse: 0.2767 - mlp_out_cosine: 0.0044 - mlp_out_mae: 0.0151 - mlp_out_correlation: 1.0005 - mlp_out_rmse: 0.0229 - val_loss: 0.0518 - val_decoder_loss: -2.7597e-02 - val_mlp_out_loss: 7.7793e-04 - val_decoder_cosine: -3.4535e-03 - val_decoder_mae: 0.2232 - val_decoder_correlation: 0.9762 - val_decoder_rmse: 0.3714 - val_mlp_out_cosine: -1.6176e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0095 - val_mlp_out_rmse: 0.0279 - 43s/epoch - 752ms/step\n",
      "Epoch 23/200\n",
      "57/57 - 43s - loss: -7.9478e-02 - decoder_loss: -1.5340e-01 - mlp_out_loss: 5.2204e-04 - decoder_cosine: 0.3300 - decoder_mae: 0.1723 - decoder_correlation: 0.8467 - decoder_rmse: 0.2587 - mlp_out_cosine: 0.0056 - mlp_out_mae: 0.0151 - mlp_out_correlation: 1.0009 - mlp_out_rmse: 0.0228 - val_loss: 0.0359 - val_decoder_loss: -3.2983e-02 - val_mlp_out_loss: 7.7775e-04 - val_decoder_cosine: 0.0292 - val_decoder_mae: 0.2234 - val_decoder_correlation: 0.9690 - val_decoder_rmse: 0.3808 - val_mlp_out_cosine: -1.6227e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0106 - val_mlp_out_rmse: 0.0279 - 43s/epoch - 753ms/step\n",
      "Epoch 24/200\n",
      "57/57 - 43s - loss: -9.1534e-02 - decoder_loss: -1.5568e-01 - mlp_out_loss: 5.2181e-04 - decoder_cosine: 0.3447 - decoder_mae: 0.1593 - decoder_correlation: 0.8446 - decoder_rmse: 0.2389 - mlp_out_cosine: 0.0042 - mlp_out_mae: 0.0151 - mlp_out_correlation: 1.0012 - mlp_out_rmse: 0.0228 - val_loss: 0.0299 - val_decoder_loss: -2.9972e-02 - val_mlp_out_loss: 7.7747e-04 - val_decoder_cosine: 0.0040 - val_decoder_mae: 0.2006 - val_decoder_correlation: 0.9704 - val_decoder_rmse: 0.3411 - val_mlp_out_cosine: -4.9363e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0116 - val_mlp_out_rmse: 0.0279 - 43s/epoch - 762ms/step\n",
      "Epoch 25/200\n",
      "57/57 - 43s - loss: -9.9913e-02 - decoder_loss: -1.5565e-01 - mlp_out_loss: 5.2168e-04 - decoder_cosine: 0.3482 - decoder_mae: 0.1568 - decoder_correlation: 0.8445 - decoder_rmse: 0.2355 - mlp_out_cosine: 0.0041 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0025 - mlp_out_rmse: 0.0228 - val_loss: 0.0130 - val_decoder_loss: -3.9124e-02 - val_mlp_out_loss: 7.7736e-04 - val_decoder_cosine: 0.0587 - val_decoder_mae: 0.1875 - val_decoder_correlation: 0.9659 - val_decoder_rmse: 0.3107 - val_mlp_out_cosine: -7.5445e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0139 - val_mlp_out_rmse: 0.0279 - 43s/epoch - 758ms/step\n",
      "Epoch 26/200\n",
      "57/57 - 43s - loss: -1.0947e-01 - decoder_loss: -1.5795e-01 - mlp_out_loss: 5.2156e-04 - decoder_cosine: 0.3492 - decoder_mae: 0.1469 - decoder_correlation: 0.8420 - decoder_rmse: 0.2212 - mlp_out_cosine: 0.0043 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0025 - mlp_out_rmse: 0.0228 - val_loss: 0.0068 - val_decoder_loss: -3.8577e-02 - val_mlp_out_loss: 7.7717e-04 - val_decoder_cosine: 0.0700 - val_decoder_mae: 0.1735 - val_decoder_correlation: 0.9648 - val_decoder_rmse: 0.2977 - val_mlp_out_cosine: -7.5336e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0143 - val_mlp_out_rmse: 0.0279 - 43s/epoch - 759ms/step\n",
      "Epoch 27/200\n",
      "57/57 - 43s - loss: -1.1765e-01 - decoder_loss: -1.5985e-01 - mlp_out_loss: 5.2149e-04 - decoder_cosine: 0.3494 - decoder_mae: 0.1394 - decoder_correlation: 0.8403 - decoder_rmse: 0.2113 - mlp_out_cosine: 0.0024 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0039 - mlp_out_rmse: 0.0228 - val_loss: 0.0130 - val_decoder_loss: -2.6573e-02 - val_mlp_out_loss: 7.7701e-04 - val_decoder_cosine: 0.0277 - val_decoder_mae: 0.1636 - val_decoder_correlation: 0.9759 - val_decoder_rmse: 0.2771 - val_mlp_out_cosine: -5.0131e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0149 - val_mlp_out_rmse: 0.0279 - 43s/epoch - 756ms/step\n",
      "Epoch 28/200\n",
      "57/57 - 43s - loss: -1.2424e-01 - decoder_loss: -1.6100e-01 - mlp_out_loss: 5.2142e-04 - decoder_cosine: 0.3581 - decoder_mae: 0.1277 - decoder_correlation: 0.8390 - decoder_rmse: 0.1931 - mlp_out_cosine: 0.0025 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0043 - mlp_out_rmse: 0.0228 - val_loss: 0.0057 - val_decoder_loss: -2.8784e-02 - val_mlp_out_loss: 7.7687e-04 - val_decoder_cosine: 0.0100 - val_decoder_mae: 0.1598 - val_decoder_correlation: 0.9702 - val_decoder_rmse: 0.2739 - val_mlp_out_cosine: -8.7600e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0166 - val_mlp_out_rmse: 0.0279 - 43s/epoch - 758ms/step\n",
      "Epoch 29/200\n",
      "57/57 - 43s - loss: -1.3017e-01 - decoder_loss: -1.6222e-01 - mlp_out_loss: 5.2139e-04 - decoder_cosine: 0.3571 - decoder_mae: 0.1272 - decoder_correlation: 0.8379 - decoder_rmse: 0.1926 - mlp_out_cosine: 0.0025 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0056 - mlp_out_rmse: 0.0228 - val_loss: -1.7982e-03 - val_decoder_loss: -3.1903e-02 - val_mlp_out_loss: 7.7676e-04 - val_decoder_cosine: 0.0100 - val_decoder_mae: 0.1673 - val_decoder_correlation: 0.9713 - val_decoder_rmse: 0.2875 - val_mlp_out_cosine: -1.0457e-02 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0173 - val_mlp_out_rmse: 0.0279 - 43s/epoch - 754ms/step\n",
      "Epoch 30/200\n",
      "57/57 - 43s - loss: -1.3609e-01 - decoder_loss: -1.6403e-01 - mlp_out_loss: 5.2134e-04 - decoder_cosine: 0.3641 - decoder_mae: 0.1226 - decoder_correlation: 0.8360 - decoder_rmse: 0.1856 - mlp_out_cosine: 0.0015 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0055 - mlp_out_rmse: 0.0228 - val_loss: 8.4177e-04 - val_decoder_loss: -2.5449e-02 - val_mlp_out_loss: 7.7665e-04 - val_decoder_cosine: -1.8902e-02 - val_decoder_mae: 0.1581 - val_decoder_correlation: 0.9756 - val_decoder_rmse: 0.2722 - val_mlp_out_cosine: -1.0699e-02 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0189 - val_mlp_out_rmse: 0.0279 - 43s/epoch - 756ms/step\n",
      "Epoch 31/200\n",
      "57/57 - 43s - loss: -1.4002e-01 - decoder_loss: -1.6440e-01 - mlp_out_loss: 5.2133e-04 - decoder_cosine: 0.3665 - decoder_mae: 0.1235 - decoder_correlation: 0.8359 - decoder_rmse: 0.1877 - mlp_out_cosine: 0.0013 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0060 - mlp_out_rmse: 0.0228 - val_loss: -5.5678e-03 - val_decoder_loss: -2.8540e-02 - val_mlp_out_loss: 7.7654e-04 - val_decoder_cosine: 0.0106 - val_decoder_mae: 0.1536 - val_decoder_correlation: 0.9710 - val_decoder_rmse: 0.2587 - val_mlp_out_cosine: -1.1138e-02 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0198 - val_mlp_out_rmse: 0.0279 - 43s/epoch - 760ms/step\n",
      "Epoch 32/200\n",
      "57/57 - 43s - loss: -1.4532e-01 - decoder_loss: -1.6659e-01 - mlp_out_loss: 5.2130e-04 - decoder_cosine: 0.3759 - decoder_mae: 0.1153 - decoder_correlation: 0.8334 - decoder_rmse: 0.1756 - mlp_out_cosine: -1.0210e-04 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0065 - mlp_out_rmse: 0.0228 - val_loss: 0.0013 - val_decoder_loss: -1.8763e-02 - val_mlp_out_loss: 7.7645e-04 - val_decoder_cosine: 0.0101 - val_decoder_mae: 0.1413 - val_decoder_correlation: 0.9796 - val_decoder_rmse: 0.2469 - val_mlp_out_cosine: -1.0440e-02 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0204 - val_mlp_out_rmse: 0.0279 - 43s/epoch - 760ms/step\n",
      "Epoch 33/200\n",
      "57/57 - 44s - loss: -1.4871e-01 - decoder_loss: -1.6728e-01 - mlp_out_loss: 5.2129e-04 - decoder_cosine: 0.3741 - decoder_mae: 0.1094 - decoder_correlation: 0.8325 - decoder_rmse: 0.1672 - mlp_out_cosine: -4.0842e-04 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0073 - mlp_out_rmse: 0.0228 - val_loss: -2.4564e-03 - val_decoder_loss: -2.0018e-02 - val_mlp_out_loss: 7.7641e-04 - val_decoder_cosine: -2.5410e-02 - val_decoder_mae: 0.1365 - val_decoder_correlation: 0.9783 - val_decoder_rmse: 0.2221 - val_mlp_out_cosine: -1.1228e-02 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0217 - val_mlp_out_rmse: 0.0279 - 44s/epoch - 764ms/step\n",
      "Epoch 34/200\n",
      "57/57 - 43s - loss: -1.5378e-01 - decoder_loss: -1.6999e-01 - mlp_out_loss: 5.2128e-04 - decoder_cosine: 0.3842 - decoder_mae: 0.1068 - decoder_correlation: 0.8302 - decoder_rmse: 0.1637 - mlp_out_cosine: -1.5890e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0084 - mlp_out_rmse: 0.0228 - val_loss: -1.0684e-02 - val_decoder_loss: -2.6049e-02 - val_mlp_out_loss: 7.7632e-04 - val_decoder_cosine: 0.0638 - val_decoder_mae: 0.1268 - val_decoder_correlation: 0.9777 - val_decoder_rmse: 0.2148 - val_mlp_out_cosine: -9.7359e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0216 - val_mlp_out_rmse: 0.0279 - 43s/epoch - 753ms/step\n",
      "Epoch 35/200\n",
      "57/57 - 43s - loss: -1.5768e-01 - decoder_loss: -1.7183e-01 - mlp_out_loss: 5.2126e-04 - decoder_cosine: 0.3860 - decoder_mae: 0.0971 - decoder_correlation: 0.8280 - decoder_rmse: 0.1494 - mlp_out_cosine: -1.9167e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0088 - mlp_out_rmse: 0.0228 - val_loss: -1.7562e-02 - val_decoder_loss: -3.1010e-02 - val_mlp_out_loss: 7.7627e-04 - val_decoder_cosine: 0.0584 - val_decoder_mae: 0.1283 - val_decoder_correlation: 0.9734 - val_decoder_rmse: 0.2195 - val_mlp_out_cosine: -9.0196e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0215 - val_mlp_out_rmse: 0.0279 - 43s/epoch - 755ms/step\n",
      "Epoch 36/200\n",
      "57/57 - 43s - loss: -1.6052e-01 - decoder_loss: -1.7288e-01 - mlp_out_loss: 5.2127e-04 - decoder_cosine: 0.3857 - decoder_mae: 0.0960 - decoder_correlation: 0.8269 - decoder_rmse: 0.1482 - mlp_out_cosine: -1.7952e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0098 - mlp_out_rmse: 0.0228 - val_loss: -8.9011e-03 - val_decoder_loss: -2.0678e-02 - val_mlp_out_loss: 7.7622e-04 - val_decoder_cosine: 0.0198 - val_decoder_mae: 0.1181 - val_decoder_correlation: 0.9783 - val_decoder_rmse: 0.2067 - val_mlp_out_cosine: -6.3238e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0214 - val_mlp_out_rmse: 0.0279 - 43s/epoch - 751ms/step\n",
      "Epoch 37/200\n",
      "57/57 - 43s - loss: -1.6609e-01 - decoder_loss: -1.7689e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.3961 - decoder_mae: 0.0873 - decoder_correlation: 0.8234 - decoder_rmse: 0.1366 - mlp_out_cosine: -1.5940e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0092 - mlp_out_rmse: 0.0228 - val_loss: -9.8655e-03 - val_decoder_loss: -2.0185e-02 - val_mlp_out_loss: 7.7619e-04 - val_decoder_cosine: 0.0084 - val_decoder_mae: 0.1237 - val_decoder_correlation: 0.9842 - val_decoder_rmse: 0.2111 - val_mlp_out_cosine: -3.7775e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0212 - val_mlp_out_rmse: 0.0279 - 43s/epoch - 746ms/step\n",
      "Epoch 38/200\n",
      "57/57 - 41s - loss: -1.6943e-01 - decoder_loss: -1.7886e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.4008 - decoder_mae: 0.0852 - decoder_correlation: 0.8211 - decoder_rmse: 0.1338 - mlp_out_cosine: -1.8283e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0080 - mlp_out_rmse: 0.0228 - val_loss: -1.4360e-02 - val_decoder_loss: -2.3409e-02 - val_mlp_out_loss: 7.7617e-04 - val_decoder_cosine: 0.0743 - val_decoder_mae: 0.1070 - val_decoder_correlation: 0.9797 - val_decoder_rmse: 0.1806 - val_mlp_out_cosine: -4.2734e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0212 - val_mlp_out_rmse: 0.0279 - 41s/epoch - 712ms/step\n",
      "Epoch 39/200\n",
      "57/57 - 42s - loss: -1.7315e-01 - decoder_loss: -1.8139e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.4086 - decoder_mae: 0.0801 - decoder_correlation: 0.8188 - decoder_rmse: 0.1275 - mlp_out_cosine: -1.7950e-04 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0078 - mlp_out_rmse: 0.0228 - val_loss: -4.2537e-03 - val_decoder_loss: -1.2195e-02 - val_mlp_out_loss: 7.7616e-04 - val_decoder_cosine: -3.4210e-02 - val_decoder_mae: 0.1033 - val_decoder_correlation: 0.9789 - val_decoder_rmse: 0.1741 - val_mlp_out_cosine: -3.8438e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0203 - val_mlp_out_rmse: 0.0279 - 42s/epoch - 737ms/step\n",
      "Epoch 40/200\n",
      "57/57 - 42s - loss: -1.7168e-01 - decoder_loss: -1.7889e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.4093 - decoder_mae: 0.0815 - decoder_correlation: 0.8212 - decoder_rmse: 0.1298 - mlp_out_cosine: -1.7178e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0088 - mlp_out_rmse: 0.0228 - val_loss: -1.4703e-02 - val_decoder_loss: -2.1679e-02 - val_mlp_out_loss: 7.7615e-04 - val_decoder_cosine: 0.0276 - val_decoder_mae: 0.1098 - val_decoder_correlation: 0.9809 - val_decoder_rmse: 0.1801 - val_mlp_out_cosine: -3.7351e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0192 - val_mlp_out_rmse: 0.0279 - 42s/epoch - 730ms/step\n",
      "Epoch 41/200\n",
      "57/57 - 40s - loss: -1.7663e-01 - decoder_loss: -1.8294e-01 - mlp_out_loss: 5.2126e-04 - decoder_cosine: 0.4172 - decoder_mae: 0.0832 - decoder_correlation: 0.8172 - decoder_rmse: 0.1326 - mlp_out_cosine: -1.4827e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0078 - mlp_out_rmse: 0.0228 - val_loss: -3.3506e-03 - val_decoder_loss: -9.4859e-03 - val_mlp_out_loss: 7.7612e-04 - val_decoder_cosine: -5.3050e-02 - val_decoder_mae: 0.1048 - val_decoder_correlation: 0.9861 - val_decoder_rmse: 0.1709 - val_mlp_out_cosine: -3.7372e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0181 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 705ms/step\n",
      "Epoch 42/200\n",
      "57/57 - 41s - loss: -1.7981e-01 - decoder_loss: -1.8533e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.4193 - decoder_mae: 0.0778 - decoder_correlation: 0.8147 - decoder_rmse: 0.1244 - mlp_out_cosine: -1.0353e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0059 - mlp_out_rmse: 0.0228 - val_loss: -1.1923e-02 - val_decoder_loss: -1.7327e-02 - val_mlp_out_loss: 7.7613e-04 - val_decoder_cosine: 0.0509 - val_decoder_mae: 0.0936 - val_decoder_correlation: 0.9891 - val_decoder_rmse: 0.1583 - val_mlp_out_cosine: -3.7366e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0172 - val_mlp_out_rmse: 0.0279 - 41s/epoch - 711ms/step\n",
      "Epoch 43/200\n",
      "57/57 - 40s - loss: -1.8221e-01 - decoder_loss: -1.8704e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.4174 - decoder_mae: 0.0759 - decoder_correlation: 0.8127 - decoder_rmse: 0.1221 - mlp_out_cosine: -1.7969e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0050 - mlp_out_rmse: 0.0228 - val_loss: -6.9559e-03 - val_decoder_loss: -1.1724e-02 - val_mlp_out_loss: 7.7611e-04 - val_decoder_cosine: 0.0372 - val_decoder_mae: 0.0929 - val_decoder_correlation: 0.9847 - val_decoder_rmse: 0.1560 - val_mlp_out_cosine: -3.7316e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0153 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 709ms/step\n",
      "Epoch 44/200\n",
      "57/57 - 40s - loss: -1.8235e-01 - decoder_loss: -1.8658e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.4191 - decoder_mae: 0.0770 - decoder_correlation: 0.8135 - decoder_rmse: 0.1233 - mlp_out_cosine: -1.6162e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0045 - mlp_out_rmse: 0.0228 - val_loss: -9.6488e-03 - val_decoder_loss: -1.3864e-02 - val_mlp_out_loss: 7.7612e-04 - val_decoder_cosine: 0.0267 - val_decoder_mae: 0.0914 - val_decoder_correlation: 0.9822 - val_decoder_rmse: 0.1554 - val_mlp_out_cosine: -3.7335e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0151 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 704ms/step\n",
      "Epoch 45/200\n",
      "57/57 - 40s - loss: -1.8404e-01 - decoder_loss: -1.8776e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.4192 - decoder_mae: 0.0766 - decoder_correlation: 0.8122 - decoder_rmse: 0.1232 - mlp_out_cosine: -1.7956e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0047 - mlp_out_rmse: 0.0228 - val_loss: -2.0740e-02 - val_decoder_loss: -2.4475e-02 - val_mlp_out_loss: 7.7611e-04 - val_decoder_cosine: 0.0857 - val_decoder_mae: 0.0915 - val_decoder_correlation: 0.9763 - val_decoder_rmse: 0.1572 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0164 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 708ms/step\n",
      "Epoch 46/200\n",
      "57/57 - 40s - loss: -1.8571e-01 - decoder_loss: -1.8898e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.4352 - decoder_mae: 0.0758 - decoder_correlation: 0.8107 - decoder_rmse: 0.1228 - mlp_out_cosine: -1.8018e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0046 - mlp_out_rmse: 0.0228 - val_loss: -1.5259e-02 - val_decoder_loss: -1.8578e-02 - val_mlp_out_loss: 7.7611e-04 - val_decoder_cosine: 0.0132 - val_decoder_mae: 0.0976 - val_decoder_correlation: 0.9781 - val_decoder_rmse: 0.1676 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0154 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 702ms/step\n",
      "Epoch 47/200\n",
      "57/57 - 40s - loss: -1.8735e-01 - decoder_loss: -1.9023e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.4298 - decoder_mae: 0.0765 - decoder_correlation: 0.8099 - decoder_rmse: 0.1237 - mlp_out_cosine: -1.7878e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0055 - mlp_out_rmse: 0.0228 - val_loss: -8.3197e-03 - val_decoder_loss: -1.1278e-02 - val_mlp_out_loss: 7.7613e-04 - val_decoder_cosine: 0.0215 - val_decoder_mae: 0.0889 - val_decoder_correlation: 0.9917 - val_decoder_rmse: 0.1486 - val_mlp_out_cosine: -3.7468e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0152 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 709ms/step\n",
      "Epoch 48/200\n",
      "57/57 - 40s - loss: -1.8753e-01 - decoder_loss: -1.9008e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.4306 - decoder_mae: 0.0752 - decoder_correlation: 0.8100 - decoder_rmse: 0.1217 - mlp_out_cosine: -1.6803e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0032 - mlp_out_rmse: 0.0228 - val_loss: -7.4112e-03 - val_decoder_loss: -1.0057e-02 - val_mlp_out_loss: 7.7610e-04 - val_decoder_cosine: -2.5600e-02 - val_decoder_mae: 0.0917 - val_decoder_correlation: 0.9883 - val_decoder_rmse: 0.1526 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0144 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 708ms/step\n",
      "Epoch 49/200\n",
      "57/57 - 40s - loss: -1.8913e-01 - decoder_loss: -1.9139e-01 - mlp_out_loss: 5.2123e-04 - decoder_cosine: 0.4253 - decoder_mae: 0.0747 - decoder_correlation: 0.8085 - decoder_rmse: 0.1213 - mlp_out_cosine: -1.7924e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0023 - mlp_out_rmse: 0.0228 - val_loss: -2.4787e-02 - val_decoder_loss: -2.7162e-02 - val_mlp_out_loss: 7.7610e-04 - val_decoder_cosine: 0.0724 - val_decoder_mae: 0.0909 - val_decoder_correlation: 0.9734 - val_decoder_rmse: 0.1536 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0147 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 706ms/step\n",
      "Epoch 50/200\n",
      "57/57 - 40s - loss: -1.9011e-01 - decoder_loss: -1.9211e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.4322 - decoder_mae: 0.0757 - decoder_correlation: 0.8080 - decoder_rmse: 0.1234 - mlp_out_cosine: -9.8945e-04 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0027 - mlp_out_rmse: 0.0228 - val_loss: -1.8327e-02 - val_decoder_loss: -2.0470e-02 - val_mlp_out_loss: 7.7611e-04 - val_decoder_cosine: 0.0539 - val_decoder_mae: 0.0921 - val_decoder_correlation: 0.9851 - val_decoder_rmse: 0.1556 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0145 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 704ms/step\n",
      "Epoch 51/200\n",
      "57/57 - 40s - loss: -1.9230e-01 - decoder_loss: -1.9409e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.4306 - decoder_mae: 0.0728 - decoder_correlation: 0.8060 - decoder_rmse: 0.1186 - mlp_out_cosine: -1.7884e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0013 - mlp_out_rmse: 0.0228 - val_loss: -9.7791e-03 - val_decoder_loss: -1.1720e-02 - val_mlp_out_loss: 7.7611e-04 - val_decoder_cosine: -1.8129e-02 - val_decoder_mae: 0.0919 - val_decoder_correlation: 0.9901 - val_decoder_rmse: 0.1503 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0139 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 709ms/step\n",
      "Epoch 52/200\n",
      "57/57 - 40s - loss: -1.9286e-01 - decoder_loss: -1.9446e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.4289 - decoder_mae: 0.0714 - decoder_correlation: 0.8057 - decoder_rmse: 0.1169 - mlp_out_cosine: -1.0895e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0000 - mlp_out_rmse: 0.0228 - val_loss: -1.8854e-02 - val_decoder_loss: -2.0622e-02 - val_mlp_out_loss: 7.7611e-04 - val_decoder_cosine: 0.0799 - val_decoder_mae: 0.0888 - val_decoder_correlation: 0.9899 - val_decoder_rmse: 0.1463 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0132 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 707ms/step\n",
      "Epoch 53/200\n",
      "57/57 - 40s - loss: -1.9485e-01 - decoder_loss: -1.9629e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.4368 - decoder_mae: 0.0733 - decoder_correlation: 0.8037 - decoder_rmse: 0.1187 - mlp_out_cosine: -1.8899e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0003 - mlp_out_rmse: 0.0228 - val_loss: -1.7682e-02 - val_decoder_loss: -1.9301e-02 - val_mlp_out_loss: 7.7610e-04 - val_decoder_cosine: 0.0772 - val_decoder_mae: 0.0865 - val_decoder_correlation: 0.9953 - val_decoder_rmse: 0.1457 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0139 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 706ms/step\n",
      "Epoch 54/200\n",
      "57/57 - 41s - loss: -1.9315e-01 - decoder_loss: -1.9446e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.4324 - decoder_mae: 0.0735 - decoder_correlation: 0.8053 - decoder_rmse: 0.1203 - mlp_out_cosine: -1.5538e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0007 - mlp_out_rmse: 0.0228 - val_loss: -1.1457e-02 - val_decoder_loss: -1.2949e-02 - val_mlp_out_loss: 7.7610e-04 - val_decoder_cosine: 0.0045 - val_decoder_mae: 0.0907 - val_decoder_correlation: 0.9907 - val_decoder_rmse: 0.1493 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0139 - val_mlp_out_rmse: 0.0279 - 41s/epoch - 714ms/step\n",
      "Epoch 55/200\n",
      "57/57 - 40s - loss: -1.9548e-01 - decoder_loss: -1.9666e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.4385 - decoder_mae: 0.0703 - decoder_correlation: 0.8032 - decoder_rmse: 0.1154 - mlp_out_cosine: -1.7933e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0001 - mlp_out_rmse: 0.0228 - val_loss: -9.6304e-03 - val_decoder_loss: -1.1012e-02 - val_mlp_out_loss: 7.7613e-04 - val_decoder_cosine: -5.6756e-03 - val_decoder_mae: 0.0867 - val_decoder_correlation: 0.9913 - val_decoder_rmse: 0.1428 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0142 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 708ms/step\n",
      "Epoch 56/200\n",
      "57/57 - 40s - loss: -1.9553e-01 - decoder_loss: -1.9661e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.4359 - decoder_mae: 0.0729 - decoder_correlation: 0.8034 - decoder_rmse: 0.1195 - mlp_out_cosine: -1.3207e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9992 - mlp_out_rmse: 0.0228 - val_loss: -1.2579e-02 - val_decoder_loss: -1.3867e-02 - val_mlp_out_loss: 7.7622e-04 - val_decoder_cosine: 0.0486 - val_decoder_mae: 0.0826 - val_decoder_correlation: 0.9866 - val_decoder_rmse: 0.1410 - val_mlp_out_cosine: 0.0037 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0141 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 706ms/step\n",
      "Epoch 57/200\n",
      "57/57 - 41s - loss: -1.9639e-01 - decoder_loss: -1.9738e-01 - mlp_out_loss: 5.2126e-04 - decoder_cosine: 0.4293 - decoder_mae: 0.0727 - decoder_correlation: 0.8028 - decoder_rmse: 0.1195 - mlp_out_cosine: -1.0078e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9990 - mlp_out_rmse: 0.0228 - val_loss: -1.9247e-02 - val_decoder_loss: -2.0455e-02 - val_mlp_out_loss: 7.7611e-04 - val_decoder_cosine: 0.0416 - val_decoder_mae: 0.0870 - val_decoder_correlation: 0.9843 - val_decoder_rmse: 0.1487 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0117 - val_mlp_out_rmse: 0.0279 - 41s/epoch - 711ms/step\n",
      "Epoch 58/200\n",
      "57/57 - 41s - loss: -1.9636e-01 - decoder_loss: -1.9728e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.4313 - decoder_mae: 0.0729 - decoder_correlation: 0.8026 - decoder_rmse: 0.1194 - mlp_out_cosine: -1.7933e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9985 - mlp_out_rmse: 0.0228 - val_loss: -2.7053e-02 - val_decoder_loss: -2.8193e-02 - val_mlp_out_loss: 7.7610e-04 - val_decoder_cosine: 0.1012 - val_decoder_mae: 0.0882 - val_decoder_correlation: 0.9787 - val_decoder_rmse: 0.1540 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0124 - val_mlp_out_rmse: 0.0279 - 41s/epoch - 713ms/step\n",
      "Epoch 59/200\n",
      "57/57 - 40s - loss: -1.9721e-01 - decoder_loss: -1.9807e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.4413 - decoder_mae: 0.0708 - decoder_correlation: 0.8019 - decoder_rmse: 0.1162 - mlp_out_cosine: -8.9991e-04 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9987 - mlp_out_rmse: 0.0228 - val_loss: -4.6384e-03 - val_decoder_loss: -5.7198e-03 - val_mlp_out_loss: 7.7613e-04 - val_decoder_cosine: -4.1994e-02 - val_decoder_mae: 0.0864 - val_decoder_correlation: 0.9843 - val_decoder_rmse: 0.1485 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0115 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 706ms/step\n",
      "Epoch 60/200\n",
      "57/57 - 40s - loss: -1.9675e-01 - decoder_loss: -1.9756e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.4416 - decoder_mae: 0.0725 - decoder_correlation: 0.8025 - decoder_rmse: 0.1191 - mlp_out_cosine: -2.2381e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9978 - mlp_out_rmse: 0.0228 - val_loss: -9.3658e-03 - val_decoder_loss: -1.0398e-02 - val_mlp_out_loss: 7.7612e-04 - val_decoder_cosine: 0.0146 - val_decoder_mae: 0.0840 - val_decoder_correlation: 0.9904 - val_decoder_rmse: 0.1438 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0127 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 705ms/step\n",
      "Epoch 61/200\n",
      "57/57 - 40s - loss: -1.9859e-01 - decoder_loss: -1.9935e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.4359 - decoder_mae: 0.0709 - decoder_correlation: 0.8007 - decoder_rmse: 0.1170 - mlp_out_cosine: -1.0726e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9975 - mlp_out_rmse: 0.0228 - val_loss: -1.0008e-02 - val_decoder_loss: -1.0998e-02 - val_mlp_out_loss: 7.7610e-04 - val_decoder_cosine: -1.0187e-02 - val_decoder_mae: 0.0870 - val_decoder_correlation: 0.9878 - val_decoder_rmse: 0.1472 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0107 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 707ms/step\n",
      "Epoch 62/200\n",
      "57/57 - 40s - loss: -1.9966e-01 - decoder_loss: -2.0037e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.4465 - decoder_mae: 0.0704 - decoder_correlation: 0.7996 - decoder_rmse: 0.1161 - mlp_out_cosine: -1.8106e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9977 - mlp_out_rmse: 0.0228 - val_loss: -5.6969e-03 - val_decoder_loss: -6.6515e-03 - val_mlp_out_loss: 7.7617e-04 - val_decoder_cosine: 0.0239 - val_decoder_mae: 0.0800 - val_decoder_correlation: 0.9940 - val_decoder_rmse: 0.1366 - val_mlp_out_cosine: 0.0037 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0080 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 708ms/step\n",
      "Epoch 63/200\n",
      "57/57 - 40s - loss: -1.9927e-01 - decoder_loss: -1.9995e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.4366 - decoder_mae: 0.0711 - decoder_correlation: 0.8001 - decoder_rmse: 0.1167 - mlp_out_cosine: -2.1581e-04 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9970 - mlp_out_rmse: 0.0228 - val_loss: -2.0920e-02 - val_decoder_loss: -2.1845e-02 - val_mlp_out_loss: 7.7610e-04 - val_decoder_cosine: 0.1164 - val_decoder_mae: 0.0847 - val_decoder_correlation: 0.9961 - val_decoder_rmse: 0.1442 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0094 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 706ms/step\n",
      "Epoch 64/200\n",
      "57/57 - 40s - loss: -1.9940e-01 - decoder_loss: -2.0006e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.4535 - decoder_mae: 0.0711 - decoder_correlation: 0.7999 - decoder_rmse: 0.1169 - mlp_out_cosine: -1.0316e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9959 - mlp_out_rmse: 0.0228 - val_loss: -8.7527e-03 - val_decoder_loss: -9.6521e-03 - val_mlp_out_loss: 7.7610e-04 - val_decoder_cosine: -1.1794e-02 - val_decoder_mae: 0.0872 - val_decoder_correlation: 0.9964 - val_decoder_rmse: 0.1451 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0122 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 704ms/step\n",
      "Epoch 65/200\n",
      "57/57 - 40s - loss: -1.9996e-01 - decoder_loss: -2.0059e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.4353 - decoder_mae: 0.0711 - decoder_correlation: 0.7995 - decoder_rmse: 0.1170 - mlp_out_cosine: -1.7933e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9969 - mlp_out_rmse: 0.0228 - val_loss: 5.0376e-04 - val_decoder_loss: -3.7436e-04 - val_mlp_out_loss: 7.7610e-04 - val_decoder_cosine: -7.4558e-02 - val_decoder_mae: 0.0954 - val_decoder_correlation: 1.0023 - val_decoder_rmse: 0.1584 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0035 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 706ms/step\n",
      "Epoch 66/200\n",
      "57/57 - 40s - loss: -2.0028e-01 - decoder_loss: -2.0090e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.4491 - decoder_mae: 0.0716 - decoder_correlation: 0.7991 - decoder_rmse: 0.1176 - mlp_out_cosine: -1.6725e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9979 - mlp_out_rmse: 0.0228 - val_loss: -1.2956e-02 - val_decoder_loss: -1.3817e-02 - val_mlp_out_loss: 7.7611e-04 - val_decoder_cosine: 0.0080 - val_decoder_mae: 0.0853 - val_decoder_correlation: 0.9873 - val_decoder_rmse: 0.1452 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0091 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 702ms/step\n",
      "Epoch 67/200\n",
      "57/57 - 40s - loss: -2.0282e-01 - decoder_loss: -2.0341e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.4500 - decoder_mae: 0.0696 - decoder_correlation: 0.7964 - decoder_rmse: 0.1149 - mlp_out_cosine: -2.2206e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9985 - mlp_out_rmse: 0.0228 - val_loss: -1.0522e-02 - val_decoder_loss: -1.1367e-02 - val_mlp_out_loss: 7.7611e-04 - val_decoder_cosine: 0.0324 - val_decoder_mae: 0.0838 - val_decoder_correlation: 0.9982 - val_decoder_rmse: 0.1419 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0117 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 710ms/step\n",
      "Epoch 68/200\n",
      "57/57 - 40s - loss: -2.0207e-01 - decoder_loss: -2.0265e-01 - mlp_out_loss: 5.2126e-04 - decoder_cosine: 0.4480 - decoder_mae: 0.0700 - decoder_correlation: 0.7973 - decoder_rmse: 0.1152 - mlp_out_cosine: 6.0426e-05 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9982 - mlp_out_rmse: 0.0228 - val_loss: -9.3687e-03 - val_decoder_loss: -1.0202e-02 - val_mlp_out_loss: 7.7610e-04 - val_decoder_cosine: 0.0235 - val_decoder_mae: 0.0862 - val_decoder_correlation: 1.0009 - val_decoder_rmse: 0.1450 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0004 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 707ms/step\n",
      "Epoch 69/200\n",
      "57/57 - 40s - loss: -2.0300e-01 - decoder_loss: -2.0358e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.4496 - decoder_mae: 0.0695 - decoder_correlation: 0.7966 - decoder_rmse: 0.1151 - mlp_out_cosine: -1.7933e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9982 - mlp_out_rmse: 0.0228 - val_loss: -7.0756e-03 - val_decoder_loss: -7.8984e-03 - val_mlp_out_loss: 7.7612e-04 - val_decoder_cosine: -1.6151e-02 - val_decoder_mae: 0.0840 - val_decoder_correlation: 0.9926 - val_decoder_rmse: 0.1422 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 705ms/step\n",
      "Epoch 70/200\n",
      "57/57 - 40s - loss: -2.0338e-01 - decoder_loss: -2.0395e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.4523 - decoder_mae: 0.0696 - decoder_correlation: 0.7962 - decoder_rmse: 0.1148 - mlp_out_cosine: -1.7049e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0002 - mlp_out_rmse: 0.0228 - val_loss: -7.8625e-03 - val_decoder_loss: -8.6767e-03 - val_mlp_out_loss: 7.7612e-04 - val_decoder_cosine: 0.0056 - val_decoder_mae: 0.0865 - val_decoder_correlation: 0.9933 - val_decoder_rmse: 0.1463 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0007 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 704ms/step\n",
      "Epoch 71/200\n",
      "57/57 - 40s - loss: -2.0386e-01 - decoder_loss: -2.0442e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.4511 - decoder_mae: 0.0683 - decoder_correlation: 0.7955 - decoder_rmse: 0.1135 - mlp_out_cosine: -2.8055e-04 - mlp_out_mae: 0.0150 - mlp_out_correlation: nan - mlp_out_rmse: 0.0228 - val_loss: -1.0379e-02 - val_decoder_loss: -1.1186e-02 - val_mlp_out_loss: 7.7612e-04 - val_decoder_cosine: -3.9610e-03 - val_decoder_mae: 0.0897 - val_decoder_correlation: 0.9928 - val_decoder_rmse: 0.1510 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: nan - val_mlp_out_rmse: 0.0279 - 40s/epoch - 705ms/step\n",
      "Epoch 72/200\n",
      "57/57 - 40s - loss: -2.0321e-01 - decoder_loss: -2.0376e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.4575 - decoder_mae: 0.0685 - decoder_correlation: 0.7960 - decoder_rmse: 0.1131 - mlp_out_cosine: -2.1732e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: nan - mlp_out_rmse: 0.0228 - val_loss: -1.1211e-02 - val_decoder_loss: -1.2013e-02 - val_mlp_out_loss: 7.7611e-04 - val_decoder_cosine: 0.0238 - val_decoder_mae: 0.0821 - val_decoder_correlation: 0.9911 - val_decoder_rmse: 0.1410 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 704ms/step\n",
      "Epoch 73/200\n",
      "57/57 - 40s - loss: -2.0588e-01 - decoder_loss: -2.0643e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.4627 - decoder_mae: 0.0665 - decoder_correlation: 0.7932 - decoder_rmse: 0.1101 - mlp_out_cosine: -6.8842e-04 - mlp_out_mae: 0.0150 - mlp_out_correlation: nan - mlp_out_rmse: 0.0228 - val_loss: -1.3887e-02 - val_decoder_loss: -1.4683e-02 - val_mlp_out_loss: 7.7610e-04 - val_decoder_cosine: 0.0826 - val_decoder_mae: 0.0762 - val_decoder_correlation: 0.9913 - val_decoder_rmse: 0.1308 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0027 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 705ms/step\n",
      "Epoch 74/200\n",
      "57/57 - 40s - loss: -2.0366e-01 - decoder_loss: -2.0420e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.4546 - decoder_mae: 0.0674 - decoder_correlation: 0.7958 - decoder_rmse: 0.1121 - mlp_out_cosine: -9.7328e-04 - mlp_out_mae: 0.0150 - mlp_out_correlation: nan - mlp_out_rmse: 0.0228 - val_loss: -1.1032e-02 - val_decoder_loss: -1.1824e-02 - val_mlp_out_loss: 7.7610e-04 - val_decoder_cosine: 0.0366 - val_decoder_mae: 0.0839 - val_decoder_correlation: 0.9916 - val_decoder_rmse: 0.1443 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: nan - val_mlp_out_rmse: 0.0279 - 40s/epoch - 705ms/step\n",
      "Epoch 75/200\n",
      "57/57 - 40s - loss: -2.0400e-01 - decoder_loss: -2.0454e-01 - mlp_out_loss: 5.2126e-04 - decoder_cosine: 0.4616 - decoder_mae: 0.0686 - decoder_correlation: 0.7957 - decoder_rmse: 0.1138 - mlp_out_cosine: 9.0638e-05 - mlp_out_mae: 0.0150 - mlp_out_correlation: nan - mlp_out_rmse: 0.0228 - val_loss: -8.7602e-03 - val_decoder_loss: -9.5496e-03 - val_mlp_out_loss: 7.7610e-04 - val_decoder_cosine: 0.0023 - val_decoder_mae: 0.0862 - val_decoder_correlation: 0.9942 - val_decoder_rmse: 0.1458 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 710ms/step\n",
      "Epoch 76/200\n",
      "57/57 - 40s - loss: -2.0447e-01 - decoder_loss: -2.0500e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.4593 - decoder_mae: 0.0685 - decoder_correlation: 0.7951 - decoder_rmse: 0.1134 - mlp_out_cosine: -1.7113e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: nan - mlp_out_rmse: 0.0228 - val_loss: -1.6672e-02 - val_decoder_loss: -1.7459e-02 - val_mlp_out_loss: 7.7610e-04 - val_decoder_cosine: 0.0336 - val_decoder_mae: 0.0876 - val_decoder_correlation: 0.9853 - val_decoder_rmse: 0.1474 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0279 - 40s/epoch - 706ms/step\n",
      "Epoch 77/200\n",
      "57/57 - 41s - loss: -2.0671e-01 - decoder_loss: -2.0724e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.4661 - decoder_mae: 0.0674 - decoder_correlation: 0.7927 - decoder_rmse: 0.1119 - mlp_out_cosine: -1.6768e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: nan - mlp_out_rmse: 0.0228 - val_loss: -2.4435e-02 - val_decoder_loss: -2.5219e-02 - val_mlp_out_loss: 7.7611e-04 - val_decoder_cosine: 0.0841 - val_decoder_mae: 0.0831 - val_decoder_correlation: 0.9919 - val_decoder_rmse: 0.1395 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0279 - 41s/epoch - 713ms/step\n",
      "Epoch 78/200\n",
      "57/57 - 41s - loss: -2.0564e-01 - decoder_loss: -2.0617e-01 - mlp_out_loss: 5.2127e-04 - decoder_cosine: 0.4531 - decoder_mae: 0.0677 - decoder_correlation: 0.7938 - decoder_rmse: 0.1122 - mlp_out_cosine: -2.7774e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: nan - mlp_out_rmse: 0.0228 - val_loss: -1.8342e-02 - val_decoder_loss: -1.9125e-02 - val_mlp_out_loss: 7.7610e-04 - val_decoder_cosine: 0.0650 - val_decoder_mae: 0.0828 - val_decoder_correlation: 0.9894 - val_decoder_rmse: 0.1388 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0279 - 41s/epoch - 716ms/step\n",
      "Epoch 79/200\n",
      "57/57 - 44s - loss: -2.0657e-01 - decoder_loss: -2.0710e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.4622 - decoder_mae: 0.0674 - decoder_correlation: 0.7929 - decoder_rmse: 0.1115 - mlp_out_cosine: -2.0933e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: nan - mlp_out_rmse: 0.0228 - val_loss: -1.0946e-03 - val_decoder_loss: -1.8761e-03 - val_mlp_out_loss: 7.7611e-04 - val_decoder_cosine: -5.2278e-02 - val_decoder_mae: 0.0820 - val_decoder_correlation: 1.0014 - val_decoder_rmse: 0.1362 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0279 - 44s/epoch - 769ms/step\n",
      "Epoch 80/200\n",
      "57/57 - 43s - loss: -2.0742e-01 - decoder_loss: -2.0794e-01 - mlp_out_loss: 5.2128e-04 - decoder_cosine: 0.4650 - decoder_mae: 0.0681 - decoder_correlation: 0.7922 - decoder_rmse: 0.1129 - mlp_out_cosine: 6.6900e-04 - mlp_out_mae: 0.0150 - mlp_out_correlation: nan - mlp_out_rmse: 0.0228 - val_loss: -2.2895e-02 - val_decoder_loss: -2.3675e-02 - val_mlp_out_loss: 7.7615e-04 - val_decoder_cosine: 0.0718 - val_decoder_mae: 0.0821 - val_decoder_correlation: 0.9840 - val_decoder_rmse: 0.1385 - val_mlp_out_cosine: 0.0037 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: nan - val_mlp_out_rmse: 0.0279 - 43s/epoch - 760ms/step\n",
      "Epoch 81/200\n",
      "57/57 - 44s - loss: -2.0659e-01 - decoder_loss: -2.0711e-01 - mlp_out_loss: 5.2126e-04 - decoder_cosine: 0.4642 - decoder_mae: 0.0678 - decoder_correlation: 0.7929 - decoder_rmse: 0.1124 - mlp_out_cosine: -1.8581e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: nan - mlp_out_rmse: 0.0228 - val_loss: -1.3689e-02 - val_decoder_loss: -1.4468e-02 - val_mlp_out_loss: 7.7617e-04 - val_decoder_cosine: -4.9321e-03 - val_decoder_mae: 0.0902 - val_decoder_correlation: 0.9836 - val_decoder_rmse: 0.1560 - val_mlp_out_cosine: 0.0037 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: nan - val_mlp_out_rmse: 0.0279 - 44s/epoch - 768ms/step\n",
      "Epoch 82/200\n",
      "57/57 - 44s - loss: -2.0781e-01 - decoder_loss: -2.0833e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.4702 - decoder_mae: 0.0673 - decoder_correlation: 0.7917 - decoder_rmse: 0.1120 - mlp_out_cosine: -1.6631e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: nan - mlp_out_rmse: 0.0228 - val_loss: -1.4593e-02 - val_decoder_loss: -1.5372e-02 - val_mlp_out_loss: 7.7610e-04 - val_decoder_cosine: 0.0335 - val_decoder_mae: 0.0812 - val_decoder_correlation: 0.9868 - val_decoder_rmse: 0.1397 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: nan - val_mlp_out_rmse: 0.0279 - 44s/epoch - 768ms/step\n",
      "Epoch 83/200\n",
      "57/57 - 43s - loss: -2.0734e-01 - decoder_loss: -2.0786e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.4567 - decoder_mae: 0.0675 - decoder_correlation: 0.7919 - decoder_rmse: 0.1123 - mlp_out_cosine: -1.4394e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: nan - mlp_out_rmse: 0.0228 - val_loss: -1.8043e-02 - val_decoder_loss: -1.8821e-02 - val_mlp_out_loss: 7.7611e-04 - val_decoder_cosine: 0.0404 - val_decoder_mae: 0.0824 - val_decoder_correlation: 0.9784 - val_decoder_rmse: 0.1419 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0279 - 43s/epoch - 759ms/step\n",
      "================================================================================================\n",
      ">>> AEMLP_FOLD:1\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_82), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(62, 1) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "Epoch 1/200\n",
      "230/230 - 8s - loss: 0.6423 - decoder_loss: -6.2033e-02 - mlp_out_loss: 0.0092 - decoder_cosine: 0.1451 - decoder_mae: 0.4507 - decoder_correlation: 0.9387 - decoder_rmse: 0.6551 - mlp_out_cosine: 0.0012 - mlp_out_mae: 0.0307 - mlp_out_correlation: 1.0019 - mlp_out_rmse: 0.0958 - val_loss: 0.2959 - val_decoder_loss: -1.2183e-02 - val_mlp_out_loss: 6.5837e-04 - val_decoder_cosine: -1.8484e-02 - val_decoder_mae: 0.3064 - val_decoder_correlation: 0.9710 - val_decoder_rmse: 0.3925 - val_mlp_out_cosine: -2.0192e-02 - val_mlp_out_mae: 0.0176 - val_mlp_out_correlation: 1.0110 - val_mlp_out_rmse: 0.0257 - 8s/epoch - 35ms/step\n",
      "Epoch 2/200\n",
      "230/230 - 6s - loss: 0.0390 - decoder_loss: -1.4013e-01 - mlp_out_loss: 7.5525e-04 - decoder_cosine: 0.2822 - decoder_mae: 0.3665 - decoder_correlation: 0.8603 - decoder_rmse: 0.5533 - mlp_out_cosine: 0.0095 - mlp_out_mae: 0.0180 - mlp_out_correlation: 1.0045 - mlp_out_rmse: 0.0275 - val_loss: 0.1022 - val_decoder_loss: 0.0020 - val_mlp_out_loss: 6.5365e-04 - val_decoder_cosine: -7.7834e-02 - val_decoder_mae: 0.2059 - val_decoder_correlation: 0.9856 - val_decoder_rmse: 0.2751 - val_mlp_out_cosine: -1.7383e-02 - val_mlp_out_mae: 0.0174 - val_mlp_out_correlation: 1.0074 - val_mlp_out_rmse: 0.0256 - 6s/epoch - 27ms/step\n",
      "Epoch 3/200\n",
      "230/230 - 6s - loss: -1.0066e-01 - decoder_loss: -1.6875e-01 - mlp_out_loss: 6.5295e-04 - decoder_cosine: 0.3152 - decoder_mae: 0.2648 - decoder_correlation: 0.8318 - decoder_rmse: 0.4151 - mlp_out_cosine: 0.0127 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9877 - mlp_out_rmse: 0.0256 - val_loss: 0.0379 - val_decoder_loss: -6.8989e-03 - val_mlp_out_loss: 6.3074e-04 - val_decoder_cosine: -3.6493e-02 - val_decoder_mae: 0.1805 - val_decoder_correlation: 0.9876 - val_decoder_rmse: 0.2466 - val_mlp_out_cosine: -1.2745e-02 - val_mlp_out_mae: 0.0172 - val_mlp_out_correlation: 1.0023 - val_mlp_out_rmse: 0.0251 - 6s/epoch - 26ms/step\n",
      "Epoch 4/200\n",
      "230/230 - 6s - loss: -1.5380e-01 - decoder_loss: -1.8578e-01 - mlp_out_loss: 6.3057e-04 - decoder_cosine: 0.3403 - decoder_mae: 0.2153 - decoder_correlation: 0.8148 - decoder_rmse: 0.3372 - mlp_out_cosine: 0.0164 - mlp_out_mae: 0.0167 - mlp_out_correlation: 0.9804 - mlp_out_rmse: 0.0251 - val_loss: 0.0160 - val_decoder_loss: -5.8161e-03 - val_mlp_out_loss: 6.2220e-04 - val_decoder_cosine: -4.1580e-02 - val_decoder_mae: 0.1675 - val_decoder_correlation: 0.9927 - val_decoder_rmse: 0.2350 - val_mlp_out_cosine: -1.3566e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 0.9968 - val_mlp_out_rmse: 0.0249 - 6s/epoch - 25ms/step\n",
      "Epoch 5/200\n",
      "230/230 - 6s - loss: -1.8416e-01 - decoder_loss: -1.9988e-01 - mlp_out_loss: 6.2747e-04 - decoder_cosine: 0.3603 - decoder_mae: 0.1775 - decoder_correlation: 0.8005 - decoder_rmse: 0.2791 - mlp_out_cosine: 0.0230 - mlp_out_mae: 0.0166 - mlp_out_correlation: 0.9760 - mlp_out_rmse: 0.0250 - val_loss: 0.0126 - val_decoder_loss: 0.0018 - val_mlp_out_loss: 6.2166e-04 - val_decoder_cosine: -5.6173e-02 - val_decoder_mae: 0.1325 - val_decoder_correlation: 0.9912 - val_decoder_rmse: 0.1856 - val_mlp_out_cosine: -1.9708e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 0.9960 - val_mlp_out_rmse: 0.0249 - 6s/epoch - 27ms/step\n",
      "Epoch 6/200\n",
      "230/230 - 6s - loss: -2.0249e-01 - decoder_loss: -2.1024e-01 - mlp_out_loss: 6.2764e-04 - decoder_cosine: 0.3660 - decoder_mae: 0.1436 - decoder_correlation: 0.7898 - decoder_rmse: 0.2275 - mlp_out_cosine: 0.0246 - mlp_out_mae: 0.0166 - mlp_out_correlation: 0.9739 - mlp_out_rmse: 0.0251 - val_loss: -7.4477e-03 - val_decoder_loss: -1.2770e-02 - val_mlp_out_loss: 6.2094e-04 - val_decoder_cosine: -3.0646e-02 - val_decoder_mae: 0.1161 - val_decoder_correlation: 0.9856 - val_decoder_rmse: 0.1636 - val_mlp_out_cosine: -2.0155e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 0.9930 - val_mlp_out_rmse: 0.0249 - 6s/epoch - 26ms/step\n",
      "Epoch 7/200\n",
      "230/230 - 6s - loss: -2.1280e-01 - decoder_loss: -2.1667e-01 - mlp_out_loss: 6.2779e-04 - decoder_cosine: 0.3859 - decoder_mae: 0.1266 - decoder_correlation: 0.7833 - decoder_rmse: 0.2019 - mlp_out_cosine: 0.0190 - mlp_out_mae: 0.0166 - mlp_out_correlation: 0.9705 - mlp_out_rmse: 0.0251 - val_loss: -1.9806e-02 - val_decoder_loss: -2.2510e-02 - val_mlp_out_loss: 6.2186e-04 - val_decoder_cosine: 0.0425 - val_decoder_mae: 0.1048 - val_decoder_correlation: 0.9822 - val_decoder_rmse: 0.1478 - val_mlp_out_cosine: -1.9856e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 0.9902 - val_mlp_out_rmse: 0.0249 - 6s/epoch - 26ms/step\n",
      "Epoch 8/200\n",
      "230/230 - 6s - loss: -2.2196e-01 - decoder_loss: -2.2399e-01 - mlp_out_loss: 6.2783e-04 - decoder_cosine: 0.4051 - decoder_mae: 0.1142 - decoder_correlation: 0.7763 - decoder_rmse: 0.1828 - mlp_out_cosine: 0.0204 - mlp_out_mae: 0.0166 - mlp_out_correlation: 0.9665 - mlp_out_rmse: 0.0251 - val_loss: -7.7013e-03 - val_decoder_loss: -9.1993e-03 - val_mlp_out_loss: 6.2087e-04 - val_decoder_cosine: -9.2047e-03 - val_decoder_mae: 0.0892 - val_decoder_correlation: 0.9880 - val_decoder_rmse: 0.1269 - val_mlp_out_cosine: -1.9839e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 0.9903 - val_mlp_out_rmse: 0.0249 - 6s/epoch - 27ms/step\n",
      "Epoch 9/200\n",
      "230/230 - 6s - loss: -2.2786e-01 - decoder_loss: -2.2907e-01 - mlp_out_loss: 6.2781e-04 - decoder_cosine: 0.4039 - decoder_mae: 0.1057 - decoder_correlation: 0.7712 - decoder_rmse: 0.1703 - mlp_out_cosine: 0.0218 - mlp_out_mae: 0.0166 - mlp_out_correlation: 0.9648 - mlp_out_rmse: 0.0251 - val_loss: -5.2723e-03 - val_decoder_loss: -6.2419e-03 - val_mlp_out_loss: 6.2114e-04 - val_decoder_cosine: -3.9719e-02 - val_decoder_mae: 0.0825 - val_decoder_correlation: 0.9846 - val_decoder_rmse: 0.1174 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 0.9889 - val_mlp_out_rmse: 0.0249 - 6s/epoch - 25ms/step\n",
      "Epoch 10/200\n",
      "230/230 - 5s - loss: -2.3310e-01 - decoder_loss: -2.3395e-01 - mlp_out_loss: 6.2783e-04 - decoder_cosine: 0.4279 - decoder_mae: 0.0992 - decoder_correlation: 0.7663 - decoder_rmse: 0.1605 - mlp_out_cosine: 0.0208 - mlp_out_mae: 0.0166 - mlp_out_correlation: 0.9626 - mlp_out_rmse: 0.0251 - val_loss: 0.0026 - val_decoder_loss: 0.0019 - val_mlp_out_loss: 6.2319e-04 - val_decoder_cosine: -6.1766e-02 - val_decoder_mae: 0.0815 - val_decoder_correlation: 0.9873 - val_decoder_rmse: 0.1149 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 0.9876 - val_mlp_out_rmse: 0.0250 - 5s/epoch - 23ms/step\n",
      "Epoch 11/200\n",
      "230/230 - 5s - loss: -2.3677e-01 - decoder_loss: -2.3747e-01 - mlp_out_loss: 6.2792e-04 - decoder_cosine: 0.4404 - decoder_mae: 0.0941 - decoder_correlation: 0.7625 - decoder_rmse: 0.1529 - mlp_out_cosine: 0.0140 - mlp_out_mae: 0.0166 - mlp_out_correlation: 0.9624 - mlp_out_rmse: 0.0251 - val_loss: -2.4050e-02 - val_decoder_loss: -2.4715e-02 - val_mlp_out_loss: 6.2145e-04 - val_decoder_cosine: 0.0394 - val_decoder_mae: 0.0765 - val_decoder_correlation: 0.9817 - val_decoder_rmse: 0.1091 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 0.9865 - val_mlp_out_rmse: 0.0249 - 5s/epoch - 23ms/step\n",
      "Epoch 12/200\n",
      "230/230 - 5s - loss: -2.4122e-01 - decoder_loss: -2.4187e-01 - mlp_out_loss: 6.2786e-04 - decoder_cosine: 0.4365 - decoder_mae: 0.0906 - decoder_correlation: 0.7583 - decoder_rmse: 0.1477 - mlp_out_cosine: 0.0219 - mlp_out_mae: 0.0166 - mlp_out_correlation: 0.9724 - mlp_out_rmse: 0.0251 - val_loss: -2.0440e-02 - val_decoder_loss: -2.1075e-02 - val_mlp_out_loss: 6.2162e-04 - val_decoder_cosine: 0.0739 - val_decoder_mae: 0.0689 - val_decoder_correlation: 0.9870 - val_decoder_rmse: 0.0992 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 0.9948 - val_mlp_out_rmse: 0.0249 - 5s/epoch - 24ms/step\n",
      "Epoch 13/200\n",
      "230/230 - 6s - loss: -2.4561e-01 - decoder_loss: -2.4624e-01 - mlp_out_loss: 6.2789e-04 - decoder_cosine: 0.4488 - decoder_mae: 0.0852 - decoder_correlation: 0.7538 - decoder_rmse: 0.1388 - mlp_out_cosine: 0.0176 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -1.0479e-02 - val_decoder_loss: -1.1104e-02 - val_mlp_out_loss: 6.2166e-04 - val_decoder_cosine: 0.0217 - val_decoder_mae: 0.0667 - val_decoder_correlation: 0.9906 - val_decoder_rmse: 0.0943 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: nan - val_mlp_out_rmse: 0.0249 - 6s/epoch - 26ms/step\n",
      "Epoch 14/200\n",
      "230/230 - 6s - loss: -2.4847e-01 - decoder_loss: -2.4910e-01 - mlp_out_loss: 6.2788e-04 - decoder_cosine: 0.4615 - decoder_mae: 0.0805 - decoder_correlation: 0.7511 - decoder_rmse: 0.1318 - mlp_out_cosine: 0.0149 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -6.3091e-03 - val_decoder_loss: -6.9311e-03 - val_mlp_out_loss: 6.2103e-04 - val_decoder_cosine: -3.0604e-02 - val_decoder_mae: 0.0667 - val_decoder_correlation: 0.9860 - val_decoder_rmse: 0.0936 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 6s/epoch - 26ms/step\n",
      "Epoch 15/200\n",
      "230/230 - 6s - loss: -2.5161e-01 - decoder_loss: -2.5224e-01 - mlp_out_loss: 6.2786e-04 - decoder_cosine: 0.4651 - decoder_mae: 0.0767 - decoder_correlation: 0.7478 - decoder_rmse: 0.1257 - mlp_out_cosine: 0.0147 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -1.0778e-02 - val_decoder_loss: -1.1401e-02 - val_mlp_out_loss: 6.2308e-04 - val_decoder_cosine: 0.0507 - val_decoder_mae: 0.0610 - val_decoder_correlation: 0.9927 - val_decoder_rmse: 0.0870 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0250 - 6s/epoch - 25ms/step\n",
      "Epoch 16/200\n",
      "230/230 - 6s - loss: -2.5673e-01 - decoder_loss: -2.5736e-01 - mlp_out_loss: 6.2787e-04 - decoder_cosine: 0.4794 - decoder_mae: 0.0724 - decoder_correlation: 0.7429 - decoder_rmse: 0.1190 - mlp_out_cosine: 0.0155 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -3.0359e-02 - val_decoder_loss: -3.0982e-02 - val_mlp_out_loss: 6.2332e-04 - val_decoder_cosine: 0.1439 - val_decoder_mae: 0.0577 - val_decoder_correlation: 0.9819 - val_decoder_rmse: 0.0826 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0250 - 6s/epoch - 25ms/step\n",
      "Epoch 17/200\n",
      "230/230 - 6s - loss: -2.5820e-01 - decoder_loss: -2.5883e-01 - mlp_out_loss: 6.2793e-04 - decoder_cosine: 0.4822 - decoder_mae: 0.0690 - decoder_correlation: 0.7413 - decoder_rmse: 0.1136 - mlp_out_cosine: 0.0162 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -1.1361e-02 - val_decoder_loss: -1.1983e-02 - val_mlp_out_loss: 6.2253e-04 - val_decoder_cosine: -2.3992e-02 - val_decoder_mae: 0.0578 - val_decoder_correlation: 0.9769 - val_decoder_rmse: 0.0826 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: nan - val_mlp_out_rmse: 0.0250 - 6s/epoch - 26ms/step\n",
      "Epoch 18/200\n",
      "230/230 - 6s - loss: -2.6159e-01 - decoder_loss: -2.6221e-01 - mlp_out_loss: 6.2789e-04 - decoder_cosine: 0.4897 - decoder_mae: 0.0662 - decoder_correlation: 0.7381 - decoder_rmse: 0.1091 - mlp_out_cosine: 0.0183 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -1.7754e-02 - val_decoder_loss: -1.8376e-02 - val_mlp_out_loss: 6.2162e-04 - val_decoder_cosine: 0.0432 - val_decoder_mae: 0.0518 - val_decoder_correlation: 0.9802 - val_decoder_rmse: 0.0747 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 6s/epoch - 27ms/step\n",
      "Epoch 19/200\n",
      "230/230 - 6s - loss: -2.6461e-01 - decoder_loss: -2.6524e-01 - mlp_out_loss: 6.2789e-04 - decoder_cosine: 0.4999 - decoder_mae: 0.0650 - decoder_correlation: 0.7347 - decoder_rmse: 0.1075 - mlp_out_cosine: 0.0192 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -1.4481e-02 - val_decoder_loss: -1.5105e-02 - val_mlp_out_loss: 6.2433e-04 - val_decoder_cosine: 0.0630 - val_decoder_mae: 0.0518 - val_decoder_correlation: 0.9863 - val_decoder_rmse: 0.0747 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0171 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0250 - 6s/epoch - 25ms/step\n",
      "Epoch 20/200\n",
      "230/230 - 6s - loss: -2.6709e-01 - decoder_loss: -2.6772e-01 - mlp_out_loss: 6.2788e-04 - decoder_cosine: 0.5075 - decoder_mae: 0.0627 - decoder_correlation: 0.7324 - decoder_rmse: 0.1039 - mlp_out_cosine: 0.0161 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -1.5886e-02 - val_decoder_loss: -1.6505e-02 - val_mlp_out_loss: 6.1898e-04 - val_decoder_cosine: 0.0344 - val_decoder_mae: 0.0527 - val_decoder_correlation: 0.9811 - val_decoder_rmse: 0.0749 - val_mlp_out_cosine: 0.0198 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: nan - val_mlp_out_rmse: 0.0249 - 6s/epoch - 25ms/step\n",
      "Epoch 21/200\n",
      "230/230 - 6s - loss: -2.7002e-01 - decoder_loss: -2.7064e-01 - mlp_out_loss: 6.2803e-04 - decoder_cosine: 0.5146 - decoder_mae: 0.0618 - decoder_correlation: 0.7296 - decoder_rmse: 0.1025 - mlp_out_cosine: 0.0142 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -2.0301e-02 - val_decoder_loss: -2.0920e-02 - val_mlp_out_loss: 6.1931e-04 - val_decoder_cosine: 0.0389 - val_decoder_mae: 0.0503 - val_decoder_correlation: 0.9758 - val_decoder_rmse: 0.0734 - val_mlp_out_cosine: 0.0198 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 6s/epoch - 26ms/step\n",
      "Epoch 22/200\n",
      "230/230 - 6s - loss: -2.7175e-01 - decoder_loss: -2.7238e-01 - mlp_out_loss: 6.2806e-04 - decoder_cosine: 0.5136 - decoder_mae: 0.0599 - decoder_correlation: 0.7282 - decoder_rmse: 0.1000 - mlp_out_cosine: 0.0122 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -3.6282e-03 - val_decoder_loss: -4.2528e-03 - val_mlp_out_loss: 6.2456e-04 - val_decoder_cosine: -2.6356e-04 - val_decoder_mae: 0.0505 - val_decoder_correlation: 0.9877 - val_decoder_rmse: 0.0733 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0171 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0250 - 6s/epoch - 27ms/step\n",
      "Epoch 23/200\n",
      "230/230 - 6s - loss: -2.7341e-01 - decoder_loss: -2.7404e-01 - mlp_out_loss: 6.2792e-04 - decoder_cosine: 0.5247 - decoder_mae: 0.0594 - decoder_correlation: 0.7261 - decoder_rmse: 0.0993 - mlp_out_cosine: 0.0140 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -9.9887e-03 - val_decoder_loss: -1.0607e-02 - val_mlp_out_loss: 6.1877e-04 - val_decoder_cosine: 0.0536 - val_decoder_mae: 0.0494 - val_decoder_correlation: 0.9863 - val_decoder_rmse: 0.0723 - val_mlp_out_cosine: 0.0198 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 6s/epoch - 27ms/step\n",
      "Epoch 24/200\n",
      "230/230 - 6s - loss: -2.7633e-01 - decoder_loss: -2.7696e-01 - mlp_out_loss: 6.2807e-04 - decoder_cosine: 0.5217 - decoder_mae: 0.0585 - decoder_correlation: 0.7230 - decoder_rmse: 0.0982 - mlp_out_cosine: 0.0101 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -2.5866e-02 - val_decoder_loss: -2.6493e-02 - val_mlp_out_loss: 6.2654e-04 - val_decoder_cosine: 0.1533 - val_decoder_mae: 0.0472 - val_decoder_correlation: 0.9776 - val_decoder_rmse: 0.0688 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0171 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0250 - 6s/epoch - 26ms/step\n",
      "Epoch 25/200\n",
      "230/230 - 6s - loss: -2.7850e-01 - decoder_loss: -2.7913e-01 - mlp_out_loss: 6.2817e-04 - decoder_cosine: 0.5305 - decoder_mae: 0.0577 - decoder_correlation: 0.7209 - decoder_rmse: 0.0971 - mlp_out_cosine: 0.0102 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -3.5701e-02 - val_decoder_loss: -3.6321e-02 - val_mlp_out_loss: 6.2032e-04 - val_decoder_cosine: 0.1616 - val_decoder_mae: 0.0485 - val_decoder_correlation: 0.9818 - val_decoder_rmse: 0.0706 - val_mlp_out_cosine: 0.0198 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 6s/epoch - 27ms/step\n",
      "Epoch 26/200\n",
      "230/230 - 6s - loss: -2.8155e-01 - decoder_loss: -2.8218e-01 - mlp_out_loss: 6.2799e-04 - decoder_cosine: 0.5291 - decoder_mae: 0.0556 - decoder_correlation: 0.7178 - decoder_rmse: 0.0943 - mlp_out_cosine: 0.0139 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -9.5549e-03 - val_decoder_loss: -1.0181e-02 - val_mlp_out_loss: 6.2583e-04 - val_decoder_cosine: 1.9235e-04 - val_decoder_mae: 0.0489 - val_decoder_correlation: 0.9796 - val_decoder_rmse: 0.0714 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0171 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0250 - 6s/epoch - 27ms/step\n",
      "Epoch 27/200\n",
      "230/230 - 6s - loss: -2.8452e-01 - decoder_loss: -2.8515e-01 - mlp_out_loss: 6.2813e-04 - decoder_cosine: 0.5430 - decoder_mae: 0.0547 - decoder_correlation: 0.7147 - decoder_rmse: 0.0934 - mlp_out_cosine: 0.0117 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -2.5728e-02 - val_decoder_loss: -2.6356e-02 - val_mlp_out_loss: 6.2735e-04 - val_decoder_cosine: 0.0982 - val_decoder_mae: 0.0452 - val_decoder_correlation: 0.9780 - val_decoder_rmse: 0.0657 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0171 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0250 - 6s/epoch - 26ms/step\n",
      "Epoch 28/200\n",
      "230/230 - 6s - loss: -2.8835e-01 - decoder_loss: -2.8898e-01 - mlp_out_loss: 6.2817e-04 - decoder_cosine: 0.5489 - decoder_mae: 0.0524 - decoder_correlation: 0.7111 - decoder_rmse: 0.0905 - mlp_out_cosine: 0.0108 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -2.9485e-02 - val_decoder_loss: -3.0106e-02 - val_mlp_out_loss: 6.2057e-04 - val_decoder_cosine: 0.1001 - val_decoder_mae: 0.0436 - val_decoder_correlation: 0.9769 - val_decoder_rmse: 0.0638 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 6s/epoch - 26ms/step\n",
      "Epoch 29/200\n",
      "230/230 - 6s - loss: -2.9150e-01 - decoder_loss: -2.9212e-01 - mlp_out_loss: 6.2810e-04 - decoder_cosine: 0.5526 - decoder_mae: 0.0518 - decoder_correlation: 0.7078 - decoder_rmse: 0.0901 - mlp_out_cosine: 0.0116 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -2.0040e-02 - val_decoder_loss: -2.0664e-02 - val_mlp_out_loss: 6.2410e-04 - val_decoder_cosine: 0.0432 - val_decoder_mae: 0.0436 - val_decoder_correlation: 0.9735 - val_decoder_rmse: 0.0642 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0171 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0250 - 6s/epoch - 26ms/step\n",
      "Epoch 30/200\n",
      "230/230 - 6s - loss: -2.9308e-01 - decoder_loss: -2.9371e-01 - mlp_out_loss: 6.2816e-04 - decoder_cosine: 0.5581 - decoder_mae: 0.0512 - decoder_correlation: 0.7064 - decoder_rmse: 0.0897 - mlp_out_cosine: 0.0105 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -4.2746e-02 - val_decoder_loss: -4.3366e-02 - val_mlp_out_loss: 6.2009e-04 - val_decoder_cosine: 0.1697 - val_decoder_mae: 0.0434 - val_decoder_correlation: 0.9739 - val_decoder_rmse: 0.0630 - val_mlp_out_cosine: 0.0198 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 6s/epoch - 27ms/step\n",
      "Epoch 31/200\n",
      "230/230 - 6s - loss: -2.9606e-01 - decoder_loss: -2.9669e-01 - mlp_out_loss: 6.2820e-04 - decoder_cosine: 0.5635 - decoder_mae: 0.0512 - decoder_correlation: 0.7033 - decoder_rmse: 0.0900 - mlp_out_cosine: 0.0075 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -2.0994e-02 - val_decoder_loss: -2.1614e-02 - val_mlp_out_loss: 6.2077e-04 - val_decoder_cosine: 0.0878 - val_decoder_mae: 0.0418 - val_decoder_correlation: 0.9740 - val_decoder_rmse: 0.0623 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 6s/epoch - 26ms/step\n",
      "Epoch 32/200\n",
      "230/230 - 6s - loss: -2.9720e-01 - decoder_loss: -2.9783e-01 - mlp_out_loss: 6.2812e-04 - decoder_cosine: 0.5590 - decoder_mae: 0.0507 - decoder_correlation: 0.7022 - decoder_rmse: 0.0894 - mlp_out_cosine: 0.0108 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -4.0434e-02 - val_decoder_loss: -4.1053e-02 - val_mlp_out_loss: 6.1965e-04 - val_decoder_cosine: 0.1431 - val_decoder_mae: 0.0405 - val_decoder_correlation: 0.9692 - val_decoder_rmse: 0.0598 - val_mlp_out_cosine: 0.0198 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 6s/epoch - 27ms/step\n",
      "Epoch 33/200\n",
      "230/230 - 6s - loss: -2.9826e-01 - decoder_loss: -2.9889e-01 - mlp_out_loss: 6.2801e-04 - decoder_cosine: 0.5743 - decoder_mae: 0.0508 - decoder_correlation: 0.7011 - decoder_rmse: 0.0896 - mlp_out_cosine: 0.0134 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -2.3497e-02 - val_decoder_loss: -2.4119e-02 - val_mlp_out_loss: 6.2249e-04 - val_decoder_cosine: 0.0284 - val_decoder_mae: 0.0425 - val_decoder_correlation: 0.9718 - val_decoder_rmse: 0.0623 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 6s/epoch - 27ms/step\n",
      "Epoch 34/200\n",
      "230/230 - 6s - loss: -3.0022e-01 - decoder_loss: -3.0085e-01 - mlp_out_loss: 6.2815e-04 - decoder_cosine: 0.5776 - decoder_mae: 0.0507 - decoder_correlation: 0.6993 - decoder_rmse: 0.0899 - mlp_out_cosine: 0.0125 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -3.2694e-02 - val_decoder_loss: -3.3313e-02 - val_mlp_out_loss: 6.1947e-04 - val_decoder_cosine: 0.0840 - val_decoder_mae: 0.0426 - val_decoder_correlation: 0.9652 - val_decoder_rmse: 0.0625 - val_mlp_out_cosine: 0.0198 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 6s/epoch - 26ms/step\n",
      "Epoch 35/200\n",
      "230/230 - 6s - loss: -3.0185e-01 - decoder_loss: -3.0248e-01 - mlp_out_loss: 6.2828e-04 - decoder_cosine: 0.5722 - decoder_mae: 0.0505 - decoder_correlation: 0.6975 - decoder_rmse: 0.0897 - mlp_out_cosine: 0.0066 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -2.9801e-02 - val_decoder_loss: -3.0422e-02 - val_mlp_out_loss: 6.2138e-04 - val_decoder_cosine: 0.0864 - val_decoder_mae: 0.0416 - val_decoder_correlation: 0.9713 - val_decoder_rmse: 0.0605 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: nan - val_mlp_out_rmse: 0.0249 - 6s/epoch - 25ms/step\n",
      "Epoch 36/200\n",
      "230/230 - 6s - loss: -3.0274e-01 - decoder_loss: -3.0336e-01 - mlp_out_loss: 6.2828e-04 - decoder_cosine: 0.5801 - decoder_mae: 0.0506 - decoder_correlation: 0.6967 - decoder_rmse: 0.0900 - mlp_out_cosine: 0.0079 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -3.5529e-02 - val_decoder_loss: -3.6153e-02 - val_mlp_out_loss: 6.2384e-04 - val_decoder_cosine: 0.1363 - val_decoder_mae: 0.0411 - val_decoder_correlation: 0.9719 - val_decoder_rmse: 0.0596 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0171 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0250 - 6s/epoch - 26ms/step\n",
      "Epoch 37/200\n",
      "230/230 - 6s - loss: -3.0352e-01 - decoder_loss: -3.0415e-01 - mlp_out_loss: 6.2808e-04 - decoder_cosine: 0.5889 - decoder_mae: 0.0502 - decoder_correlation: 0.6961 - decoder_rmse: 0.0894 - mlp_out_cosine: 0.0123 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -2.8787e-02 - val_decoder_loss: -2.9410e-02 - val_mlp_out_loss: 6.2281e-04 - val_decoder_cosine: 0.1086 - val_decoder_mae: 0.0412 - val_decoder_correlation: 0.9753 - val_decoder_rmse: 0.0613 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0250 - 6s/epoch - 26ms/step\n",
      "Epoch 38/200\n",
      "230/230 - 7s - loss: -3.0538e-01 - decoder_loss: -3.0600e-01 - mlp_out_loss: 6.2839e-04 - decoder_cosine: 0.5899 - decoder_mae: 0.0505 - decoder_correlation: 0.6941 - decoder_rmse: 0.0902 - mlp_out_cosine: 0.0071 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -2.4826e-02 - val_decoder_loss: -2.5445e-02 - val_mlp_out_loss: 6.1972e-04 - val_decoder_cosine: 0.1059 - val_decoder_mae: 0.0412 - val_decoder_correlation: 0.9759 - val_decoder_rmse: 0.0606 - val_mlp_out_cosine: 0.0198 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 7s/epoch - 30ms/step\n",
      "================================================================================================\n",
      ">>> AEMLP_FOLD:2\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_83), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(62, 1) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "Epoch 1/200\n",
      "30/30 - 10s - loss: 1.4701 - decoder_loss: -1.8182e-02 - mlp_out_loss: 0.0031 - decoder_cosine: 0.0298 - decoder_mae: 0.4110 - decoder_correlation: 0.9846 - decoder_rmse: 0.6288 - mlp_out_cosine: 0.0021 - mlp_out_mae: 0.0308 - mlp_out_correlation: 0.9891 - mlp_out_rmse: 0.0554 - val_loss: 1.1566 - val_decoder_loss: -8.3935e-03 - val_mlp_out_loss: 8.3871e-04 - val_decoder_cosine: 0.0564 - val_decoder_mae: 0.2809 - val_decoder_correlation: 0.9862 - val_decoder_rmse: 0.5407 - val_mlp_out_cosine: 0.0037 - val_mlp_out_mae: 0.0166 - val_mlp_out_correlation: 0.9963 - val_mlp_out_rmse: 0.0290 - 10s/epoch - 336ms/step\n",
      "Epoch 2/200\n",
      "30/30 - 7s - loss: 0.8828 - decoder_loss: -6.3438e-02 - mlp_out_loss: 7.5499e-04 - decoder_cosine: 0.0914 - decoder_mae: 0.4679 - decoder_correlation: 0.9395 - decoder_rmse: 0.7176 - mlp_out_cosine: 0.0111 - mlp_out_mae: 0.0189 - mlp_out_correlation: 0.9643 - mlp_out_rmse: 0.0275 - val_loss: 0.7140 - val_decoder_loss: -1.7721e-02 - val_mlp_out_loss: 5.0121e-04 - val_decoder_cosine: 0.1242 - val_decoder_mae: 0.2464 - val_decoder_correlation: 0.9672 - val_decoder_rmse: 0.3698 - val_mlp_out_cosine: -1.0795e-02 - val_mlp_out_mae: 0.0151 - val_mlp_out_correlation: 0.9929 - val_mlp_out_rmse: 0.0224 - 7s/epoch - 249ms/step\n",
      "Epoch 3/200\n",
      "30/30 - 7s - loss: 0.4961 - decoder_loss: -9.6727e-02 - mlp_out_loss: 6.7313e-04 - decoder_cosine: 0.1570 - decoder_mae: 0.5380 - decoder_correlation: 0.9050 - decoder_rmse: 0.8138 - mlp_out_cosine: 0.0189 - mlp_out_mae: 0.0176 - mlp_out_correlation: 0.9474 - mlp_out_rmse: 0.0259 - val_loss: 0.4318 - val_decoder_loss: -2.5851e-02 - val_mlp_out_loss: 4.6216e-04 - val_decoder_cosine: 0.1418 - val_decoder_mae: 0.2751 - val_decoder_correlation: 0.9642 - val_decoder_rmse: 0.3712 - val_mlp_out_cosine: 1.5981e-05 - val_mlp_out_mae: 0.0149 - val_mlp_out_correlation: 0.9878 - val_mlp_out_rmse: 0.0215 - 7s/epoch - 240ms/step\n",
      "Epoch 4/200\n",
      "30/30 - 7s - loss: 0.2619 - decoder_loss: -1.0912e-01 - mlp_out_loss: 6.4791e-04 - decoder_cosine: 0.1824 - decoder_mae: 0.5440 - decoder_correlation: 0.8935 - decoder_rmse: 0.8262 - mlp_out_cosine: 0.0231 - mlp_out_mae: 0.0171 - mlp_out_correlation: 0.9320 - mlp_out_rmse: 0.0255 - val_loss: 0.2536 - val_decoder_loss: -3.3228e-02 - val_mlp_out_loss: 4.5726e-04 - val_decoder_cosine: 0.1461 - val_decoder_mae: 0.2552 - val_decoder_correlation: 0.9619 - val_decoder_rmse: 0.3509 - val_mlp_out_cosine: 0.0080 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 0.9809 - val_mlp_out_rmse: 0.0214 - 7s/epoch - 241ms/step\n",
      "Epoch 5/200\n",
      "30/30 - 7s - loss: 0.1164 - decoder_loss: -1.1681e-01 - mlp_out_loss: 6.4337e-04 - decoder_cosine: 0.1948 - decoder_mae: 0.4938 - decoder_correlation: 0.8848 - decoder_rmse: 0.7617 - mlp_out_cosine: 0.0211 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9286 - mlp_out_rmse: 0.0254 - val_loss: 0.1473 - val_decoder_loss: -3.3529e-02 - val_mlp_out_loss: 4.5668e-04 - val_decoder_cosine: 0.1391 - val_decoder_mae: 0.2259 - val_decoder_correlation: 0.9631 - val_decoder_rmse: 0.3171 - val_mlp_out_cosine: 0.0041 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 0.9826 - val_mlp_out_rmse: 0.0214 - 7s/epoch - 246ms/step\n",
      "Epoch 6/200\n",
      "30/30 - 7s - loss: 0.0245 - decoder_loss: -1.2313e-01 - mlp_out_loss: 6.4425e-04 - decoder_cosine: 0.2010 - decoder_mae: 0.4253 - decoder_correlation: 0.8789 - decoder_rmse: 0.6579 - mlp_out_cosine: 0.0120 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9340 - mlp_out_rmse: 0.0254 - val_loss: 0.0815 - val_decoder_loss: -3.3469e-02 - val_mlp_out_loss: 4.5665e-04 - val_decoder_cosine: 0.1310 - val_decoder_mae: 0.1948 - val_decoder_correlation: 0.9655 - val_decoder_rmse: 0.2742 - val_mlp_out_cosine: -3.7143e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 0.9878 - val_mlp_out_rmse: 0.0214 - 7s/epoch - 247ms/step\n",
      "Epoch 7/200\n",
      "30/30 - 7s - loss: -3.5729e-02 - decoder_loss: -1.3002e-01 - mlp_out_loss: 6.4549e-04 - decoder_cosine: 0.2132 - decoder_mae: 0.3509 - decoder_correlation: 0.8688 - decoder_rmse: 0.5482 - mlp_out_cosine: -4.2544e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9365 - mlp_out_rmse: 0.0254 - val_loss: 0.0386 - val_decoder_loss: -3.5162e-02 - val_mlp_out_loss: 4.5667e-04 - val_decoder_cosine: 0.1460 - val_decoder_mae: 0.1719 - val_decoder_correlation: 0.9638 - val_decoder_rmse: 0.2392 - val_mlp_out_cosine: -3.8057e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 0.9908 - val_mlp_out_rmse: 0.0214 - 7s/epoch - 240ms/step\n",
      "Epoch 8/200\n",
      "30/30 - 7s - loss: -7.6026e-02 - decoder_loss: -1.3673e-01 - mlp_out_loss: 6.4614e-04 - decoder_cosine: 0.2216 - decoder_mae: 0.2945 - decoder_correlation: 0.8639 - decoder_rmse: 0.4614 - mlp_out_cosine: -5.8023e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9443 - mlp_out_rmse: 0.0254 - val_loss: 0.0163 - val_decoder_loss: -3.1233e-02 - val_mlp_out_loss: 4.5671e-04 - val_decoder_cosine: 0.1460 - val_decoder_mae: 0.1398 - val_decoder_correlation: 0.9674 - val_decoder_rmse: 0.1904 - val_mlp_out_cosine: -3.8168e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 0.9950 - val_mlp_out_rmse: 0.0214 - 7s/epoch - 243ms/step\n",
      "Epoch 9/200\n",
      "30/30 - 7s - loss: -1.0469e-01 - decoder_loss: -1.4398e-01 - mlp_out_loss: 6.4642e-04 - decoder_cosine: 0.2312 - decoder_mae: 0.2477 - decoder_correlation: 0.8576 - decoder_rmse: 0.3884 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9519 - mlp_out_rmse: 0.0254 - val_loss: -8.4565e-03 - val_decoder_loss: -3.9242e-02 - val_mlp_out_loss: 4.5670e-04 - val_decoder_cosine: 0.1857 - val_decoder_mae: 0.1302 - val_decoder_correlation: 0.9596 - val_decoder_rmse: 0.1753 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 0.9974 - val_mlp_out_rmse: 0.0214 - 7s/epoch - 238ms/step\n",
      "Epoch 10/200\n",
      "30/30 - 7s - loss: -1.2468e-01 - decoder_loss: -1.5017e-01 - mlp_out_loss: 6.4650e-04 - decoder_cosine: 0.2387 - decoder_mae: 0.2169 - decoder_correlation: 0.8495 - decoder_rmse: 0.3365 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9560 - mlp_out_rmse: 0.0254 - val_loss: -1.5402e-02 - val_decoder_loss: -3.5315e-02 - val_mlp_out_loss: 4.5670e-04 - val_decoder_cosine: 0.1771 - val_decoder_mae: 0.1057 - val_decoder_correlation: 0.9641 - val_decoder_rmse: 0.1414 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0006 - val_mlp_out_rmse: 0.0214 - 7s/epoch - 245ms/step\n",
      "Epoch 11/200\n",
      "30/30 - 7s - loss: -1.3685e-01 - decoder_loss: -1.5339e-01 - mlp_out_loss: 6.4652e-04 - decoder_cosine: 0.2442 - decoder_mae: 0.1922 - decoder_correlation: 0.8452 - decoder_rmse: 0.2958 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9629 - mlp_out_rmse: 0.0254 - val_loss: -2.6283e-02 - val_decoder_loss: -3.9130e-02 - val_mlp_out_loss: 4.5673e-04 - val_decoder_cosine: 0.1847 - val_decoder_mae: 0.0974 - val_decoder_correlation: 0.9612 - val_decoder_rmse: 0.1308 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0028 - val_mlp_out_rmse: 0.0214 - 7s/epoch - 243ms/step\n",
      "Epoch 12/200\n",
      "30/30 - 7s - loss: -1.4754e-01 - decoder_loss: -1.5826e-01 - mlp_out_loss: 6.4653e-04 - decoder_cosine: 0.2436 - decoder_mae: 0.1758 - decoder_correlation: 0.8422 - decoder_rmse: 0.2745 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9698 - mlp_out_rmse: 0.0254 - val_loss: -3.0160e-02 - val_decoder_loss: -3.8425e-02 - val_mlp_out_loss: 4.5670e-04 - val_decoder_cosine: 0.1929 - val_decoder_mae: 0.0999 - val_decoder_correlation: 0.9609 - val_decoder_rmse: 0.1330 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0047 - val_mlp_out_rmse: 0.0214 - 7s/epoch - 244ms/step\n",
      "Epoch 13/200\n",
      "30/30 - 7s - loss: -1.5399e-01 - decoder_loss: -1.6095e-01 - mlp_out_loss: 6.4655e-04 - decoder_cosine: 0.2433 - decoder_mae: 0.1652 - decoder_correlation: 0.8411 - decoder_rmse: 0.2576 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9792 - mlp_out_rmse: 0.0254 - val_loss: -3.5279e-02 - val_decoder_loss: -4.0594e-02 - val_mlp_out_loss: 4.5670e-04 - val_decoder_cosine: 0.1911 - val_decoder_mae: 0.0881 - val_decoder_correlation: 0.9625 - val_decoder_rmse: 0.1211 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0073 - val_mlp_out_rmse: 0.0214 - 7s/epoch - 248ms/step\n",
      "Epoch 14/200\n",
      "30/30 - 8s - loss: -1.6100e-01 - decoder_loss: -1.6555e-01 - mlp_out_loss: 6.4653e-04 - decoder_cosine: 0.2510 - decoder_mae: 0.1565 - decoder_correlation: 0.8362 - decoder_rmse: 0.2429 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9881 - mlp_out_rmse: 0.0254 - val_loss: -3.7110e-02 - val_decoder_loss: -4.0546e-02 - val_mlp_out_loss: 4.5671e-04 - val_decoder_cosine: 0.1778 - val_decoder_mae: 0.0765 - val_decoder_correlation: 0.9647 - val_decoder_rmse: 0.1062 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0094 - val_mlp_out_rmse: 0.0214 - 8s/epoch - 276ms/step\n",
      "Epoch 15/200\n",
      "30/30 - 7s - loss: -1.6668e-01 - decoder_loss: -1.6971e-01 - mlp_out_loss: 6.4653e-04 - decoder_cosine: 0.2565 - decoder_mae: 0.1408 - decoder_correlation: 0.8309 - decoder_rmse: 0.2209 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9995 - mlp_out_rmse: 0.0254 - val_loss: -3.7512e-02 - val_decoder_loss: -3.9769e-02 - val_mlp_out_loss: 4.5674e-04 - val_decoder_cosine: 0.1753 - val_decoder_mae: 0.0714 - val_decoder_correlation: 0.9657 - val_decoder_rmse: 0.0990 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0126 - val_mlp_out_rmse: 0.0214 - 7s/epoch - 249ms/step\n",
      "Epoch 16/200\n",
      "30/30 - 7s - loss: -1.7059e-01 - decoder_loss: -1.7267e-01 - mlp_out_loss: 6.4653e-04 - decoder_cosine: 0.2484 - decoder_mae: 0.1249 - decoder_correlation: 0.8280 - decoder_rmse: 0.1959 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 1.0102 - mlp_out_rmse: 0.0254 - val_loss: -3.8232e-02 - val_decoder_loss: -3.9759e-02 - val_mlp_out_loss: 4.5671e-04 - val_decoder_cosine: 0.1853 - val_decoder_mae: 0.0660 - val_decoder_correlation: 0.9655 - val_decoder_rmse: 0.0921 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0100 - val_mlp_out_rmse: 0.0214 - 7s/epoch - 243ms/step\n",
      "Epoch 17/200\n",
      "30/30 - 7s - loss: -1.7280e-01 - decoder_loss: -1.7429e-01 - mlp_out_loss: 6.4654e-04 - decoder_cosine: 0.2551 - decoder_mae: 0.1100 - decoder_correlation: 0.8290 - decoder_rmse: 0.1725 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 1.0136 - mlp_out_rmse: 0.0254 - val_loss: -3.5222e-02 - val_decoder_loss: -3.6304e-02 - val_mlp_out_loss: 4.5670e-04 - val_decoder_cosine: 0.1554 - val_decoder_mae: 0.0572 - val_decoder_correlation: 0.9724 - val_decoder_rmse: 0.0804 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0110 - val_mlp_out_rmse: 0.0214 - 7s/epoch - 240ms/step\n",
      "Epoch 18/200\n",
      "30/30 - 7s - loss: -1.7670e-01 - decoder_loss: -1.7783e-01 - mlp_out_loss: 6.4653e-04 - decoder_cosine: 0.2569 - decoder_mae: 0.1023 - decoder_correlation: 0.8239 - decoder_rmse: 0.1614 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 1.0087 - mlp_out_rmse: 0.0254 - val_loss: -3.5265e-02 - val_decoder_loss: -3.6081e-02 - val_mlp_out_loss: 4.5672e-04 - val_decoder_cosine: 0.1453 - val_decoder_mae: 0.0545 - val_decoder_correlation: 0.9733 - val_decoder_rmse: 0.0765 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0012 - val_mlp_out_rmse: 0.0214 - 7s/epoch - 236ms/step\n",
      "Epoch 19/200\n",
      "30/30 - 7s - loss: -1.7950e-01 - decoder_loss: -1.8043e-01 - mlp_out_loss: 6.4655e-04 - decoder_cosine: 0.2518 - decoder_mae: 0.0926 - decoder_correlation: 0.8206 - decoder_rmse: 0.1474 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 1.0019 - mlp_out_rmse: 0.0254 - val_loss: -3.3017e-02 - val_decoder_loss: -3.3677e-02 - val_mlp_out_loss: 4.5674e-04 - val_decoder_cosine: 0.1299 - val_decoder_mae: 0.0495 - val_decoder_correlation: 0.9758 - val_decoder_rmse: 0.0700 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0013 - val_mlp_out_rmse: 0.0214 - 7s/epoch - 248ms/step\n",
      "Epoch 20/200\n",
      "30/30 - 7s - loss: -1.8206e-01 - decoder_loss: -1.8286e-01 - mlp_out_loss: 6.4656e-04 - decoder_cosine: 0.2603 - decoder_mae: 0.0846 - decoder_correlation: 0.8168 - decoder_rmse: 0.1343 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 1.0007 - mlp_out_rmse: 0.0254 - val_loss: -3.4244e-02 - val_decoder_loss: -3.4813e-02 - val_mlp_out_loss: 4.5676e-04 - val_decoder_cosine: 0.1646 - val_decoder_mae: 0.0492 - val_decoder_correlation: 0.9696 - val_decoder_rmse: 0.0694 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0214 - 7s/epoch - 239ms/step\n",
      "Epoch 21/200\n",
      "30/30 - 7s - loss: -1.8398e-01 - decoder_loss: -1.8472e-01 - mlp_out_loss: 6.4654e-04 - decoder_cosine: 0.2548 - decoder_mae: 0.0810 - decoder_correlation: 0.8148 - decoder_rmse: 0.1283 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 1.0001 - mlp_out_rmse: 0.0254 - val_loss: -2.4834e-02 - val_decoder_loss: -2.5353e-02 - val_mlp_out_loss: 4.5681e-04 - val_decoder_cosine: 0.1196 - val_decoder_mae: 0.0456 - val_decoder_correlation: 0.9822 - val_decoder_rmse: 0.0664 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0214 - 7s/epoch - 241ms/step\n",
      "================================================================================================\n"
     ]
    }
   ],
   "source": [
    "gkf = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, \n",
    "                                 group_gap = GROUP_GAP, \n",
    "                                 max_train_group_size = MAX_TRAIN_GROUP_SIZE, \n",
    "                                 max_test_group_size  = MAX_TEST_GROUP_SIZE).split(X, y, groups[0])\n",
    "models = []\n",
    "for fold, (train_idx, val_idx) in enumerate(list(gkf)):\n",
    "    x_train, x_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    print(f'>>> AEMLP_FOLD:{fold}')\n",
    "    K.clear_session()\n",
    "    with strategy.scope(): model = build_model(hp, dim = x_train.shape[1], fold=fold)\n",
    "    model_save = tf.keras.callbacks.ModelCheckpoint('./fold-%i.hdf5' %(fold), \n",
    "                                                         monitor = 'val_mlp_out_rmse', verbose = 0, \n",
    "                                                         save_best_only = True, save_weights_only = True,\n",
    "                                                         mode = 'min', save_freq = 'epoch')\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_mlp_out_rmse', patience=15, mode='min', restore_best_weights=True)\n",
    "    history = model.fit(x_train, y_train ,\n",
    "                        epochs          = 200, \n",
    "                        callbacks       = [model_save, early_stop], \n",
    "                        validation_data = (x_val, y_val), \n",
    "                        batch_size      = batch_size[fold],\n",
    "                        verbose         = 2) \n",
    "    print('='*96)\n",
    "    models.append(model)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_parquet('../Output/valid_scaling.parquet')\n",
    "test = pd.read_parquet('../Output/test_scaling.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_rank(df):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df (pd.DataFrame): including predict column\n",
    "    Returns:\n",
    "        df (pd.DataFrame): df with Rank\n",
    "    \"\"\"\n",
    "    # sort records to set Rank\n",
    "    df = df.sort_values(\"predict\", ascending=False)\n",
    "    # set Rank starting from 0\n",
    "    df.loc[:, \"Rank\"] = np.arange(len(df[\"predict\"]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2, rank_col='Rank') -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df (pd.DataFrame): predicted results\n",
    "        portfolio_size (int): # of equities to buy/sell\n",
    "        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n",
    "    Returns:\n",
    "        (float): sharpe ratio\n",
    "    \"\"\"\n",
    "\n",
    "    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): predicted results\n",
    "            portfolio_size (int): # of equities to buy/sell\n",
    "            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n",
    "        Returns:\n",
    "            (float): spread return\n",
    "        \"\"\"\n",
    "        assert df[rank_col].min() == 0\n",
    "        assert df[rank_col].max() == len(df[rank_col]) - 1\n",
    "        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n",
    "        purchase = (df.sort_values(by=rank_col)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n",
    "        short = (df.sort_values(by=rank_col, ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n",
    "        return purchase - short\n",
    "\n",
    "    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n",
    "    sharpe_ratio = buf.mean() / buf.std()\n",
    "    return sharpe_ratio, buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2625/2625 [==============================] - 14s 5ms/step\n",
      "2625/2625 [==============================] - 3s 1ms/step\n",
      "2625/2625 [==============================] - 4s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "ap = [0.10,0.10,0.80]\n",
    "model_x = list()\n",
    "for i in range(FOLDS):\n",
    "    prediction_x = models[i].predict(valid[col_use])[-1] * ap[i]\n",
    "    model_x.append(prediction_x)\n",
    "model_x = np.mean(model_x, axis = 0)\n",
    "valid['predict'] = model_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson: 0.009001090138007897\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "pearson_score = stats.pearsonr(valid['predict'], valid.Target)[0]\n",
    "print('Pearson:', pearson_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = valid.sort_values([\"Date\", \"predict\"], ascending=[True, False])\n",
    "ranking = valid.groupby(\"Date\").apply(set_rank).reset_index(drop=True)\n",
    "sharp_ratio, _ = calc_spread_return_sharpe(ranking, portfolio_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1928431330686605"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sharp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2313/2313 [==============================] - 8s 3ms/step\n",
      "2313/2313 [==============================] - 3s 1ms/step\n",
      "2313/2313 [==============================] - 3s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "ap = [0.10,0.10,0.80]\n",
    "model_x = list()\n",
    "for i in range(FOLDS):\n",
    "    prediction_x = models[i].predict(test[col_use])[-1] * ap[i]\n",
    "    model_x.append(prediction_x)\n",
    "model_x = np.mean(model_x, axis = 0)\n",
    "test['predict'] = model_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson: 0.010917813954741768\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "pearson_score = stats.pearsonr(test['predict'], test.Target)[0]\n",
    "print('Pearson:', pearson_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.sort_values([\"Date\", \"predict\"], ascending=[True, False])\n",
    "ranking = test.groupby(\"Date\").apply(set_rank).reset_index(drop=True)\n",
    "sharp_ratio, _ = calc_spread_return_sharpe(ranking, portfolio_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30791324414623966"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sharp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2022-01-04    0.916231\n",
       "2022-01-05    0.811388\n",
       "2022-01-06    2.046765\n",
       "2022-01-07   -1.404698\n",
       "2022-01-11    1.425312\n",
       "2022-01-12   -0.038884\n",
       "2022-01-13    1.322542\n",
       "2022-01-14    0.826248\n",
       "2022-01-17    0.515257\n",
       "2022-01-18   -0.207358\n",
       "2022-01-19   -0.136236\n",
       "2022-01-20   -0.483709\n",
       "2022-01-21    0.558189\n",
       "2022-01-24    0.524910\n",
       "2022-01-25   -3.420761\n",
       "2022-01-26   -0.239314\n",
       "2022-01-27    2.638703\n",
       "2022-01-28    1.840159\n",
       "2022-01-31    4.204446\n",
       "2022-02-01    1.640637\n",
       "2022-02-02    0.187583\n",
       "2022-02-03   -0.383913\n",
       "2022-02-04    2.313215\n",
       "2022-02-07   -0.382194\n",
       "2022-02-08    0.504940\n",
       "2022-02-09   -0.431038\n",
       "2022-02-10    0.300020\n",
       "2022-02-14    1.258598\n",
       "2022-02-15   -1.652183\n",
       "2022-02-16    0.093271\n",
       "2022-02-17   -1.167805\n",
       "2022-02-18   -0.396076\n",
       "2022-02-21   -2.895031\n",
       "2022-02-22    4.152860\n",
       "2022-02-24    0.626257\n",
       "2022-02-25    6.007314\n",
       "2022-02-28   -0.769547\n",
       "dtype: float64"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.polynomial.hermite as Herm\n",
    "import math\n",
    "from tensorflow.python.ops import math_ops\n",
    "from scipy import stats\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_num = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2, rank_col='Rank') -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df (pd.DataFrame): predicted results\n",
    "        portfolio_size (int): # of equities to buy/sell\n",
    "        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n",
    "    Returns:\n",
    "        (float): sharpe ratio\n",
    "    \"\"\"\n",
    "\n",
    "    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): predicted results\n",
    "            portfolio_size (int): # of equities to buy/sell\n",
    "            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n",
    "        Returns:\n",
    "            (float): spread return\n",
    "        \"\"\"\n",
    "        assert df[rank_col].min() == 0\n",
    "        assert df[rank_col].max() == len(df[rank_col]) - 1\n",
    "        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n",
    "        purchase = (df.sort_values(by=rank_col)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n",
    "        short = (df.sort_values(by=rank_col, ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n",
    "        return purchase - short\n",
    "\n",
    "    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n",
    "    sharpe_ratio = buf.mean() / buf.std()\n",
    "    return sharpe_ratio, buf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_rank(df):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df (pd.DataFrame): including predict column\n",
    "    Returns:\n",
    "        df (pd.DataFrame): df with Rank\n",
    "    \"\"\"\n",
    "    # sort records to set Rank\n",
    "    df = df.sort_values(\"predict\", ascending=False)\n",
    "    # set Rank starting from 0\n",
    "    df.loc[:, \"Rank\"] = np.arange(len(df[\"predict\"]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet('../Output/train_scaling.parquet')\n",
    "val = pd.read_parquet('../Output/valid_scaling.parquet')\n",
    "test = pd.read_parquet('../Output/test_scaling.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_use  = [\n",
    "    'day', 'Volume',\n",
    "    'ScaledAdjustedOpen', 'ScaledAdjustedHigh', 'ScaledAdjustedLow',\n",
    "    'ScaledAdjustedClose',\n",
    "    'close_arccos_deg',\n",
    "    'trend_psar_up_indicator', 'trend_psar_down_indicator',\n",
    "    'trend_aroon_ind',\n",
    "    'trend_psa_indicator',\n",
    "    'trend_aroon_ind_diff1',\n",
    "    'volume_pct_change_ror_1',\n",
    "    'sma_5_25', 'sma_25_30',\n",
    "    'd_atr',\n",
    "    'ror_1', 'ror_5',\n",
    "    'ror1_ror2', 'ror1_ror3', 'ror1_ror4', 'ror1_ror5',\n",
    "    'ror_1_shift1', 'ror_1_shift5',\n",
    "    'TradedAmount_1', 'd_Amount',\n",
    "    'range_1',\n",
    "    'gap_range_1',\n",
    "    'day_range_1',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df['close_arcsin_deg'] = np.degrees(np.arcsin(join_df['ScaledAdjustedClose']))\n",
    "join_df['close_arccos_deg'] = np.degrees(np.arccos(join_df['ScaledAdjustedClose']))\n",
    "join_df['close_arctan_deg'] = np.degrees(np.arctan(join_df['ScaledAdjustedClose']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_model_rank(train_x, train_y, validation_x, validation_y, query_train, query_validation):\n",
    "    # params = {\n",
    "    #     # baseline parameters\n",
    "    #     \"objective\" : \"lambdarank\",\n",
    "    #     # \"objective\" : \"rank_xendcg\",\n",
    "    #     # \"metric\" : \"None\",\n",
    "    #     \"num_leaves\" : 23,\n",
    "    #     \"learning_rate\" : 0.01,\n",
    "    #     # \"bagging_fraction\" : 0.6,\n",
    "    #     # \"feature_fraction\" : 0.6,\n",
    "    #     \"bagging_seed\" : 42,\n",
    "    #     \"verbosity\" : -1,\n",
    "    #     \"seed\": 42,\n",
    "    #     # \"num_class\": 6,\n",
    "    #     # \"max_bin\": 128,\n",
    "    #     # \"colsample_bytree\": 0.9,\n",
    "    #     # n_jobs=cpu_count(),\n",
    "    #     # \"reg_alpha\": 0.2,\n",
    "    #     # \"reg_lambda\": 0.2,\n",
    "    #     \"label_gain\": np.arange(rank_num),\n",
    "    #     \"lambdarank_truncation_level\": rank_num,\n",
    "    #     # \"ndcg_eval_at\": np.concatenate([np.arange(200), np.arange(query_train.min()-200, query_train.min())]),\n",
    "    #     \"ndcg_eval_at\": [1]\n",
    "    # }\n",
    "    params = {\n",
    "        # baseline parameters\n",
    "        \"objective\" : \"lambdarank\",\n",
    "        # \"objective\" : \"rank_xendcg\",\n",
    "        # \"metric\" : \"map\",\n",
    "        \"num_leaves\" : 23,\n",
    "        \"learning_rate\" : 0.01,\n",
    "        # \"bagging_fraction\" : 0.6,\n",
    "        # \"feature_fraction\" : 0.6,\n",
    "        \"bagging_seed\" : 42,\n",
    "        \"verbosity\" : -1,\n",
    "        \"seed\": 42,\n",
    "        # \"num_class\": 6,\n",
    "        # \"max_bin\": 128,\n",
    "        # \"colsample_bytree\": 0.9,\n",
    "        # n_jobs=cpu_count(),\n",
    "        # \"reg_alpha\": 0.2,\n",
    "        # \"reg_lambda\": 0.2,\n",
    "        \"label_gain\": np.arange(rank_num),\n",
    "        \"lambdarank_truncation_level\": rank_num,\n",
    "        # \"ndcg_eval_at\": np.concatenate([np.arange(200), np.arange(query_train.min()-200, query_train.min())]),\n",
    "        \"ndcg_eval_at\": [1]\n",
    "    }\n",
    "\n",
    "\n",
    "    lg_train = lgb.Dataset(train_x, label=train_y, group=query_train)\n",
    "    lg_validation = lgb.Dataset(validation_x, label=validation_y, group=query_validation)\n",
    "    evals_result_lgbm = {}\n",
    "    \n",
    "    model_lightgbm = lgb.train(params,lg_train, valid_sets=[lg_validation], num_boost_round=2000,\n",
    "                               early_stopping_rounds=50, evals_result=evals_result_lgbm, verbose_eval=5,\n",
    "                            #    eval_at=[1],  # Make evaluation for target=1 ranking, I choosed arbitrarily\n",
    "                            #    feval=custom_metric,\n",
    "                               )\n",
    "    \n",
    "    # model_lightgbm.save_model(f'model_lightgbm_{index}.txt')\n",
    "    # pre_test_lightgbm = model_lightgbm.predict(test_x, num_iteration=model_lightgbm.best_iteration)\n",
    "    \n",
    "    \n",
    "    # return pre_test_lightgbm, model_lightgbm, evals_result_lgbm\n",
    "    return  model_lightgbm, evals_result_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import indexable\n",
    "from sklearn.utils.validation import _num_samples\n",
    "from sklearn.model_selection._split import _BaseKFold\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self defined GroupTimeSeriesSplit\n",
    "class GroupTimeSeriesSplit(_BaseKFold):\n",
    "\n",
    "    def __init__(self, n_splits=5, *, max_train_size=None):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_size = max_train_size\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_splits = self.n_splits\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_folds = n_splits + 1\n",
    "        indices = np.arange(n_samples)\n",
    "        group_counts = np.unique(groups, return_counts=True)[1]\n",
    "        groups = np.split(indices, np.cumsum(group_counts)[:-1])\n",
    "        n_groups = _num_samples(groups)\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds ={0} greater\"\n",
    "                 \" than the number of groups: {1}.\").format(n_folds, n_groups))\n",
    "        test_size = (n_groups // n_folds)\n",
    "        test_starts = range(test_size + n_groups % n_folds,\n",
    "                            n_groups, test_size)\n",
    "        for test_start in test_starts:\n",
    "            if self.max_train_size:\n",
    "                train_start = np.searchsorted(\n",
    "                    np.cumsum(\n",
    "                        group_counts[:test_start][::-1])[::-1] < self.max_train_size + 1, \n",
    "                        True)\n",
    "                yield (np.concatenate(groups[train_start:test_start]),\n",
    "                       np.concatenate(groups[test_start:test_start + test_size]))\n",
    "            else:\n",
    "                yield (np.concatenate(groups[:test_start]),\n",
    "                       np.concatenate(groups[test_start:test_start + test_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title GroupTimeSeriesSplit { display-mode: \"form\" }\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class GroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_size : int, default=None\n",
    "        Maximum size for a single training set.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.model_selection import GroupTimeSeriesSplit\n",
    "    >>> groups = np.array(['a', 'a', 'a', 'a', 'a', 'a',\\\n",
    "                           'b', 'b', 'b', 'b', 'b',\\\n",
    "                           'c', 'c', 'c', 'c',\\\n",
    "                           'd', 'd', 'd'])\n",
    "    >>> gtss = GroupTimeSeriesSplit(n_splits=3)\n",
    "    >>> for train_idx, test_idx in gtss.split(groups, groups=groups):\n",
    "    ...     print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
    "    ...     print(\"TRAIN GROUP:\", groups[train_idx],\\\n",
    "                  \"TEST GROUP:\", groups[test_idx])\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5] TEST: [6, 7, 8, 9, 10]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a']\\\n",
    "    TEST GROUP: ['b' 'b' 'b' 'b' 'b']\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] TEST: [11, 12, 13, 14]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b']\\\n",
    "    TEST GROUP: ['c' 'c' 'c' 'c']\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\\\n",
    "    TEST: [15, 16, 17]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b' 'c' 'c' 'c' 'c']\\\n",
    "    TEST GROUP: ['d' 'd' 'd']\n",
    "    \"\"\"\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_size=None\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_size = max_train_size\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "        group_test_size = n_groups // n_folds\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "            for train_group_idx in unique_groups[:group_test_start]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "            train_end = train_array.size\n",
    "            if self.max_train_size and self.max_train_size < train_end:\n",
    "                train_array = train_array[train_end -\n",
    "                                          self.max_train_size:train_end]\n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "\n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    "\n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "            \n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "    jet     = plt.cm.get_cmap('jet', 256)\n",
    "    seq     = np.linspace(0, 1, 256)\n",
    "    _       = np.random.shuffle(seq)   # inplace\n",
    "    cmap_data = ListedColormap(jet(seq))    \n",
    "    for ii, (tr, tt) in enumerate(list(cv.split(X=X, y=y, groups=group))):\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0        \n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices), c=indices, marker='_', lw=lw, cmap=cmap_cv, vmin=-.2, vmax=1.2)\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X), c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n",
    "    ax.scatter(range(len(X)), [ii + 2.5] * len(X), c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "    yticklabels = list(range(n_splits)) + ['target', 'day']\n",
    "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels, xlabel='Sample index', ylabel=\"CV iteration\", ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, os\n",
    "# import cudf\n",
    "# import talib as ta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import jpx_tokyo_market_prediction\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.polynomial.hermite as Herm\n",
    "import math\n",
    "from tensorflow.python.ops import math_ops\n",
    "from scipy import stats\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "import random\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"TPU\":\n",
    "    print(\"connecting to TPU...\")\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        tpu = None\n",
    "    if tpu:\n",
    "        try:\n",
    "            print(\"initializing  TPU ...\")\n",
    "            tf.config.experimental_connect_to_cluster(tpu)\n",
    "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "            strategy = tf.distribute.TPUStrategy(tpu)\n",
    "            print(\"TPU initialized\")\n",
    "        except: print(\"failed to initialize TPU\")\n",
    "    else: device = \"GPU\"\n",
    "\n",
    "if device != \"TPU\": strategy = tf.distribute.get_strategy()\n",
    "if device == \"GPU\": print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "AUTO     = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_overfit_sharp_ratio_list = []\n",
    "sharp_ratio_list = []\n",
    "importance_list = []\n",
    "\n",
    "gtscv = GroupTimeSeriesSplit(n_splits=5, max_train_size=None)\n",
    "for index, (train_id, val_id) in enumerate(gtscv.split(train_x, groups=groups)):\n",
    "    train_groups = train.iloc[train_id].groupby('Date')['SecuritiesCode'].nunique()\n",
    "    val_groups = train.iloc[val_id].groupby('Date')['SecuritiesCode'].nunique()\n",
    "\n",
    "    model_lightgbm, evals_result_lgbm = lgbm_model_rank(train_x.iloc[train_id], train_y.iloc[train_id], train_x.iloc[val_id], train_y.iloc[val_id], train_groups, val_groups)\n",
    "    # train_overfit = train.iloc[train_id]\n",
    "    # train_overfit['predict'] = model_lightgbm.predict(train_overfit[col_use])\n",
    "    # train_overfit = train_overfit.sort_values([\"Date\", \"predict\"], ascending=[True, False])\n",
    "    # ranking = train_overfit.groupby(\"Date\").apply(set_rank).reset_index(drop=True)\n",
    "    # t_sharp_ratio, _ = calc_spread_return_sharpe(ranking, portfolio_size=200)\n",
    "    # train_overfit_sharp_ratio_list.append(t_sharp_ratio)\n",
    "    # print(t_sharp_ratio)\n",
    "\n",
    "    test['predict'] = model_lightgbm.predict(test[col_use])\n",
    "    test = test.sort_values([\"Date\", \"predict\"], ascending=[True, False])\n",
    "    ranking = test.groupby(\"Date\").apply(set_rank).reset_index(drop=True)\n",
    "    sharp_ratio, _ = calc_spread_return_sharpe(ranking, portfolio_size=200)\n",
    "    sharp_ratio_list.append(sharp_ratio)\n",
    "    print(sharp_ratio)\n",
    "    ax = lgb.plot_metric(evals_result_lgbm)\n",
    "    plt.show()\n",
    "    importance = pd.DataFrame(model_lightgbm.feature_importance(\"gain\"), index=col_use, columns=['importance'])\n",
    "    importance_list.append(importance.sort_values(by=['importance'], ascending=False))\n",
    "print('Finish Model training..')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12543d8aedefe43b1af9739f504be95c6574cbf3bd96045c58756e8a60ca5178"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
