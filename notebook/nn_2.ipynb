{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, os\n",
    "# import cudf\n",
    "# import talib as ta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import jpx_tokyo_market_prediction\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.polynomial.hermite as Herm\n",
    "import math\n",
    "from tensorflow.python.ops import math_ops\n",
    "from scipy import stats\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "import random\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title GroupTimeSeriesSplit { display-mode: \"form\" }\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class GroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_size : int, default=None\n",
    "        Maximum size for a single training set.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.model_selection import GroupTimeSeriesSplit\n",
    "    >>> groups = np.array(['a', 'a', 'a', 'a', 'a', 'a',\\\n",
    "                           'b', 'b', 'b', 'b', 'b',\\\n",
    "                           'c', 'c', 'c', 'c',\\\n",
    "                           'd', 'd', 'd'])\n",
    "    >>> gtss = GroupTimeSeriesSplit(n_splits=3)\n",
    "    >>> for train_idx, test_idx in gtss.split(groups, groups=groups):\n",
    "    ...     print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
    "    ...     print(\"TRAIN GROUP:\", groups[train_idx],\\\n",
    "                  \"TEST GROUP:\", groups[test_idx])\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5] TEST: [6, 7, 8, 9, 10]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a']\\\n",
    "    TEST GROUP: ['b' 'b' 'b' 'b' 'b']\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] TEST: [11, 12, 13, 14]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b']\\\n",
    "    TEST GROUP: ['c' 'c' 'c' 'c']\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\\\n",
    "    TEST: [15, 16, 17]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b' 'c' 'c' 'c' 'c']\\\n",
    "    TEST GROUP: ['d' 'd' 'd']\n",
    "    \"\"\"\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_size=None\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_size = max_train_size\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "        group_test_size = n_groups // n_folds\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "            for train_group_idx in unique_groups[:group_test_start]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "            train_end = train_array.size\n",
    "            if self.max_train_size and self.max_train_size < train_end:\n",
    "                train_array = train_array[train_end -\n",
    "                                          self.max_train_size:train_end]\n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "\n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    "\n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "            \n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "    jet     = plt.cm.get_cmap('jet', 256)\n",
    "    seq     = np.linspace(0, 1, 256)\n",
    "    _       = np.random.shuffle(seq)   # inplace\n",
    "    cmap_data = ListedColormap(jet(seq))    \n",
    "    for ii, (tr, tt) in enumerate(list(cv.split(X=X, y=y, groups=group))):\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0        \n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices), c=indices, marker='_', lw=lw, cmap=cmap_cv, vmin=-.2, vmax=1.2)\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X), c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n",
    "    ax.scatter(range(len(X)), [ii + 2.5] * len(X), c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "    yticklabels = list(range(n_splits)) + ['target', 'day']\n",
    "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels, xlabel='Sample index', ylabel=\"CV iteration\", ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"CPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"TPU\":\n",
    "    print(\"connecting to TPU...\")\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        tpu = None\n",
    "    if tpu:\n",
    "        try:\n",
    "            print(\"initializing  TPU ...\")\n",
    "            tf.config.experimental_connect_to_cluster(tpu)\n",
    "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "            strategy = tf.distribute.TPUStrategy(tpu)\n",
    "            print(\"TPU initialized\")\n",
    "        except: print(\"failed to initialize TPU\")\n",
    "    else: device = \"GPU\"\n",
    "\n",
    "if device != \"TPU\": strategy = tf.distribute.get_strategy()\n",
    "if device == \"GPU\": print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "AUTO     = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2025\n",
    "set_all_seeds(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_use  = [\n",
    "    # 'day', 'Volume',\n",
    "    'ScaledAdjustedClose',\n",
    "    # 'close_arccos_deg',\n",
    "    'trend_psa_indicator',\n",
    "    'trend_aroon_ind_diff1',\n",
    "    # 'volume_pct_change_ror_1',\n",
    "    'sma_5_25', 'sma_25_30',\n",
    "    'd_atr',\n",
    "    'ror_1', 'ror_5', 'ror_10',\n",
    "    'TradedAmount_1', 'TradedAmount_5',\n",
    "    # 'ror1_ror2', 'ror1_ror3', 'ror1_ror4', 'ror1_ror5',\n",
    "    'ror_1_shift1', 'ror_1_shift2', 'ror_1_shift3', 'ror_1_shift4', 'ror_1_shift5',\n",
    "    'ror_1_shift6', 'ror_1_shift7', 'ror_1_shift8', 'ror_1_shift9',\n",
    "    'd_Amount',\n",
    "    'range_1', 'range_5',\n",
    "    'gap_range_1', 'gap_range_5',\n",
    "    'day_range_1', 'day_range_5',\n",
    "    'hig_range_1', 'hig_range_5',\n",
    "    'mi_1', 'mi_5',\n",
    "    'vola_10',\n",
    "    'hl_5', 'hl_10',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_cols = col_use + ['Date', 'Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_parquet('../Output/train_scaling.parquet', columns=read_cols)\n",
    "groups = pd.factorize(pd.to_datetime(X['Date']).dt.strftime('%d').astype(str) + '_' + pd.to_datetime(X['Date']).dt.strftime('%m').astype(str) + '_' +pd.to_datetime(X['Date']).dt.strftime('%Y').astype(str))\n",
    "y = X.Target\n",
    "# X = X.drop(['RowId','Target','AdjustmentFactor','ExpectedDividend','SupervisionFlag','Date'],axis=1)\n",
    "X = X[col_use]\n",
    "# valid = pd.read_parquet('../input/scaling/valid_scaling.parquet')\n",
    "# test = pd.read_parquet('../input/scaling/test_scaling.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV PARAMS\n",
    "FOLDS                = 3\n",
    "GROUP_GAP            = 14\n",
    "MAX_TEST_GROUP_SIZE  = 180  \n",
    "MAX_TRAIN_GROUP_SIZE = 485\n",
    "\n",
    "# USE VERBOSE=0 for silent, VERBOSE=1 for interactive, VERBOSE=2 for commit\n",
    "VERBOSE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2286531, 34), (2286531,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import backend as K\n",
    "def e_swish(beta=0.25):\n",
    "    def beta_swish(x): return x*K.sigmoid(x)*(1+beta)\n",
    "    return beta_swish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlationLoss(x,y, axis=-2):\n",
    "    \n",
    "    \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n",
    "    while trying to have the same mean and variance\"\"\"\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    y = math_ops.cast(y, x.dtype)\n",
    "    n = tf.cast(tf.shape(x)[axis], x.dtype)\n",
    "    xsum = tf.reduce_sum(x, axis=axis)\n",
    "    ysum = tf.reduce_sum(y, axis=axis)\n",
    "    xmean = xsum / n\n",
    "    ymean = ysum / n\n",
    "    xsqsum = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n",
    "    ysqsum = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n",
    "    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n",
    "    corr = cov / tf.sqrt(xsqsum * ysqsum)\n",
    "    return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr ) , dtype=tf.float32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(x, y, axis=-2):\n",
    "    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    y = math_ops.cast(y, x.dtype)\n",
    "    n = tf.cast(tf.shape(x)[axis], x.dtype)\n",
    "    xsum = tf.reduce_sum(x, axis=axis)\n",
    "    ysum = tf.reduce_sum(y, axis=axis)\n",
    "    xmean = xsum / n\n",
    "    ymean = ysum / n\n",
    "    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n",
    "    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n",
    "    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n",
    "    corr = cov / tf.sqrt(xvar * yvar)\n",
    "    return tf.constant(1.0, dtype=x.dtype) - corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_loss(X_train,y_pred):\n",
    "    y_pred = tf.Variable(y_pred,dtype=tf.float64)\n",
    "    port_ret = tf.reduce_sum(tf.multiply(_,y_pred),axis=1)\n",
    "    s_ratio = K.mean(port_ret)/K.std(port_ret)\n",
    "    \n",
    "    return tf.math.exp(-s_ratio,  name='sharpe_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp, dim = 128, fold=0):\n",
    "    \n",
    "    features_inputs = tf.keras.layers.Input(shape = (dim, ))\n",
    "    x0      =  tf.keras.layers.BatchNormalization()(features_inputs)\n",
    "    \n",
    "    weight = tf.Variable(tf.keras.backend.random_normal((dim, 1), stddev=hp.Float(f'weight_{fold}',1e-10, 0.09), dtype=tf.float32))\n",
    "    var    = tf.Variable(tf.zeros((1,1), dtype=tf.float32))\n",
    "   \n",
    "    encoder = tf.keras.layers.GaussianNoise(0.4)(x0)\n",
    "    encoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_en0',32, 1024))(encoder)\n",
    "    encoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_en1',32, 1024))(encoder)\n",
    "    encoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_en2',32, 1024))(encoder)\n",
    "    encoder = tf.keras.layers.BatchNormalization()(encoder)\n",
    "    encoder = tf.keras.layers.Activation(e_swish(beta=hp.Float(f'e{fold}_en0',0.001, 1 )))(encoder)\n",
    "    \n",
    "    decoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_de0',32, 1024), name='decoder')(encoder)\n",
    "#     decoder = tf.keras.layers.Dropout(hp.Float(f'dropout{fold}_de0',0.001, 0.8))(encoder)\n",
    "#     decoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_de0',32, 1024), name='decoder')(decoder)\n",
    "    \n",
    "    x_ae = tf.keras.layers.Dense(hp.Int(f'layers{fold}_ae0',32, 1024))(decoder)\n",
    "    x_ae = tf.keras.layers.BatchNormalization()(x_ae)\n",
    "    x_ae = tf.keras.layers.Activation(e_swish(beta=hp.Float(f'e{fold}_ae0',0.001, 1 )))(x_ae)\n",
    "#     x_ae = tf.keras.layers.Dropout(hp.Float(f'dropout{fold}_ae0',0.001, 0.8))(x_ae) \n",
    "    \n",
    "    feature_x = tf.keras.layers.Concatenate()([x0, encoder])\n",
    "    feature_x = tf.keras.layers.BatchNormalization()(feature_x)\n",
    "    feature_x = tf.keras.layers.Dense(hp.Int(f'layers{fold}_fx0',32, 1024))(feature_x)\n",
    "    feature_x = tf.keras.layers.Activation(e_swish(beta=hp.Float(f'e_fx0',0.001, 1 )))(feature_x)\n",
    "#     feature_x = tf.keras.layers.Dropout(hp.Float(f'dropout{fold}_fx0',0.001, 0.8))(feature_x)\n",
    "\n",
    "    x = layers.Dense(hp.Int(f'layers{fold}_x0',32, 1024), activation= e_swish(beta=hp.Float(f'e{fold}_x0',0.001, 1 )), kernel_regularizer=\"l2\")(feature_x)\n",
    "    x = layers.Dense(hp.Int(f'layers{fold}_x1',32, 1024), activation= e_swish(beta=hp.Float(f'e{fold}_x1',0.001, 1 )), kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dense(hp.Int(f'layers{fold}_x2',32, 1024), activation= e_swish(beta=hp.Float(f'e{fold}_x2',0.001, 1 )), kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dense(hp.Int(f'layers{fold}_x3',32, 1024), activation= e_swish(beta=hp.Float(f'e{fold}_x3',0.001, 1 )), kernel_regularizer=\"l2\")(x)\n",
    "#     x = tf.keras.layers.Dropout(hp.Float(f'dropout{fold}_x0',0.001, 0.8))(x)\n",
    "\n",
    "    mlp_out = layers.Dense(1, name ='mlp_out')(x)\n",
    "\n",
    "    model  = tf.keras.Model(inputs=[features_inputs], outputs=[decoder, mlp_out])\n",
    "    \n",
    "    loss_out = tf.add(tf.matmul(features_inputs,weight), tf.math.reduce_sum(weight*var))\n",
    "    tf.compat.v1.losses.add_loss(loss_out)\n",
    "  \n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Float(f'lr_adam{fold}',1e-3, 1e-5)),\n",
    "                  loss = {'decoder': [tf.keras.losses.CosineSimilarity(axis=-2), \n",
    "                                      tf.keras.losses.MeanSquaredError(), \n",
    "                                      correlationLoss],         \n",
    "                          \n",
    "                          'mlp_out' : [sharpe_loss],\n",
    "                         },\n",
    "                  metrics = {'decoder': [tf.keras.metrics.CosineSimilarity(name='cosine'),\n",
    "                                         tf.keras.metrics.MeanAbsoluteError(name=\"mae\"), \n",
    "                                         correlation, \n",
    "                                         tf.keras.metrics.RootMeanSquaredError(name='rmse')], \n",
    "                             \n",
    "                             'mlp_out' : [tf.keras.metrics.CosineSimilarity(name='cosine'),\n",
    "                                          tf.keras.metrics.MeanAbsoluteError(name=\"mae\"), \n",
    "                                          correlation, \n",
    "                                          tf.keras.metrics.RootMeanSquaredError(name='rmse')],\n",
    "                            },\n",
    "                 ) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = pd.read_pickle(f'../Output/hp-jpx-aemlp/best_hp_ae_jpx_3gkf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 22:31:04.915176: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(128, 1) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "# tf.keras.utils.plot_model(build_model(hp, fold=0), show_shapes=True, expand_nested=True, show_dtype=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [4096*4,4096,4096*8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> AEMLP_FOLD:0\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_1), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(34, 1) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "Epoch 1/200\n",
      "57/57 - 56s - loss: 9.0438 - decoder_loss: -1.0582e-02 - mlp_out_loss: 0.2898 - decoder_cosine: 0.0436 - decoder_mae: 0.4069 - decoder_correlation: 0.9894 - decoder_rmse: 0.6689 - mlp_out_cosine: -1.5659e-03 - mlp_out_mae: 0.1902 - mlp_out_correlation: 1.0018 - mlp_out_rmse: 0.5383 - val_loss: 6.5779 - val_decoder_loss: -1.6807e-02 - val_mlp_out_loss: 0.0030 - val_decoder_cosine: 0.0783 - val_decoder_mae: 0.3498 - val_decoder_correlation: 0.9895 - val_decoder_rmse: 0.9021 - val_mlp_out_cosine: 0.0089 - val_mlp_out_mae: 0.0413 - val_mlp_out_correlation: 0.9778 - val_mlp_out_rmse: 0.0548 - 56s/epoch - 982ms/step\n",
      "Epoch 2/200\n",
      "57/57 - 53s - loss: 5.0952 - decoder_loss: -4.0612e-02 - mlp_out_loss: 0.0015 - decoder_cosine: 0.1194 - decoder_mae: 0.4597 - decoder_correlation: 0.9591 - decoder_rmse: 0.6706 - mlp_out_cosine: 0.0021 - mlp_out_mae: 0.0268 - mlp_out_correlation: 0.9994 - mlp_out_rmse: 0.0384 - val_loss: 3.8689 - val_decoder_loss: -5.2506e-02 - val_mlp_out_loss: 8.7798e-04 - val_decoder_cosine: 0.1329 - val_decoder_mae: 0.3447 - val_decoder_correlation: 0.9531 - val_decoder_rmse: 0.5468 - val_mlp_out_cosine: 0.0225 - val_mlp_out_mae: 0.0203 - val_mlp_out_correlation: 0.9865 - val_mlp_out_rmse: 0.0296 - 53s/epoch - 926ms/step\n",
      "Epoch 3/200\n",
      "57/57 - 48s - loss: 3.1108 - decoder_loss: -6.5458e-02 - mlp_out_loss: 7.9856e-04 - decoder_cosine: 0.1637 - decoder_mae: 0.5944 - decoder_correlation: 0.9345 - decoder_rmse: 0.7849 - mlp_out_cosine: 0.0051 - mlp_out_mae: 0.0197 - mlp_out_correlation: 0.9972 - mlp_out_rmse: 0.0283 - val_loss: 2.4685 - val_decoder_loss: -6.6900e-02 - val_mlp_out_loss: 8.2450e-04 - val_decoder_cosine: 0.1524 - val_decoder_mae: 0.4598 - val_decoder_correlation: 0.9335 - val_decoder_rmse: 0.6148 - val_mlp_out_cosine: 0.0094 - val_mlp_out_mae: 0.0195 - val_mlp_out_correlation: 1.0047 - val_mlp_out_rmse: 0.0287 - 48s/epoch - 843ms/step\n",
      "Epoch 4/200\n",
      "57/57 - 47s - loss: 2.0386 - decoder_loss: -7.5392e-02 - mlp_out_loss: 6.6324e-04 - decoder_cosine: 0.1773 - decoder_mae: 0.6538 - decoder_correlation: 0.9246 - decoder_rmse: 0.8670 - mlp_out_cosine: 0.0072 - mlp_out_mae: 0.0176 - mlp_out_correlation: 0.9948 - mlp_out_rmse: 0.0258 - val_loss: 1.6713 - val_decoder_loss: -6.9356e-02 - val_mlp_out_loss: 8.3266e-04 - val_decoder_cosine: 0.1614 - val_decoder_mae: 0.5020 - val_decoder_correlation: 0.9290 - val_decoder_rmse: 0.6788 - val_mlp_out_cosine: -5.7562e-04 - val_mlp_out_mae: 0.0197 - val_mlp_out_correlation: 1.0114 - val_mlp_out_rmse: 0.0289 - 47s/epoch - 833ms/step\n",
      "Epoch 5/200\n",
      "57/57 - 48s - loss: 1.4011 - decoder_loss: -8.0656e-02 - mlp_out_loss: 6.1267e-04 - decoder_cosine: 0.1878 - decoder_mae: 0.6182 - decoder_correlation: 0.9194 - decoder_rmse: 0.8415 - mlp_out_cosine: 0.0073 - mlp_out_mae: 0.0167 - mlp_out_correlation: 0.9924 - mlp_out_rmse: 0.0248 - val_loss: 1.1760 - val_decoder_loss: -7.1495e-02 - val_mlp_out_loss: 8.2172e-04 - val_decoder_cosine: 0.1712 - val_decoder_mae: 0.4786 - val_decoder_correlation: 0.9255 - val_decoder_rmse: 0.6515 - val_mlp_out_cosine: -2.2082e-03 - val_mlp_out_mae: 0.0194 - val_mlp_out_correlation: 1.0100 - val_mlp_out_rmse: 0.0287 - 48s/epoch - 835ms/step\n",
      "Epoch 6/200\n",
      "57/57 - 47s - loss: 0.9919 - decoder_loss: -8.6521e-02 - mlp_out_loss: 5.8664e-04 - decoder_cosine: 0.1968 - decoder_mae: 0.5435 - decoder_correlation: 0.9137 - decoder_rmse: 0.7524 - mlp_out_cosine: 0.0060 - mlp_out_mae: 0.0162 - mlp_out_correlation: 0.9942 - mlp_out_rmse: 0.0242 - val_loss: 0.8524 - val_decoder_loss: -7.0665e-02 - val_mlp_out_loss: 8.0655e-04 - val_decoder_cosine: 0.1724 - val_decoder_mae: 0.4098 - val_decoder_correlation: 0.9254 - val_decoder_rmse: 0.5743 - val_mlp_out_cosine: -3.4503e-03 - val_mlp_out_mae: 0.0191 - val_mlp_out_correlation: 1.0073 - val_mlp_out_rmse: 0.0284 - 47s/epoch - 833ms/step\n",
      "Epoch 7/200\n",
      "57/57 - 47s - loss: 0.7141 - decoder_loss: -9.3397e-02 - mlp_out_loss: 5.6590e-04 - decoder_cosine: 0.2083 - decoder_mae: 0.4470 - decoder_correlation: 0.9068 - decoder_rmse: 0.6336 - mlp_out_cosine: 0.0043 - mlp_out_mae: 0.0159 - mlp_out_correlation: 0.9982 - mlp_out_rmse: 0.0238 - val_loss: 0.6322 - val_decoder_loss: -6.7895e-02 - val_mlp_out_loss: 7.9385e-04 - val_decoder_cosine: 0.1606 - val_decoder_mae: 0.3324 - val_decoder_correlation: 0.9235 - val_decoder_rmse: 0.4817 - val_mlp_out_cosine: -3.6120e-03 - val_mlp_out_mae: 0.0188 - val_mlp_out_correlation: 1.0046 - val_mlp_out_rmse: 0.0282 - 47s/epoch - 831ms/step\n",
      "Epoch 8/200\n",
      "57/57 - 48s - loss: 0.5195 - decoder_loss: -9.8679e-02 - mlp_out_loss: 5.5098e-04 - decoder_cosine: 0.2225 - decoder_mae: 0.3915 - decoder_correlation: 0.9015 - decoder_rmse: 0.5606 - mlp_out_cosine: 0.0029 - mlp_out_mae: 0.0156 - mlp_out_correlation: 1.0024 - mlp_out_rmse: 0.0235 - val_loss: 0.4759 - val_decoder_loss: -6.5489e-02 - val_mlp_out_loss: 7.8901e-04 - val_decoder_cosine: 0.1754 - val_decoder_mae: 0.3030 - val_decoder_correlation: 0.9268 - val_decoder_rmse: 0.4417 - val_mlp_out_cosine: -3.0069e-03 - val_mlp_out_mae: 0.0187 - val_mlp_out_correlation: 0.9992 - val_mlp_out_rmse: 0.0281 - 48s/epoch - 850ms/step\n",
      "Epoch 9/200\n",
      "57/57 - 54s - loss: 0.3771 - decoder_loss: -1.0447e-01 - mlp_out_loss: 5.4241e-04 - decoder_cosine: 0.2311 - decoder_mae: 0.3594 - decoder_correlation: 0.8957 - decoder_rmse: 0.5176 - mlp_out_cosine: 0.0017 - mlp_out_mae: 0.0154 - mlp_out_correlation: 1.0053 - mlp_out_rmse: 0.0233 - val_loss: 0.3594 - val_decoder_loss: -6.5818e-02 - val_mlp_out_loss: 7.8583e-04 - val_decoder_cosine: 0.1718 - val_decoder_mae: 0.2928 - val_decoder_correlation: 0.9290 - val_decoder_rmse: 0.4353 - val_mlp_out_cosine: -2.1602e-03 - val_mlp_out_mae: 0.0186 - val_mlp_out_correlation: 0.9945 - val_mlp_out_rmse: 0.0280 - 54s/epoch - 946ms/step\n",
      "Epoch 10/200\n",
      "57/57 - 48s - loss: 0.2727 - decoder_loss: -1.0781e-01 - mlp_out_loss: 5.3659e-04 - decoder_cosine: 0.2334 - decoder_mae: 0.3396 - decoder_correlation: 0.8923 - decoder_rmse: 0.4915 - mlp_out_cosine: 0.0013 - mlp_out_mae: 0.0153 - mlp_out_correlation: 1.0066 - mlp_out_rmse: 0.0232 - val_loss: 0.2782 - val_decoder_loss: -6.0032e-02 - val_mlp_out_loss: 7.8365e-04 - val_decoder_cosine: 0.1211 - val_decoder_mae: 0.3138 - val_decoder_correlation: 0.9308 - val_decoder_rmse: 0.5108 - val_mlp_out_cosine: -1.7064e-03 - val_mlp_out_mae: 0.0185 - val_mlp_out_correlation: 0.9922 - val_mlp_out_rmse: 0.0280 - 48s/epoch - 850ms/step\n",
      "Epoch 11/200\n",
      "57/57 - 48s - loss: 0.1934 - decoder_loss: -1.1070e-01 - mlp_out_loss: 5.3216e-04 - decoder_cosine: 0.2417 - decoder_mae: 0.3192 - decoder_correlation: 0.8896 - decoder_rmse: 0.4676 - mlp_out_cosine: -3.0227e-06 - mlp_out_mae: 0.0152 - mlp_out_correlation: 1.0075 - mlp_out_rmse: 0.0231 - val_loss: 0.2059 - val_decoder_loss: -6.5891e-02 - val_mlp_out_loss: 7.8261e-04 - val_decoder_cosine: 0.1464 - val_decoder_mae: 0.3487 - val_decoder_correlation: 0.9252 - val_decoder_rmse: 0.5594 - val_mlp_out_cosine: -1.7440e-03 - val_mlp_out_mae: 0.0185 - val_mlp_out_correlation: 0.9904 - val_mlp_out_rmse: 0.0280 - 48s/epoch - 840ms/step\n",
      "Epoch 12/200\n",
      "57/57 - 49s - loss: 0.1315 - decoder_loss: -1.1388e-01 - mlp_out_loss: 5.2908e-04 - decoder_cosine: 0.2477 - decoder_mae: 0.3075 - decoder_correlation: 0.8865 - decoder_rmse: 0.4502 - mlp_out_cosine: -5.5340e-04 - mlp_out_mae: 0.0152 - mlp_out_correlation: 1.0074 - mlp_out_rmse: 0.0230 - val_loss: 0.1653 - val_decoder_loss: -5.5056e-02 - val_mlp_out_loss: 7.8155e-04 - val_decoder_cosine: 0.0993 - val_decoder_mae: 0.3336 - val_decoder_correlation: 0.9378 - val_decoder_rmse: 0.5477 - val_mlp_out_cosine: -6.7659e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 0.9893 - val_mlp_out_rmse: 0.0280 - 49s/epoch - 855ms/step\n",
      "Epoch 13/200\n",
      "57/57 - 48s - loss: 0.0848 - decoder_loss: -1.1482e-01 - mlp_out_loss: 5.2657e-04 - decoder_cosine: 0.2472 - decoder_mae: 0.2948 - decoder_correlation: 0.8854 - decoder_rmse: 0.4322 - mlp_out_cosine: -2.7056e-03 - mlp_out_mae: 0.0151 - mlp_out_correlation: 1.0082 - mlp_out_rmse: 0.0229 - val_loss: 0.1172 - val_decoder_loss: -6.2661e-02 - val_mlp_out_loss: 7.8054e-04 - val_decoder_cosine: 0.1435 - val_decoder_mae: 0.3308 - val_decoder_correlation: 0.9383 - val_decoder_rmse: 0.5398 - val_mlp_out_cosine: -9.3157e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 0.9897 - val_mlp_out_rmse: 0.0279 - 48s/epoch - 838ms/step\n",
      "Epoch 14/200\n",
      "57/57 - 47s - loss: 0.0454 - decoder_loss: -1.1800e-01 - mlp_out_loss: 5.2524e-04 - decoder_cosine: 0.2561 - decoder_mae: 0.2806 - decoder_correlation: 0.8821 - decoder_rmse: 0.4135 - mlp_out_cosine: -3.6124e-03 - mlp_out_mae: 0.0151 - mlp_out_correlation: 1.0097 - mlp_out_rmse: 0.0229 - val_loss: 0.0855 - val_decoder_loss: -6.2272e-02 - val_mlp_out_loss: 7.7949e-04 - val_decoder_cosine: 0.1274 - val_decoder_mae: 0.3436 - val_decoder_correlation: 0.9365 - val_decoder_rmse: 0.5769 - val_mlp_out_cosine: -7.6739e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 0.9906 - val_mlp_out_rmse: 0.0279 - 47s/epoch - 830ms/step\n",
      "Epoch 15/200\n",
      "57/57 - 47s - loss: 0.0166 - decoder_loss: -1.1788e-01 - mlp_out_loss: 5.2425e-04 - decoder_cosine: 0.2568 - decoder_mae: 0.2679 - decoder_correlation: 0.8821 - decoder_rmse: 0.3957 - mlp_out_cosine: -5.6973e-03 - mlp_out_mae: 0.0151 - mlp_out_correlation: 1.0096 - mlp_out_rmse: 0.0229 - val_loss: 0.0650 - val_decoder_loss: -5.6953e-02 - val_mlp_out_loss: 7.7883e-04 - val_decoder_cosine: 0.1088 - val_decoder_mae: 0.3259 - val_decoder_correlation: 0.9369 - val_decoder_rmse: 0.5643 - val_mlp_out_cosine: -1.4469e-02 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 0.9925 - val_mlp_out_rmse: 0.0279 - 47s/epoch - 832ms/step\n",
      "Epoch 16/200\n",
      "57/57 - 49s - loss: -8.0711e-03 - decoder_loss: -1.1926e-01 - mlp_out_loss: 5.2359e-04 - decoder_cosine: 0.2546 - decoder_mae: 0.2638 - decoder_correlation: 0.8808 - decoder_rmse: 0.3907 - mlp_out_cosine: -6.6227e-03 - mlp_out_mae: 0.0151 - mlp_out_correlation: 1.0093 - mlp_out_rmse: 0.0229 - val_loss: 0.0454 - val_decoder_loss: -5.5668e-02 - val_mlp_out_loss: 7.7813e-04 - val_decoder_cosine: 0.0962 - val_decoder_mae: 0.3156 - val_decoder_correlation: 0.9414 - val_decoder_rmse: 0.5512 - val_mlp_out_cosine: -5.9310e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 0.9943 - val_mlp_out_rmse: 0.0279 - 49s/epoch - 853ms/step\n",
      "Epoch 17/200\n",
      "57/57 - 48s - loss: -3.0353e-02 - decoder_loss: -1.2264e-01 - mlp_out_loss: 5.2311e-04 - decoder_cosine: 0.2653 - decoder_mae: 0.2509 - decoder_correlation: 0.8774 - decoder_rmse: 0.3733 - mlp_out_cosine: -6.8933e-03 - mlp_out_mae: 0.0151 - mlp_out_correlation: 1.0090 - mlp_out_rmse: 0.0229 - val_loss: 0.0222 - val_decoder_loss: -6.1923e-02 - val_mlp_out_loss: 7.7747e-04 - val_decoder_cosine: 0.1323 - val_decoder_mae: 0.3207 - val_decoder_correlation: 0.9426 - val_decoder_rmse: 0.5397 - val_mlp_out_cosine: -5.4546e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 0.9966 - val_mlp_out_rmse: 0.0279 - 48s/epoch - 846ms/step\n",
      "Epoch 18/200\n",
      "57/57 - 48s - loss: -4.5909e-02 - decoder_loss: -1.2275e-01 - mlp_out_loss: 5.2261e-04 - decoder_cosine: 0.2684 - decoder_mae: 0.2485 - decoder_correlation: 0.8775 - decoder_rmse: 0.3704 - mlp_out_cosine: -2.9942e-03 - mlp_out_mae: 0.0151 - mlp_out_correlation: 1.0071 - mlp_out_rmse: 0.0229 - val_loss: 0.0158 - val_decoder_loss: -5.4337e-02 - val_mlp_out_loss: 7.7708e-04 - val_decoder_cosine: 0.0695 - val_decoder_mae: 0.3105 - val_decoder_correlation: 0.9445 - val_decoder_rmse: 0.5522 - val_mlp_out_cosine: -4.5041e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 0.9966 - val_mlp_out_rmse: 0.0279 - 48s/epoch - 842ms/step\n",
      "Epoch 19/200\n",
      "57/57 - 49s - loss: -5.9482e-02 - decoder_loss: -1.2364e-01 - mlp_out_loss: 5.2224e-04 - decoder_cosine: 0.2650 - decoder_mae: 0.2357 - decoder_correlation: 0.8766 - decoder_rmse: 0.3519 - mlp_out_cosine: -3.3058e-03 - mlp_out_mae: 0.0151 - mlp_out_correlation: 1.0063 - mlp_out_rmse: 0.0229 - val_loss: 0.0026 - val_decoder_loss: -5.6105e-02 - val_mlp_out_loss: 7.7693e-04 - val_decoder_cosine: 0.1026 - val_decoder_mae: 0.3120 - val_decoder_correlation: 0.9471 - val_decoder_rmse: 0.5396 - val_mlp_out_cosine: -1.3981e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 0.9990 - val_mlp_out_rmse: 0.0279 - 49s/epoch - 865ms/step\n",
      "Epoch 20/200\n",
      "57/57 - 49s - loss: -7.1369e-02 - decoder_loss: -1.2505e-01 - mlp_out_loss: 5.2196e-04 - decoder_cosine: 0.2698 - decoder_mae: 0.2323 - decoder_correlation: 0.8754 - decoder_rmse: 0.3475 - mlp_out_cosine: -1.4782e-03 - mlp_out_mae: 0.0151 - mlp_out_correlation: 1.0052 - mlp_out_rmse: 0.0228 - val_loss: -4.2322e-03 - val_decoder_loss: -5.3449e-02 - val_mlp_out_loss: 7.7659e-04 - val_decoder_cosine: 0.0799 - val_decoder_mae: 0.2984 - val_decoder_correlation: 0.9431 - val_decoder_rmse: 0.5309 - val_mlp_out_cosine: 6.2173e-04 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 0.9995 - val_mlp_out_rmse: 0.0279 - 49s/epoch - 855ms/step\n",
      "Epoch 21/200\n",
      "57/57 - 48s - loss: -8.1743e-02 - decoder_loss: -1.2674e-01 - mlp_out_loss: 5.2174e-04 - decoder_cosine: 0.2749 - decoder_mae: 0.2201 - decoder_correlation: 0.8735 - decoder_rmse: 0.3306 - mlp_out_cosine: 2.6783e-04 - mlp_out_mae: 0.0151 - mlp_out_correlation: 1.0044 - mlp_out_rmse: 0.0228 - val_loss: -1.6593e-02 - val_decoder_loss: -5.7927e-02 - val_mlp_out_loss: 7.7639e-04 - val_decoder_cosine: 0.1023 - val_decoder_mae: 0.2927 - val_decoder_correlation: 0.9384 - val_decoder_rmse: 0.5206 - val_mlp_out_cosine: 0.0024 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0279 - 48s/epoch - 841ms/step\n",
      "Epoch 22/200\n",
      "57/57 - 50s - loss: -8.9879e-02 - decoder_loss: -1.2766e-01 - mlp_out_loss: 5.2158e-04 - decoder_cosine: 0.2675 - decoder_mae: 0.2162 - decoder_correlation: 0.8725 - decoder_rmse: 0.3259 - mlp_out_cosine: 0.0021 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0035 - mlp_out_rmse: 0.0228 - val_loss: -2.5319e-02 - val_decoder_loss: -6.0090e-02 - val_mlp_out_loss: 7.7627e-04 - val_decoder_cosine: 0.1034 - val_decoder_mae: 0.2929 - val_decoder_correlation: 0.9417 - val_decoder_rmse: 0.5131 - val_mlp_out_cosine: 0.0031 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0005 - val_mlp_out_rmse: 0.0279 - 50s/epoch - 874ms/step\n",
      "Epoch 23/200\n",
      "57/57 - 50s - loss: -9.5155e-02 - decoder_loss: -1.2692e-01 - mlp_out_loss: 5.2145e-04 - decoder_cosine: 0.2772 - decoder_mae: 0.2125 - decoder_correlation: 0.8731 - decoder_rmse: 0.3209 - mlp_out_cosine: 0.0027 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0024 - mlp_out_rmse: 0.0228 - val_loss: -2.7426e-02 - val_decoder_loss: -5.6720e-02 - val_mlp_out_loss: 7.7624e-04 - val_decoder_cosine: 0.1211 - val_decoder_mae: 0.2756 - val_decoder_correlation: 0.9435 - val_decoder_rmse: 0.4845 - val_mlp_out_cosine: 0.0046 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0018 - val_mlp_out_rmse: 0.0279 - 50s/epoch - 876ms/step\n",
      "Epoch 24/200\n",
      "57/57 - 49s - loss: -1.0275e-01 - decoder_loss: -1.2949e-01 - mlp_out_loss: 5.2137e-04 - decoder_cosine: 0.2762 - decoder_mae: 0.2033 - decoder_correlation: 0.8709 - decoder_rmse: 0.3070 - mlp_out_cosine: 0.0034 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0017 - mlp_out_rmse: 0.0228 - val_loss: -2.6879e-02 - val_decoder_loss: -5.1591e-02 - val_mlp_out_loss: 7.7614e-04 - val_decoder_cosine: 0.0904 - val_decoder_mae: 0.2562 - val_decoder_correlation: 0.9469 - val_decoder_rmse: 0.4620 - val_mlp_out_cosine: -3.3447e-04 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0014 - val_mlp_out_rmse: 0.0279 - 49s/epoch - 851ms/step\n",
      "Epoch 25/200\n",
      "57/57 - 49s - loss: -1.0614e-01 - decoder_loss: -1.2867e-01 - mlp_out_loss: 5.2131e-04 - decoder_cosine: 0.2685 - decoder_mae: 0.2017 - decoder_correlation: 0.8716 - decoder_rmse: 0.3055 - mlp_out_cosine: 0.0038 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0007 - mlp_out_rmse: 0.0228 - val_loss: -3.7492e-02 - val_decoder_loss: -5.8365e-02 - val_mlp_out_loss: 7.7610e-04 - val_decoder_cosine: 0.1030 - val_decoder_mae: 0.2670 - val_decoder_correlation: 0.9430 - val_decoder_rmse: 0.4675 - val_mlp_out_cosine: -9.9150e-04 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0014 - val_mlp_out_rmse: 0.0279 - 49s/epoch - 855ms/step\n",
      "Epoch 26/200\n",
      "57/57 - 49s - loss: -1.1232e-01 - decoder_loss: -1.3132e-01 - mlp_out_loss: 5.2128e-04 - decoder_cosine: 0.2800 - decoder_mae: 0.1981 - decoder_correlation: 0.8688 - decoder_rmse: 0.2992 - mlp_out_cosine: 0.0031 - mlp_out_mae: 0.0150 - mlp_out_correlation: 1.0003 - mlp_out_rmse: 0.0228 - val_loss: -3.8826e-02 - val_decoder_loss: -5.6480e-02 - val_mlp_out_loss: 7.7608e-04 - val_decoder_cosine: 0.1029 - val_decoder_mae: 0.2505 - val_decoder_correlation: 0.9465 - val_decoder_rmse: 0.4432 - val_mlp_out_cosine: -3.9693e-04 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0018 - val_mlp_out_rmse: 0.0279 - 49s/epoch - 861ms/step\n",
      "Epoch 27/200\n",
      "57/57 - 48s - loss: -1.1566e-01 - decoder_loss: -1.3171e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.2771 - decoder_mae: 0.1869 - decoder_correlation: 0.8685 - decoder_rmse: 0.2831 - mlp_out_cosine: 0.0029 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9989 - mlp_out_rmse: 0.0228 - val_loss: -3.8870e-02 - val_decoder_loss: -5.3821e-02 - val_mlp_out_loss: 7.7606e-04 - val_decoder_cosine: 0.0843 - val_decoder_mae: 0.2492 - val_decoder_correlation: 0.9467 - val_decoder_rmse: 0.4458 - val_mlp_out_cosine: 0.0016 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0005 - val_mlp_out_rmse: 0.0279 - 48s/epoch - 838ms/step\n",
      "Epoch 28/200\n",
      "57/57 - 48s - loss: -1.1842e-01 - decoder_loss: -1.3198e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.2791 - decoder_mae: 0.1882 - decoder_correlation: 0.8680 - decoder_rmse: 0.2858 - mlp_out_cosine: 0.0022 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9987 - mlp_out_rmse: 0.0228 - val_loss: -4.1355e-02 - val_decoder_loss: -5.4036e-02 - val_mlp_out_loss: 7.7608e-04 - val_decoder_cosine: 0.0775 - val_decoder_mae: 0.2573 - val_decoder_correlation: 0.9450 - val_decoder_rmse: 0.4668 - val_mlp_out_cosine: -2.1834e-04 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0021 - val_mlp_out_rmse: 0.0279 - 48s/epoch - 845ms/step\n",
      "Epoch 29/200\n",
      "57/57 - 50s - loss: -1.2161e-01 - decoder_loss: -1.3308e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.2861 - decoder_mae: 0.1842 - decoder_correlation: 0.8673 - decoder_rmse: 0.2798 - mlp_out_cosine: 0.0020 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9991 - mlp_out_rmse: 0.0228 - val_loss: -4.9631e-02 - val_decoder_loss: -6.0404e-02 - val_mlp_out_loss: 7.7609e-04 - val_decoder_cosine: 0.1054 - val_decoder_mae: 0.2619 - val_decoder_correlation: 0.9412 - val_decoder_rmse: 0.4755 - val_mlp_out_cosine: -2.4985e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0023 - val_mlp_out_rmse: 0.0279 - 50s/epoch - 877ms/step\n",
      "Epoch 30/200\n",
      "57/57 - 50s - loss: -1.2391e-01 - decoder_loss: -1.3362e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.2840 - decoder_mae: 0.1783 - decoder_correlation: 0.8664 - decoder_rmse: 0.2709 - mlp_out_cosine: 0.0014 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9983 - mlp_out_rmse: 0.0228 - val_loss: -4.8522e-02 - val_decoder_loss: -5.7690e-02 - val_mlp_out_loss: 7.7608e-04 - val_decoder_cosine: 0.1084 - val_decoder_mae: 0.2459 - val_decoder_correlation: 0.9421 - val_decoder_rmse: 0.4422 - val_mlp_out_cosine: -2.8587e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0012 - val_mlp_out_rmse: 0.0279 - 50s/epoch - 872ms/step\n",
      "Epoch 31/200\n",
      "57/57 - 48s - loss: -1.2550e-01 - decoder_loss: -1.3374e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.2891 - decoder_mae: 0.1793 - decoder_correlation: 0.8666 - decoder_rmse: 0.2730 - mlp_out_cosine: 2.8900e-04 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9980 - mlp_out_rmse: 0.0228 - val_loss: -4.8831e-02 - val_decoder_loss: -5.6651e-02 - val_mlp_out_loss: 7.7609e-04 - val_decoder_cosine: 0.1077 - val_decoder_mae: 0.2357 - val_decoder_correlation: 0.9437 - val_decoder_rmse: 0.4186 - val_mlp_out_cosine: -9.1722e-04 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0024 - val_mlp_out_rmse: 0.0279 - 48s/epoch - 845ms/step\n",
      "Epoch 32/200\n",
      "57/57 - 49s - loss: -1.2876e-01 - decoder_loss: -1.3575e-01 - mlp_out_loss: 5.2123e-04 - decoder_cosine: 0.2903 - decoder_mae: 0.1707 - decoder_correlation: 0.8644 - decoder_rmse: 0.2603 - mlp_out_cosine: -8.0192e-04 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9975 - mlp_out_rmse: 0.0228 - val_loss: -4.3703e-02 - val_decoder_loss: -5.0389e-02 - val_mlp_out_loss: 7.7609e-04 - val_decoder_cosine: 0.0732 - val_decoder_mae: 0.2236 - val_decoder_correlation: 0.9477 - val_decoder_rmse: 0.4071 - val_mlp_out_cosine: -3.1839e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0031 - val_mlp_out_rmse: 0.0279 - 49s/epoch - 851ms/step\n",
      "Epoch 33/200\n",
      "57/57 - 45s - loss: -1.3062e-01 - decoder_loss: -1.3657e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.2814 - decoder_mae: 0.1646 - decoder_correlation: 0.8635 - decoder_rmse: 0.2518 - mlp_out_cosine: -6.0171e-04 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9970 - mlp_out_rmse: 0.0228 - val_loss: -5.0171e-02 - val_decoder_loss: -5.5903e-02 - val_mlp_out_loss: 7.7611e-04 - val_decoder_cosine: 0.1151 - val_decoder_mae: 0.2072 - val_decoder_correlation: 0.9499 - val_decoder_rmse: 0.3643 - val_mlp_out_cosine: -1.3576e-04 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0018 - val_mlp_out_rmse: 0.0279 - 45s/epoch - 797ms/step\n",
      "Epoch 34/200\n",
      "57/57 - 49s - loss: -1.3090e-01 - decoder_loss: -1.3598e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.2838 - decoder_mae: 0.1669 - decoder_correlation: 0.8643 - decoder_rmse: 0.2550 - mlp_out_cosine: -1.6935e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9966 - mlp_out_rmse: 0.0228 - val_loss: -5.2695e-02 - val_decoder_loss: -5.7626e-02 - val_mlp_out_loss: 7.7610e-04 - val_decoder_cosine: 0.1247 - val_decoder_mae: 0.2186 - val_decoder_correlation: 0.9454 - val_decoder_rmse: 0.3897 - val_mlp_out_cosine: -3.3084e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0024 - val_mlp_out_rmse: 0.0279 - 49s/epoch - 854ms/step\n",
      "Epoch 35/200\n",
      "57/57 - 48s - loss: -1.3369e-01 - decoder_loss: -1.3803e-01 - mlp_out_loss: 5.2123e-04 - decoder_cosine: 0.2925 - decoder_mae: 0.1567 - decoder_correlation: 0.8620 - decoder_rmse: 0.2398 - mlp_out_cosine: -1.6435e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9965 - mlp_out_rmse: 0.0228 - val_loss: -5.3341e-02 - val_decoder_loss: -5.7598e-02 - val_mlp_out_loss: 7.7611e-04 - val_decoder_cosine: 0.1050 - val_decoder_mae: 0.2207 - val_decoder_correlation: 0.9445 - val_decoder_rmse: 0.3932 - val_mlp_out_cosine: -3.3036e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0027 - val_mlp_out_rmse: 0.0279 - 48s/epoch - 840ms/step\n",
      "Epoch 36/200\n",
      "57/57 - 47s - loss: -1.3381e-01 - decoder_loss: -1.3753e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.2863 - decoder_mae: 0.1638 - decoder_correlation: 0.8623 - decoder_rmse: 0.2503 - mlp_out_cosine: -1.3031e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9958 - mlp_out_rmse: 0.0228 - val_loss: -4.8078e-02 - val_decoder_loss: -5.1769e-02 - val_mlp_out_loss: 7.7610e-04 - val_decoder_cosine: 0.0823 - val_decoder_mae: 0.2120 - val_decoder_correlation: 0.9463 - val_decoder_rmse: 0.3909 - val_mlp_out_cosine: -3.6869e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0023 - val_mlp_out_rmse: 0.0279 - 47s/epoch - 816ms/step\n",
      "Epoch 37/200\n",
      "57/57 - 47s - loss: -1.3397e-01 - decoder_loss: -1.3717e-01 - mlp_out_loss: 5.2123e-04 - decoder_cosine: 0.2893 - decoder_mae: 0.1581 - decoder_correlation: 0.8633 - decoder_rmse: 0.2428 - mlp_out_cosine: -1.3318e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9955 - mlp_out_rmse: 0.0228 - val_loss: -5.4558e-02 - val_decoder_loss: -5.7775e-02 - val_mlp_out_loss: 7.7609e-04 - val_decoder_cosine: 0.0883 - val_decoder_mae: 0.2283 - val_decoder_correlation: 0.9463 - val_decoder_rmse: 0.4200 - val_mlp_out_cosine: -3.7389e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0018 - val_mlp_out_rmse: 0.0279 - 47s/epoch - 830ms/step\n",
      "Epoch 38/200\n",
      "57/57 - 48s - loss: -1.3674e-01 - decoder_loss: -1.3950e-01 - mlp_out_loss: 5.2123e-04 - decoder_cosine: 0.2931 - decoder_mae: 0.1530 - decoder_correlation: 0.8606 - decoder_rmse: 0.2349 - mlp_out_cosine: -1.7951e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9952 - mlp_out_rmse: 0.0228 - val_loss: -6.0184e-02 - val_decoder_loss: -6.3001e-02 - val_mlp_out_loss: 7.7610e-04 - val_decoder_cosine: 0.1276 - val_decoder_mae: 0.2226 - val_decoder_correlation: 0.9405 - val_decoder_rmse: 0.3931 - val_mlp_out_cosine: -3.7211e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0014 - val_mlp_out_rmse: 0.0279 - 48s/epoch - 847ms/step\n",
      "Epoch 39/200\n",
      "57/57 - 48s - loss: -1.3870e-01 - decoder_loss: -1.4109e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.2932 - decoder_mae: 0.1451 - decoder_correlation: 0.8591 - decoder_rmse: 0.2227 - mlp_out_cosine: -3.1161e-04 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9941 - mlp_out_rmse: 0.0228 - val_loss: -4.1120e-02 - val_decoder_loss: -4.3603e-02 - val_mlp_out_loss: 7.7611e-04 - val_decoder_cosine: 0.0717 - val_decoder_mae: 0.1783 - val_decoder_correlation: 0.9510 - val_decoder_rmse: 0.3264 - val_mlp_out_cosine: -3.7486e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0035 - val_mlp_out_rmse: 0.0279 - 48s/epoch - 839ms/step\n",
      "Epoch 40/200\n",
      "57/57 - 48s - loss: -1.3675e-01 - decoder_loss: -1.3883e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.3026 - decoder_mae: 0.1492 - decoder_correlation: 0.8611 - decoder_rmse: 0.2308 - mlp_out_cosine: -1.5782e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9948 - mlp_out_rmse: 0.0228 - val_loss: -6.3579e-02 - val_decoder_loss: -6.5781e-02 - val_mlp_out_loss: 7.7611e-04 - val_decoder_cosine: 0.1358 - val_decoder_mae: 0.2332 - val_decoder_correlation: 0.9427 - val_decoder_rmse: 0.4088 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0030 - val_mlp_out_rmse: 0.0279 - 48s/epoch - 840ms/step\n",
      "Epoch 41/200\n",
      "57/57 - 53s - loss: -1.3838e-01 - decoder_loss: -1.4021e-01 - mlp_out_loss: 5.2125e-04 - decoder_cosine: 0.2911 - decoder_mae: 0.1556 - decoder_correlation: 0.8601 - decoder_rmse: 0.2407 - mlp_out_cosine: -1.7201e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9932 - mlp_out_rmse: 0.0228 - val_loss: -4.6439e-02 - val_decoder_loss: -4.8406e-02 - val_mlp_out_loss: 7.7610e-04 - val_decoder_cosine: 0.0811 - val_decoder_mae: 0.1889 - val_decoder_correlation: 0.9501 - val_decoder_rmse: 0.3492 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0016 - val_mlp_out_rmse: 0.0279 - 53s/epoch - 927ms/step\n",
      "Epoch 42/200\n",
      "57/57 - 50s - loss: -1.4120e-01 - decoder_loss: -1.4281e-01 - mlp_out_loss: 5.2124e-04 - decoder_cosine: 0.2970 - decoder_mae: 0.1403 - decoder_correlation: 0.8573 - decoder_rmse: 0.2168 - mlp_out_cosine: -1.0200e-03 - mlp_out_mae: 0.0150 - mlp_out_correlation: 0.9932 - mlp_out_rmse: 0.0228 - val_loss: -5.5553e-02 - val_decoder_loss: -5.7322e-02 - val_mlp_out_loss: 7.7611e-04 - val_decoder_cosine: 0.1272 - val_decoder_mae: 0.1839 - val_decoder_correlation: 0.9479 - val_decoder_rmse: 0.3251 - val_mlp_out_cosine: -3.7202e-03 - val_mlp_out_mae: 0.0184 - val_mlp_out_correlation: 1.0017 - val_mlp_out_rmse: 0.0279 - 50s/epoch - 876ms/step\n",
      "================================================================================================\n",
      ">>> AEMLP_FOLD:1\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_2), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(34, 1) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "Epoch 1/200\n",
      "230/230 - 14s - loss: 0.6479 - decoder_loss: -5.2780e-02 - mlp_out_loss: 0.0112 - decoder_cosine: 0.1339 - decoder_mae: 0.4783 - decoder_correlation: 0.9474 - decoder_rmse: 0.6854 - mlp_out_cosine: 0.0043 - mlp_out_mae: 0.0302 - mlp_out_correlation: 0.9894 - mlp_out_rmse: 0.1060 - val_loss: 0.2757 - val_decoder_loss: -2.6967e-02 - val_mlp_out_loss: 6.4006e-04 - val_decoder_cosine: 0.0792 - val_decoder_mae: 0.3855 - val_decoder_correlation: 0.9687 - val_decoder_rmse: 0.4808 - val_mlp_out_cosine: 0.0067 - val_mlp_out_mae: 0.0173 - val_mlp_out_correlation: 0.9784 - val_mlp_out_rmse: 0.0253 - 14s/epoch - 62ms/step\n",
      "Epoch 2/200\n",
      "230/230 - 8s - loss: 0.0580 - decoder_loss: -1.1807e-01 - mlp_out_loss: 7.7592e-04 - decoder_cosine: 0.2404 - decoder_mae: 0.4282 - decoder_correlation: 0.8825 - decoder_rmse: 0.6171 - mlp_out_cosine: 0.0150 - mlp_out_mae: 0.0177 - mlp_out_correlation: 0.9749 - mlp_out_rmse: 0.0279 - val_loss: 0.0821 - val_decoder_loss: -1.6866e-02 - val_mlp_out_loss: 6.4128e-04 - val_decoder_cosine: 0.0553 - val_decoder_mae: 0.2111 - val_decoder_correlation: 0.9743 - val_decoder_rmse: 0.2745 - val_mlp_out_cosine: -1.3737e-02 - val_mlp_out_mae: 0.0174 - val_mlp_out_correlation: 0.9873 - val_mlp_out_rmse: 0.0253 - 8s/epoch - 37ms/step\n",
      "Epoch 3/200\n",
      "230/230 - 9s - loss: -8.4772e-02 - decoder_loss: -1.5237e-01 - mlp_out_loss: 6.7518e-04 - decoder_cosine: 0.2673 - decoder_mae: 0.2640 - decoder_correlation: 0.8482 - decoder_rmse: 0.4110 - mlp_out_cosine: 0.0129 - mlp_out_mae: 0.0171 - mlp_out_correlation: 0.9735 - mlp_out_rmse: 0.0260 - val_loss: 0.0338 - val_decoder_loss: -1.0942e-02 - val_mlp_out_loss: 6.3140e-04 - val_decoder_cosine: 0.0082 - val_decoder_mae: 0.1819 - val_decoder_correlation: 0.9811 - val_decoder_rmse: 0.2522 - val_mlp_out_cosine: -1.1942e-02 - val_mlp_out_mae: 0.0172 - val_mlp_out_correlation: 0.9877 - val_mlp_out_rmse: 0.0251 - 9s/epoch - 37ms/step\n",
      "Epoch 4/200\n",
      "230/230 - 10s - loss: -1.3380e-01 - decoder_loss: -1.6583e-01 - mlp_out_loss: 6.3411e-04 - decoder_cosine: 0.2877 - decoder_mae: 0.2222 - decoder_correlation: 0.8347 - decoder_rmse: 0.3500 - mlp_out_cosine: 0.0144 - mlp_out_mae: 0.0167 - mlp_out_correlation: 0.9667 - mlp_out_rmse: 0.0252 - val_loss: 7.1013e-04 - val_decoder_loss: -2.1281e-02 - val_mlp_out_loss: 6.2160e-04 - val_decoder_cosine: 0.0213 - val_decoder_mae: 0.1733 - val_decoder_correlation: 0.9788 - val_decoder_rmse: 0.2505 - val_mlp_out_cosine: -8.2650e-03 - val_mlp_out_mae: 0.0171 - val_mlp_out_correlation: 0.9836 - val_mlp_out_rmse: 0.0249 - 10s/epoch - 44ms/step\n",
      "Epoch 5/200\n",
      "230/230 - 9s - loss: -1.6053e-01 - decoder_loss: -1.7641e-01 - mlp_out_loss: 6.2714e-04 - decoder_cosine: 0.3066 - decoder_mae: 0.1943 - decoder_correlation: 0.8241 - decoder_rmse: 0.3083 - mlp_out_cosine: 0.0196 - mlp_out_mae: 0.0166 - mlp_out_correlation: 0.9632 - mlp_out_rmse: 0.0250 - val_loss: 2.4218e-04 - val_decoder_loss: -1.0716e-02 - val_mlp_out_loss: 6.2083e-04 - val_decoder_cosine: 3.4133e-04 - val_decoder_mae: 0.1557 - val_decoder_correlation: 0.9838 - val_decoder_rmse: 0.2227 - val_mlp_out_cosine: -1.0858e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 0.9802 - val_mlp_out_rmse: 0.0249 - 9s/epoch - 39ms/step\n",
      "Epoch 6/200\n",
      "230/230 - 7s - loss: -1.7615e-01 - decoder_loss: -1.8407e-01 - mlp_out_loss: 6.2731e-04 - decoder_cosine: 0.3127 - decoder_mae: 0.1699 - decoder_correlation: 0.8162 - decoder_rmse: 0.2716 - mlp_out_cosine: 0.0257 - mlp_out_mae: 0.0166 - mlp_out_correlation: 0.9636 - mlp_out_rmse: 0.0250 - val_loss: -2.0472e-02 - val_decoder_loss: -2.5942e-02 - val_mlp_out_loss: 6.2064e-04 - val_decoder_cosine: 0.0449 - val_decoder_mae: 0.1461 - val_decoder_correlation: 0.9783 - val_decoder_rmse: 0.2123 - val_mlp_out_cosine: -1.2522e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 0.9780 - val_mlp_out_rmse: 0.0249 - 7s/epoch - 30ms/step\n",
      "Epoch 7/200\n",
      "230/230 - 8s - loss: -1.8386e-01 - decoder_loss: -1.8785e-01 - mlp_out_loss: 6.2770e-04 - decoder_cosine: 0.3234 - decoder_mae: 0.1581 - decoder_correlation: 0.8122 - decoder_rmse: 0.2543 - mlp_out_cosine: 0.0195 - mlp_out_mae: 0.0166 - mlp_out_correlation: 0.9647 - mlp_out_rmse: 0.0251 - val_loss: -6.0500e-03 - val_decoder_loss: -8.8444e-03 - val_mlp_out_loss: 6.2178e-04 - val_decoder_cosine: -8.8289e-03 - val_decoder_mae: 0.1342 - val_decoder_correlation: 0.9842 - val_decoder_rmse: 0.1946 - val_mlp_out_cosine: -1.9934e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 0.9778 - val_mlp_out_rmse: 0.0249 - 8s/epoch - 35ms/step\n",
      "Epoch 8/200\n",
      "230/230 - 9s - loss: -1.9183e-01 - decoder_loss: -1.9392e-01 - mlp_out_loss: 6.2781e-04 - decoder_cosine: 0.3376 - decoder_mae: 0.1467 - decoder_correlation: 0.8066 - decoder_rmse: 0.2366 - mlp_out_cosine: 0.0207 - mlp_out_mae: 0.0166 - mlp_out_correlation: 0.9642 - mlp_out_rmse: 0.0251 - val_loss: -1.4401e-02 - val_decoder_loss: -1.5944e-02 - val_mlp_out_loss: 6.2082e-04 - val_decoder_cosine: 0.0114 - val_decoder_mae: 0.1214 - val_decoder_correlation: 0.9849 - val_decoder_rmse: 0.1754 - val_mlp_out_cosine: -1.9829e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 0.9770 - val_mlp_out_rmse: 0.0249 - 9s/epoch - 38ms/step\n",
      "Epoch 9/200\n",
      "230/230 - 8s - loss: -1.9607e-01 - decoder_loss: -1.9731e-01 - mlp_out_loss: 6.2781e-04 - decoder_cosine: 0.3433 - decoder_mae: 0.1375 - decoder_correlation: 0.8030 - decoder_rmse: 0.2228 - mlp_out_cosine: 0.0217 - mlp_out_mae: 0.0166 - mlp_out_correlation: 0.9640 - mlp_out_rmse: 0.0251 - val_loss: -1.0414e-02 - val_decoder_loss: -1.1404e-02 - val_mlp_out_loss: 6.2121e-04 - val_decoder_cosine: -1.4678e-02 - val_decoder_mae: 0.1126 - val_decoder_correlation: 0.9889 - val_decoder_rmse: 0.1623 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 0.9768 - val_mlp_out_rmse: 0.0249 - 8s/epoch - 33ms/step\n",
      "Epoch 10/200\n",
      "230/230 - 10s - loss: -1.9973e-01 - decoder_loss: -2.0059e-01 - mlp_out_loss: 6.2783e-04 - decoder_cosine: 0.3425 - decoder_mae: 0.1290 - decoder_correlation: 0.7997 - decoder_rmse: 0.2095 - mlp_out_cosine: 0.0208 - mlp_out_mae: 0.0166 - mlp_out_correlation: 0.9622 - mlp_out_rmse: 0.0251 - val_loss: -1.1879e-02 - val_decoder_loss: -1.2641e-02 - val_mlp_out_loss: 6.2328e-04 - val_decoder_cosine: -9.5709e-03 - val_decoder_mae: 0.1053 - val_decoder_correlation: 0.9859 - val_decoder_rmse: 0.1509 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 0.9773 - val_mlp_out_rmse: 0.0250 - 10s/epoch - 42ms/step\n",
      "Epoch 11/200\n",
      "230/230 - 11s - loss: -2.0219e-01 - decoder_loss: -2.0290e-01 - mlp_out_loss: 6.2792e-04 - decoder_cosine: 0.3446 - decoder_mae: 0.1203 - decoder_correlation: 0.7971 - decoder_rmse: 0.1951 - mlp_out_cosine: 0.0139 - mlp_out_mae: 0.0166 - mlp_out_correlation: 0.9615 - mlp_out_rmse: 0.0251 - val_loss: -1.2424e-02 - val_decoder_loss: -1.3095e-02 - val_mlp_out_loss: 6.2143e-04 - val_decoder_cosine: -1.1216e-02 - val_decoder_mae: 0.1000 - val_decoder_correlation: 0.9850 - val_decoder_rmse: 0.1456 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 0.9775 - val_mlp_out_rmse: 0.0249 - 11s/epoch - 50ms/step\n",
      "Epoch 12/200\n",
      "230/230 - 9s - loss: -2.0549e-01 - decoder_loss: -2.0615e-01 - mlp_out_loss: 6.2787e-04 - decoder_cosine: 0.3403 - decoder_mae: 0.1175 - decoder_correlation: 0.7941 - decoder_rmse: 0.1908 - mlp_out_cosine: 0.0211 - mlp_out_mae: 0.0166 - mlp_out_correlation: 0.9588 - mlp_out_rmse: 0.0251 - val_loss: -1.1367e-02 - val_decoder_loss: -1.2005e-02 - val_mlp_out_loss: 6.2182e-04 - val_decoder_cosine: -6.9066e-03 - val_decoder_mae: 0.0962 - val_decoder_correlation: 0.9900 - val_decoder_rmse: 0.1383 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 0.9781 - val_mlp_out_rmse: 0.0249 - 9s/epoch - 40ms/step\n",
      "Epoch 13/200\n",
      "230/230 - 7s - loss: -2.0852e-01 - decoder_loss: -2.0916e-01 - mlp_out_loss: 6.2790e-04 - decoder_cosine: 0.3448 - decoder_mae: 0.1093 - decoder_correlation: 0.7910 - decoder_rmse: 0.1777 - mlp_out_cosine: 0.0179 - mlp_out_mae: 0.0166 - mlp_out_correlation: 0.9707 - mlp_out_rmse: 0.0251 - val_loss: -1.1207e-03 - val_decoder_loss: -1.7471e-03 - val_mlp_out_loss: 6.2155e-04 - val_decoder_cosine: -7.6235e-02 - val_decoder_mae: 0.0885 - val_decoder_correlation: 0.9951 - val_decoder_rmse: 0.1271 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 0.9878 - val_mlp_out_rmse: 0.0249 - 7s/epoch - 33ms/step\n",
      "Epoch 14/200\n",
      "230/230 - 8s - loss: -2.1227e-01 - decoder_loss: -2.1290e-01 - mlp_out_loss: 6.2788e-04 - decoder_cosine: 0.3544 - decoder_mae: 0.1006 - decoder_correlation: 0.7876 - decoder_rmse: 0.1652 - mlp_out_cosine: 0.0159 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -6.6785e-03 - val_decoder_loss: -7.3004e-03 - val_mlp_out_loss: 6.2044e-04 - val_decoder_cosine: -2.2241e-02 - val_decoder_mae: 0.0855 - val_decoder_correlation: 0.9890 - val_decoder_rmse: 0.1229 - val_mlp_out_cosine: -3.3090e-03 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 0.9939 - val_mlp_out_rmse: 0.0249 - 8s/epoch - 34ms/step\n",
      "Epoch 15/200\n",
      "230/230 - 8s - loss: -2.1745e-01 - decoder_loss: -2.1808e-01 - mlp_out_loss: 6.2785e-04 - decoder_cosine: 0.3754 - decoder_mae: 0.0955 - decoder_correlation: 0.7822 - decoder_rmse: 0.1580 - mlp_out_cosine: 0.0146 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: 0.0029 - val_decoder_loss: 0.0023 - val_mlp_out_loss: 6.2320e-04 - val_decoder_cosine: -6.6058e-02 - val_decoder_mae: 0.0752 - val_decoder_correlation: 0.9976 - val_decoder_rmse: 0.1104 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0250 - 8s/epoch - 36ms/step\n",
      "Epoch 16/200\n",
      "230/230 - 11s - loss: -2.2116e-01 - decoder_loss: -2.2179e-01 - mlp_out_loss: 6.2788e-04 - decoder_cosine: 0.3702 - decoder_mae: 0.0868 - decoder_correlation: 0.7786 - decoder_rmse: 0.1447 - mlp_out_cosine: 0.0155 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -3.4443e-03 - val_decoder_loss: -4.0674e-03 - val_mlp_out_loss: 6.2304e-04 - val_decoder_cosine: -3.8376e-02 - val_decoder_mae: 0.0705 - val_decoder_correlation: 0.9901 - val_decoder_rmse: 0.1059 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0250 - 11s/epoch - 47ms/step\n",
      "Epoch 17/200\n",
      "230/230 - 12s - loss: -2.2415e-01 - decoder_loss: -2.2477e-01 - mlp_out_loss: 6.2793e-04 - decoder_cosine: 0.3765 - decoder_mae: 0.0822 - decoder_correlation: 0.7755 - decoder_rmse: 0.1383 - mlp_out_cosine: 0.0157 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: 0.0014 - val_decoder_loss: 7.3692e-04 - val_mlp_out_loss: 6.2293e-04 - val_decoder_cosine: -6.3372e-02 - val_decoder_mae: 0.0687 - val_decoder_correlation: 0.9914 - val_decoder_rmse: 0.1013 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0250 - 12s/epoch - 52ms/step\n",
      "Epoch 18/200\n",
      "230/230 - 9s - loss: -2.2831e-01 - decoder_loss: -2.2894e-01 - mlp_out_loss: 6.2788e-04 - decoder_cosine: 0.3814 - decoder_mae: 0.0772 - decoder_correlation: 0.7714 - decoder_rmse: 0.1311 - mlp_out_cosine: 0.0183 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -1.4553e-02 - val_decoder_loss: -1.5175e-02 - val_mlp_out_loss: 6.2152e-04 - val_decoder_cosine: -1.8293e-02 - val_decoder_mae: 0.0615 - val_decoder_correlation: 0.9874 - val_decoder_rmse: 0.0915 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 9s/epoch - 40ms/step\n",
      "Epoch 19/200\n",
      "230/230 - 7s - loss: -2.3031e-01 - decoder_loss: -2.3094e-01 - mlp_out_loss: 6.2790e-04 - decoder_cosine: 0.3874 - decoder_mae: 0.0759 - decoder_correlation: 0.7691 - decoder_rmse: 0.1295 - mlp_out_cosine: 0.0183 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -2.2504e-03 - val_decoder_loss: -2.8746e-03 - val_mlp_out_loss: 6.2421e-04 - val_decoder_cosine: -5.0595e-02 - val_decoder_mae: 0.0619 - val_decoder_correlation: 0.9916 - val_decoder_rmse: 0.0922 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0171 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0250 - 7s/epoch - 32ms/step\n",
      "Epoch 20/200\n",
      "230/230 - 10s - loss: -2.3372e-01 - decoder_loss: -2.3435e-01 - mlp_out_loss: 6.2789e-04 - decoder_cosine: 0.3871 - decoder_mae: 0.0740 - decoder_correlation: 0.7657 - decoder_rmse: 0.1274 - mlp_out_cosine: 0.0152 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -1.1687e-02 - val_decoder_loss: -1.2306e-02 - val_mlp_out_loss: 6.1898e-04 - val_decoder_cosine: -1.7120e-02 - val_decoder_mae: 0.0633 - val_decoder_correlation: 0.9883 - val_decoder_rmse: 0.0929 - val_mlp_out_cosine: 0.0198 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 10s/epoch - 42ms/step\n",
      "Epoch 21/200\n",
      "230/230 - 11s - loss: -2.3621e-01 - decoder_loss: -2.3684e-01 - mlp_out_loss: 6.2806e-04 - decoder_cosine: 0.3967 - decoder_mae: 0.0719 - decoder_correlation: 0.7634 - decoder_rmse: 0.1249 - mlp_out_cosine: 0.0132 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -1.4015e-02 - val_decoder_loss: -1.4634e-02 - val_mlp_out_loss: 6.1922e-04 - val_decoder_cosine: -2.3043e-02 - val_decoder_mae: 0.0612 - val_decoder_correlation: 0.9859 - val_decoder_rmse: 0.0902 - val_mlp_out_cosine: 0.0198 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 11s/epoch - 47ms/step\n",
      "Epoch 22/200\n",
      "230/230 - 8s - loss: -2.3779e-01 - decoder_loss: -2.3841e-01 - mlp_out_loss: 6.2809e-04 - decoder_cosine: 0.4044 - decoder_mae: 0.0705 - decoder_correlation: 0.7622 - decoder_rmse: 0.1230 - mlp_out_cosine: 0.0109 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: 0.0014 - val_decoder_loss: 7.8748e-04 - val_mlp_out_loss: 6.2448e-04 - val_decoder_cosine: -3.0121e-02 - val_decoder_mae: 0.0590 - val_decoder_correlation: 0.9941 - val_decoder_rmse: 0.0890 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0171 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0250 - 8s/epoch - 35ms/step\n",
      "Epoch 23/200\n",
      "230/230 - 8s - loss: -2.4019e-01 - decoder_loss: -2.4082e-01 - mlp_out_loss: 6.2792e-04 - decoder_cosine: 0.4019 - decoder_mae: 0.0689 - decoder_correlation: 0.7596 - decoder_rmse: 0.1209 - mlp_out_cosine: 0.0137 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: 8.5753e-04 - val_decoder_loss: 2.3861e-04 - val_mlp_out_loss: 6.1891e-04 - val_decoder_cosine: -2.1573e-02 - val_decoder_mae: 0.0606 - val_decoder_correlation: 0.9924 - val_decoder_rmse: 0.0923 - val_mlp_out_cosine: 0.0198 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 8s/epoch - 36ms/step\n",
      "Epoch 24/200\n",
      "230/230 - 9s - loss: -2.4204e-01 - decoder_loss: -2.4267e-01 - mlp_out_loss: 6.2808e-04 - decoder_cosine: 0.4065 - decoder_mae: 0.0699 - decoder_correlation: 0.7576 - decoder_rmse: 0.1233 - mlp_out_cosine: 0.0105 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -6.1619e-03 - val_decoder_loss: -6.7887e-03 - val_mlp_out_loss: 6.2681e-04 - val_decoder_cosine: -2.4082e-02 - val_decoder_mae: 0.0572 - val_decoder_correlation: 0.9888 - val_decoder_rmse: 0.0854 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0171 - val_mlp_out_correlation: nan - val_mlp_out_rmse: 0.0250 - 9s/epoch - 39ms/step\n",
      "Epoch 25/200\n",
      "230/230 - 10s - loss: -2.4400e-01 - decoder_loss: -2.4463e-01 - mlp_out_loss: 6.2817e-04 - decoder_cosine: 0.4138 - decoder_mae: 0.0692 - decoder_correlation: 0.7553 - decoder_rmse: 0.1224 - mlp_out_cosine: 0.0105 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -4.0323e-03 - val_decoder_loss: -4.6528e-03 - val_mlp_out_loss: 6.2048e-04 - val_decoder_cosine: -2.7145e-03 - val_decoder_mae: 0.0585 - val_decoder_correlation: 0.9899 - val_decoder_rmse: 0.0871 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: nan - val_mlp_out_rmse: 0.0249 - 10s/epoch - 42ms/step\n",
      "Epoch 26/200\n",
      "230/230 - 10s - loss: -2.4592e-01 - decoder_loss: -2.4655e-01 - mlp_out_loss: 6.2798e-04 - decoder_cosine: 0.4080 - decoder_mae: 0.0669 - decoder_correlation: 0.7535 - decoder_rmse: 0.1191 - mlp_out_cosine: 0.0142 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -1.8397e-03 - val_decoder_loss: -2.4654e-03 - val_mlp_out_loss: 6.2569e-04 - val_decoder_cosine: -3.4289e-02 - val_decoder_mae: 0.0586 - val_decoder_correlation: 0.9932 - val_decoder_rmse: 0.0865 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0171 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0250 - 10s/epoch - 43ms/step\n",
      "Epoch 27/200\n",
      "230/230 - 11s - loss: -2.4855e-01 - decoder_loss: -2.4918e-01 - mlp_out_loss: 6.2815e-04 - decoder_cosine: 0.4126 - decoder_mae: 0.0666 - decoder_correlation: 0.7510 - decoder_rmse: 0.1189 - mlp_out_cosine: 0.0111 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -3.1978e-03 - val_decoder_loss: -3.8258e-03 - val_mlp_out_loss: 6.2797e-04 - val_decoder_cosine: -1.9232e-02 - val_decoder_mae: 0.0611 - val_decoder_correlation: 0.9904 - val_decoder_rmse: 0.0896 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0171 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0251 - 11s/epoch - 48ms/step\n",
      "Epoch 28/200\n",
      "230/230 - 9s - loss: -2.5018e-01 - decoder_loss: -2.5080e-01 - mlp_out_loss: 6.2816e-04 - decoder_cosine: 0.4165 - decoder_mae: 0.0662 - decoder_correlation: 0.7494 - decoder_rmse: 0.1189 - mlp_out_cosine: 0.0099 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -3.3760e-03 - val_decoder_loss: -3.9967e-03 - val_mlp_out_loss: 6.2076e-04 - val_decoder_cosine: -4.9158e-03 - val_decoder_mae: 0.0570 - val_decoder_correlation: 0.9962 - val_decoder_rmse: 0.0853 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 9s/epoch - 41ms/step\n",
      "Epoch 29/200\n",
      "230/230 - 9s - loss: -2.5070e-01 - decoder_loss: -2.5133e-01 - mlp_out_loss: 6.2810e-04 - decoder_cosine: 0.4132 - decoder_mae: 0.0662 - decoder_correlation: 0.7489 - decoder_rmse: 0.1190 - mlp_out_cosine: 0.0123 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -4.2351e-03 - val_decoder_loss: -4.8590e-03 - val_mlp_out_loss: 6.2392e-04 - val_decoder_cosine: -2.5108e-02 - val_decoder_mae: 0.0552 - val_decoder_correlation: 0.9935 - val_decoder_rmse: 0.0828 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0171 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0250 - 9s/epoch - 41ms/step\n",
      "Epoch 30/200\n",
      "230/230 - 10s - loss: -2.5174e-01 - decoder_loss: -2.5237e-01 - mlp_out_loss: 6.2817e-04 - decoder_cosine: 0.4228 - decoder_mae: 0.0659 - decoder_correlation: 0.7478 - decoder_rmse: 0.1191 - mlp_out_cosine: 0.0108 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: 0.0017 - val_decoder_loss: 0.0011 - val_mlp_out_loss: 6.2008e-04 - val_decoder_cosine: -4.8478e-02 - val_decoder_mae: 0.0565 - val_decoder_correlation: 0.9928 - val_decoder_rmse: 0.0841 - val_mlp_out_cosine: 0.0198 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 10s/epoch - 42ms/step\n",
      "Epoch 31/200\n",
      "230/230 - 11s - loss: -2.5388e-01 - decoder_loss: -2.5451e-01 - mlp_out_loss: 6.2822e-04 - decoder_cosine: 0.4217 - decoder_mae: 0.0657 - decoder_correlation: 0.7457 - decoder_rmse: 0.1190 - mlp_out_cosine: 0.0078 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -5.1097e-03 - val_decoder_loss: -5.7302e-03 - val_mlp_out_loss: 6.2053e-04 - val_decoder_cosine: -2.8720e-02 - val_decoder_mae: 0.0554 - val_decoder_correlation: 0.9950 - val_decoder_rmse: 0.0820 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: nan - val_mlp_out_rmse: 0.0249 - 11s/epoch - 47ms/step\n",
      "Epoch 32/200\n",
      "230/230 - 9s - loss: -2.5611e-01 - decoder_loss: -2.5674e-01 - mlp_out_loss: 6.2813e-04 - decoder_cosine: 0.4306 - decoder_mae: 0.0649 - decoder_correlation: 0.7432 - decoder_rmse: 0.1182 - mlp_out_cosine: 0.0114 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -3.7233e-03 - val_decoder_loss: -4.3429e-03 - val_mlp_out_loss: 6.1960e-04 - val_decoder_cosine: -1.6671e-02 - val_decoder_mae: 0.0544 - val_decoder_correlation: 0.9927 - val_decoder_rmse: 0.0811 - val_mlp_out_cosine: 0.0198 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 9s/epoch - 41ms/step\n",
      "Epoch 33/200\n",
      "230/230 - 9s - loss: -2.5546e-01 - decoder_loss: -2.5609e-01 - mlp_out_loss: 6.2801e-04 - decoder_cosine: 0.4250 - decoder_mae: 0.0655 - decoder_correlation: 0.7440 - decoder_rmse: 0.1191 - mlp_out_cosine: 0.0134 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -4.6947e-03 - val_decoder_loss: -5.3172e-03 - val_mlp_out_loss: 6.2250e-04 - val_decoder_cosine: -2.2552e-02 - val_decoder_mae: 0.0542 - val_decoder_correlation: 0.9917 - val_decoder_rmse: 0.0820 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 9s/epoch - 37ms/step\n",
      "Epoch 34/200\n",
      "230/230 - 10s - loss: -2.5757e-01 - decoder_loss: -2.5820e-01 - mlp_out_loss: 6.2815e-04 - decoder_cosine: 0.4265 - decoder_mae: 0.0658 - decoder_correlation: 0.7420 - decoder_rmse: 0.1201 - mlp_out_cosine: 0.0125 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -1.1192e-02 - val_decoder_loss: -1.1811e-02 - val_mlp_out_loss: 6.1949e-04 - val_decoder_cosine: -7.8652e-03 - val_decoder_mae: 0.0556 - val_decoder_correlation: 0.9878 - val_decoder_rmse: 0.0829 - val_mlp_out_cosine: 0.0198 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 10s/epoch - 45ms/step\n",
      "Epoch 35/200\n",
      "230/230 - 11s - loss: -2.5944e-01 - decoder_loss: -2.6007e-01 - mlp_out_loss: 6.2828e-04 - decoder_cosine: 0.4323 - decoder_mae: 0.0656 - decoder_correlation: 0.7400 - decoder_rmse: 0.1203 - mlp_out_cosine: 0.0066 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -3.6865e-03 - val_decoder_loss: -4.3079e-03 - val_mlp_out_loss: 6.2140e-04 - val_decoder_cosine: -6.3719e-03 - val_decoder_mae: 0.0578 - val_decoder_correlation: 0.9945 - val_decoder_rmse: 0.0855 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0249 - 11s/epoch - 49ms/step\n",
      "Epoch 36/200\n",
      "230/230 - 8s - loss: -2.5959e-01 - decoder_loss: -2.6022e-01 - mlp_out_loss: 6.2828e-04 - decoder_cosine: 0.4227 - decoder_mae: 0.0649 - decoder_correlation: 0.7399 - decoder_rmse: 0.1193 - mlp_out_cosine: 0.0080 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -5.0822e-03 - val_decoder_loss: -5.7060e-03 - val_mlp_out_loss: 6.2382e-04 - val_decoder_cosine: 0.0174 - val_decoder_mae: 0.0548 - val_decoder_correlation: 0.9904 - val_decoder_rmse: 0.0828 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0171 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0250 - 8s/epoch - 33ms/step\n",
      "Epoch 37/200\n",
      "230/230 - 9s - loss: -2.6272e-01 - decoder_loss: -2.6335e-01 - mlp_out_loss: 6.2808e-04 - decoder_cosine: 0.4408 - decoder_mae: 0.0644 - decoder_correlation: 0.7372 - decoder_rmse: 0.1188 - mlp_out_cosine: 0.0123 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -4.7043e-03 - val_decoder_loss: -5.3271e-03 - val_mlp_out_loss: 6.2277e-04 - val_decoder_cosine: 0.0041 - val_decoder_mae: 0.0563 - val_decoder_correlation: 0.9924 - val_decoder_rmse: 0.0855 - val_mlp_out_cosine: -1.9828e-02 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0250 - 9s/epoch - 41ms/step\n",
      "Epoch 38/200\n",
      "230/230 - 10s - loss: -2.6261e-01 - decoder_loss: -2.6324e-01 - mlp_out_loss: 6.2839e-04 - decoder_cosine: 0.4448 - decoder_mae: 0.0648 - decoder_correlation: 0.7371 - decoder_rmse: 0.1200 - mlp_out_cosine: 0.0071 - mlp_out_mae: 0.0166 - mlp_out_correlation: nan - mlp_out_rmse: 0.0251 - val_loss: -4.6283e-03 - val_decoder_loss: -5.2480e-03 - val_mlp_out_loss: 6.1972e-04 - val_decoder_cosine: -3.7505e-02 - val_decoder_mae: 0.0554 - val_decoder_correlation: 0.9935 - val_decoder_rmse: 0.0827 - val_mlp_out_cosine: 0.0198 - val_mlp_out_mae: 0.0170 - val_mlp_out_correlation: nan - val_mlp_out_rmse: 0.0249 - 10s/epoch - 43ms/step\n",
      "================================================================================================\n",
      ">>> AEMLP_FOLD:2\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_3), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(34, 1) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "Epoch 1/200\n",
      "30/30 - 11s - loss: 1.4648 - decoder_loss: -2.0611e-02 - mlp_out_loss: 0.0029 - decoder_cosine: 0.0249 - decoder_mae: 0.4163 - decoder_correlation: 0.9806 - decoder_rmse: 0.6162 - mlp_out_cosine: -1.7514e-03 - mlp_out_mae: 0.0297 - mlp_out_correlation: 1.0019 - mlp_out_rmse: 0.0538 - val_loss: 1.1537 - val_decoder_loss: -7.7383e-03 - val_mlp_out_loss: 5.5256e-04 - val_decoder_cosine: 0.0515 - val_decoder_mae: 0.1836 - val_decoder_correlation: 0.9868 - val_decoder_rmse: 0.3500 - val_mlp_out_cosine: 0.0099 - val_mlp_out_mae: 0.0161 - val_mlp_out_correlation: 0.9898 - val_mlp_out_rmse: 0.0235 - 11s/epoch - 373ms/step\n",
      "Epoch 2/200\n",
      "30/30 - 8s - loss: 0.8835 - decoder_loss: -5.9998e-02 - mlp_out_loss: 7.5345e-04 - decoder_cosine: 0.0817 - decoder_mae: 0.4562 - decoder_correlation: 0.9436 - decoder_rmse: 0.6976 - mlp_out_cosine: 0.0046 - mlp_out_mae: 0.0187 - mlp_out_correlation: 0.9821 - mlp_out_rmse: 0.0274 - val_loss: 0.7150 - val_decoder_loss: -1.4619e-02 - val_mlp_out_loss: 4.6919e-04 - val_decoder_cosine: 0.0727 - val_decoder_mae: 0.2031 - val_decoder_correlation: 0.9767 - val_decoder_rmse: 0.2945 - val_mlp_out_cosine: 0.0098 - val_mlp_out_mae: 0.0151 - val_mlp_out_correlation: 0.9941 - val_mlp_out_rmse: 0.0217 - 8s/epoch - 267ms/step\n",
      "Epoch 3/200\n",
      "30/30 - 8s - loss: 0.5084 - decoder_loss: -8.2980e-02 - mlp_out_loss: 6.7220e-04 - decoder_cosine: 0.1249 - decoder_mae: 0.4949 - decoder_correlation: 0.9196 - decoder_rmse: 0.7622 - mlp_out_cosine: 0.0096 - mlp_out_mae: 0.0175 - mlp_out_correlation: 0.9634 - mlp_out_rmse: 0.0259 - val_loss: 0.4342 - val_decoder_loss: -2.2402e-02 - val_mlp_out_loss: 4.5946e-04 - val_decoder_cosine: 0.0859 - val_decoder_mae: 0.2378 - val_decoder_correlation: 0.9704 - val_decoder_rmse: 0.3098 - val_mlp_out_cosine: 0.0068 - val_mlp_out_mae: 0.0149 - val_mlp_out_correlation: 0.9891 - val_mlp_out_rmse: 0.0214 - 8s/epoch - 282ms/step\n",
      "Epoch 4/200\n",
      "30/30 - 8s - loss: 0.2771 - decoder_loss: -9.3237e-02 - mlp_out_loss: 6.5005e-04 - decoder_cosine: 0.1525 - decoder_mae: 0.4930 - decoder_correlation: 0.9086 - decoder_rmse: 0.7659 - mlp_out_cosine: 0.0091 - mlp_out_mae: 0.0171 - mlp_out_correlation: 0.9508 - mlp_out_rmse: 0.0255 - val_loss: 0.2587 - val_decoder_loss: -2.7664e-02 - val_mlp_out_loss: 4.5716e-04 - val_decoder_cosine: 0.0985 - val_decoder_mae: 0.2439 - val_decoder_correlation: 0.9664 - val_decoder_rmse: 0.3134 - val_mlp_out_cosine: 0.0047 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 0.9881 - val_mlp_out_rmse: 0.0214 - 8s/epoch - 276ms/step\n",
      "Epoch 5/200\n",
      "30/30 - 9s - loss: 0.1310 - decoder_loss: -1.0192e-01 - mlp_out_loss: 6.4532e-04 - decoder_cosine: 0.1630 - decoder_mae: 0.4457 - decoder_correlation: 0.8999 - decoder_rmse: 0.7086 - mlp_out_cosine: 0.0044 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9508 - mlp_out_rmse: 0.0254 - val_loss: 0.1548 - val_decoder_loss: -2.5939e-02 - val_mlp_out_loss: 4.5678e-04 - val_decoder_cosine: 0.0954 - val_decoder_mae: 0.2245 - val_decoder_correlation: 0.9688 - val_decoder_rmse: 0.2880 - val_mlp_out_cosine: -2.6862e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 0.9909 - val_mlp_out_rmse: 0.0214 - 9s/epoch - 306ms/step\n",
      "Epoch 6/200\n",
      "30/30 - 9s - loss: 0.0376 - decoder_loss: -1.0993e-01 - mlp_out_loss: 6.4505e-04 - decoder_cosine: 0.1688 - decoder_mae: 0.3811 - decoder_correlation: 0.8916 - decoder_rmse: 0.6045 - mlp_out_cosine: -8.3446e-04 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9484 - mlp_out_rmse: 0.0254 - val_loss: 0.0883 - val_decoder_loss: -2.6609e-02 - val_mlp_out_loss: 4.5673e-04 - val_decoder_cosine: 0.0934 - val_decoder_mae: 0.1887 - val_decoder_correlation: 0.9705 - val_decoder_rmse: 0.2471 - val_mlp_out_cosine: -3.7695e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 0.9953 - val_mlp_out_rmse: 0.0214 - 9s/epoch - 285ms/step\n",
      "Epoch 7/200\n",
      "30/30 - 9s - loss: -2.2489e-02 - decoder_loss: -1.1671e-01 - mlp_out_loss: 6.4577e-04 - decoder_cosine: 0.1844 - decoder_mae: 0.3358 - decoder_correlation: 0.8819 - decoder_rmse: 0.5318 - mlp_out_cosine: -5.2020e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9539 - mlp_out_rmse: 0.0254 - val_loss: 0.0438 - val_decoder_loss: -2.9858e-02 - val_mlp_out_loss: 4.5673e-04 - val_decoder_cosine: 0.0985 - val_decoder_mae: 0.1591 - val_decoder_correlation: 0.9689 - val_decoder_rmse: 0.2110 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0001 - val_mlp_out_rmse: 0.0214 - 9s/epoch - 289ms/step\n",
      "Epoch 8/200\n",
      "30/30 - 9s - loss: -6.2298e-02 - decoder_loss: -1.2294e-01 - mlp_out_loss: 6.4622e-04 - decoder_cosine: 0.1816 - decoder_mae: 0.2910 - decoder_correlation: 0.8770 - decoder_rmse: 0.4670 - mlp_out_cosine: -5.8002e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9591 - mlp_out_rmse: 0.0254 - val_loss: 0.0179 - val_decoder_loss: -2.9580e-02 - val_mlp_out_loss: 4.5672e-04 - val_decoder_cosine: 0.0950 - val_decoder_mae: 0.1419 - val_decoder_correlation: 0.9693 - val_decoder_rmse: 0.1890 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0037 - val_mlp_out_rmse: 0.0214 - 9s/epoch - 290ms/step\n",
      "Epoch 9/200\n",
      "30/30 - 9s - loss: -8.5347e-02 - decoder_loss: -1.2457e-01 - mlp_out_loss: 6.4645e-04 - decoder_cosine: 0.1864 - decoder_mae: 0.2615 - decoder_correlation: 0.8788 - decoder_rmse: 0.4190 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9712 - mlp_out_rmse: 0.0254 - val_loss: 1.8305e-04 - val_decoder_loss: -3.0535e-02 - val_mlp_out_loss: 4.5670e-04 - val_decoder_cosine: 0.0943 - val_decoder_mae: 0.1305 - val_decoder_correlation: 0.9706 - val_decoder_rmse: 0.1740 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0075 - val_mlp_out_rmse: 0.0214 - 9s/epoch - 293ms/step\n",
      "Epoch 10/200\n",
      "30/30 - 8s - loss: -1.0179e-01 - decoder_loss: -1.2722e-01 - mlp_out_loss: 6.4651e-04 - decoder_cosine: 0.1847 - decoder_mae: 0.2439 - decoder_correlation: 0.8731 - decoder_rmse: 0.3868 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9717 - mlp_out_rmse: 0.0254 - val_loss: -1.0943e-02 - val_decoder_loss: -3.0790e-02 - val_mlp_out_loss: 4.5670e-04 - val_decoder_cosine: 0.0939 - val_decoder_mae: 0.1121 - val_decoder_correlation: 0.9698 - val_decoder_rmse: 0.1519 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0104 - val_mlp_out_rmse: 0.0214 - 8s/epoch - 282ms/step\n",
      "Epoch 11/200\n",
      "30/30 - 8s - loss: -1.1345e-01 - decoder_loss: -1.2992e-01 - mlp_out_loss: 6.4652e-04 - decoder_cosine: 0.1823 - decoder_mae: 0.2259 - decoder_correlation: 0.8704 - decoder_rmse: 0.3616 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9736 - mlp_out_rmse: 0.0254 - val_loss: -1.8784e-02 - val_decoder_loss: -3.1572e-02 - val_mlp_out_loss: 4.5674e-04 - val_decoder_cosine: 0.0972 - val_decoder_mae: 0.1032 - val_decoder_correlation: 0.9711 - val_decoder_rmse: 0.1433 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0119 - val_mlp_out_rmse: 0.0214 - 8s/epoch - 271ms/step\n",
      "Epoch 12/200\n",
      "30/30 - 9s - loss: -1.2064e-01 - decoder_loss: -1.3131e-01 - mlp_out_loss: 6.4654e-04 - decoder_cosine: 0.1804 - decoder_mae: 0.2078 - decoder_correlation: 0.8700 - decoder_rmse: 0.3376 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9737 - mlp_out_rmse: 0.0254 - val_loss: -1.9265e-02 - val_decoder_loss: -2.7481e-02 - val_mlp_out_loss: 4.5670e-04 - val_decoder_cosine: 0.0941 - val_decoder_mae: 0.1120 - val_decoder_correlation: 0.9711 - val_decoder_rmse: 0.1479 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0129 - val_mlp_out_rmse: 0.0214 - 9s/epoch - 302ms/step\n",
      "Epoch 13/200\n",
      "30/30 - 9s - loss: -1.2611e-01 - decoder_loss: -1.3303e-01 - mlp_out_loss: 6.4655e-04 - decoder_cosine: 0.1824 - decoder_mae: 0.1964 - decoder_correlation: 0.8702 - decoder_rmse: 0.3199 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9764 - mlp_out_rmse: 0.0254 - val_loss: -2.3628e-02 - val_decoder_loss: -2.8904e-02 - val_mlp_out_loss: 4.5670e-04 - val_decoder_cosine: 0.1041 - val_decoder_mae: 0.0907 - val_decoder_correlation: 0.9710 - val_decoder_rmse: 0.1258 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0113 - val_mlp_out_rmse: 0.0214 - 9s/epoch - 285ms/step\n",
      "Epoch 14/200\n",
      "30/30 - 8s - loss: -1.3010e-01 - decoder_loss: -1.3462e-01 - mlp_out_loss: 6.4653e-04 - decoder_cosine: 0.1886 - decoder_mae: 0.1890 - decoder_correlation: 0.8675 - decoder_rmse: 0.3058 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9799 - mlp_out_rmse: 0.0254 - val_loss: -2.6946e-02 - val_decoder_loss: -3.0353e-02 - val_mlp_out_loss: 4.5671e-04 - val_decoder_cosine: 0.1049 - val_decoder_mae: 0.0888 - val_decoder_correlation: 0.9704 - val_decoder_rmse: 0.1228 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0097 - val_mlp_out_rmse: 0.0214 - 8s/epoch - 270ms/step\n",
      "Epoch 15/200\n",
      "30/30 - 9s - loss: -1.3317e-01 - decoder_loss: -1.3617e-01 - mlp_out_loss: 6.4653e-04 - decoder_cosine: 0.1772 - decoder_mae: 0.1748 - decoder_correlation: 0.8665 - decoder_rmse: 0.2849 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9824 - mlp_out_rmse: 0.0254 - val_loss: -2.6921e-02 - val_decoder_loss: -2.9157e-02 - val_mlp_out_loss: 4.5675e-04 - val_decoder_cosine: 0.1049 - val_decoder_mae: 0.0956 - val_decoder_correlation: 0.9703 - val_decoder_rmse: 0.1305 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0080 - val_mlp_out_rmse: 0.0214 - 9s/epoch - 291ms/step\n",
      "Epoch 16/200\n",
      "30/30 - 9s - loss: -1.3514e-01 - decoder_loss: -1.3720e-01 - mlp_out_loss: 6.4653e-04 - decoder_cosine: 0.1892 - decoder_mae: 0.1648 - decoder_correlation: 0.8653 - decoder_rmse: 0.2666 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9845 - mlp_out_rmse: 0.0254 - val_loss: -2.5395e-02 - val_decoder_loss: -2.6907e-02 - val_mlp_out_loss: 4.5673e-04 - val_decoder_cosine: 0.1030 - val_decoder_mae: 0.0909 - val_decoder_correlation: 0.9709 - val_decoder_rmse: 0.1248 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0055 - val_mlp_out_rmse: 0.0214 - 9s/epoch - 287ms/step\n",
      "Epoch 17/200\n",
      "30/30 - 9s - loss: -1.3799e-01 - decoder_loss: -1.3947e-01 - mlp_out_loss: 6.4654e-04 - decoder_cosine: 0.1896 - decoder_mae: 0.1598 - decoder_correlation: 0.8635 - decoder_rmse: 0.2592 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9901 - mlp_out_rmse: 0.0254 - val_loss: -2.9443e-02 - val_decoder_loss: -3.0516e-02 - val_mlp_out_loss: 4.5670e-04 - val_decoder_cosine: 0.1056 - val_decoder_mae: 0.0774 - val_decoder_correlation: 0.9731 - val_decoder_rmse: 0.1117 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0030 - val_mlp_out_rmse: 0.0214 - 9s/epoch - 290ms/step\n",
      "Epoch 18/200\n",
      "30/30 - 8s - loss: -1.3720e-01 - decoder_loss: -1.3832e-01 - mlp_out_loss: 6.4653e-04 - decoder_cosine: 0.1848 - decoder_mae: 0.1535 - decoder_correlation: 0.8626 - decoder_rmse: 0.2476 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9937 - mlp_out_rmse: 0.0254 - val_loss: -2.9539e-02 - val_decoder_loss: -3.0349e-02 - val_mlp_out_loss: 4.5672e-04 - val_decoder_cosine: 0.1070 - val_decoder_mae: 0.0770 - val_decoder_correlation: 0.9715 - val_decoder_rmse: 0.1112 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 0.9990 - val_mlp_out_rmse: 0.0214 - 8s/epoch - 268ms/step\n",
      "Epoch 19/200\n",
      "30/30 - 9s - loss: -1.4059e-01 - decoder_loss: -1.4151e-01 - mlp_out_loss: 6.4657e-04 - decoder_cosine: 0.1940 - decoder_mae: 0.1522 - decoder_correlation: 0.8617 - decoder_rmse: 0.2471 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9975 - mlp_out_rmse: 0.0254 - val_loss: -3.2903e-02 - val_decoder_loss: -3.3558e-02 - val_mlp_out_loss: 4.5675e-04 - val_decoder_cosine: 0.1096 - val_decoder_mae: 0.0794 - val_decoder_correlation: 0.9701 - val_decoder_rmse: 0.1155 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 0.9999 - val_mlp_out_rmse: 0.0214 - 9s/epoch - 284ms/step\n",
      "Epoch 20/200\n",
      "30/30 - 9s - loss: -1.4125e-01 - decoder_loss: -1.4205e-01 - mlp_out_loss: 6.4656e-04 - decoder_cosine: 0.2002 - decoder_mae: 0.1512 - decoder_correlation: 0.8583 - decoder_rmse: 0.2451 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 0.9983 - mlp_out_rmse: 0.0254 - val_loss: -3.4491e-02 - val_decoder_loss: -3.5058e-02 - val_mlp_out_loss: 4.5675e-04 - val_decoder_cosine: 0.1163 - val_decoder_mae: 0.0802 - val_decoder_correlation: 0.9685 - val_decoder_rmse: 0.1158 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 0.9999 - val_mlp_out_rmse: 0.0214 - 9s/epoch - 288ms/step\n",
      "Epoch 21/200\n",
      "30/30 - 9s - loss: -1.4226e-01 - decoder_loss: -1.4300e-01 - mlp_out_loss: 6.4654e-04 - decoder_cosine: 0.1959 - decoder_mae: 0.1442 - decoder_correlation: 0.8573 - decoder_rmse: 0.2357 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 1.0000 - mlp_out_rmse: 0.0254 - val_loss: -3.3663e-02 - val_decoder_loss: -3.4180e-02 - val_mlp_out_loss: 4.5683e-04 - val_decoder_cosine: 0.1085 - val_decoder_mae: 0.0809 - val_decoder_correlation: 0.9713 - val_decoder_rmse: 0.1177 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 0.9998 - val_mlp_out_rmse: 0.0214 - 9s/epoch - 297ms/step\n",
      "Epoch 22/200\n",
      "30/30 - 9s - loss: -1.4365e-01 - decoder_loss: -1.4434e-01 - mlp_out_loss: 6.4658e-04 - decoder_cosine: 0.1962 - decoder_mae: 0.1398 - decoder_correlation: 0.8569 - decoder_rmse: 0.2265 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 1.0000 - mlp_out_rmse: 0.0254 - val_loss: -3.2446e-02 - val_decoder_loss: -3.2935e-02 - val_mlp_out_loss: 4.5686e-04 - val_decoder_cosine: 0.0889 - val_decoder_mae: 0.0769 - val_decoder_correlation: 0.9744 - val_decoder_rmse: 0.1142 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0214 - 9s/epoch - 305ms/step\n",
      "Epoch 23/200\n",
      "30/30 - 9s - loss: -1.4500e-01 - decoder_loss: -1.4567e-01 - mlp_out_loss: 6.4654e-04 - decoder_cosine: 0.2072 - decoder_mae: 0.1322 - decoder_correlation: 0.8533 - decoder_rmse: 0.2152 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: nan - mlp_out_rmse: 0.0254 - val_loss: -3.3593e-02 - val_decoder_loss: -3.4067e-02 - val_mlp_out_loss: 4.5697e-04 - val_decoder_cosine: 0.1112 - val_decoder_mae: 0.0751 - val_decoder_correlation: 0.9711 - val_decoder_rmse: 0.1118 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0214 - 9s/epoch - 297ms/step\n",
      "Epoch 24/200\n",
      "30/30 - 9s - loss: -1.4520e-01 - decoder_loss: -1.4586e-01 - mlp_out_loss: 6.4658e-04 - decoder_cosine: 0.1976 - decoder_mae: 0.1316 - decoder_correlation: 0.8554 - decoder_rmse: 0.2146 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 1.0000 - mlp_out_rmse: 0.0254 - val_loss: -2.9855e-02 - val_decoder_loss: -3.0321e-02 - val_mlp_out_loss: 4.5678e-04 - val_decoder_cosine: 0.0885 - val_decoder_mae: 0.0724 - val_decoder_correlation: 0.9768 - val_decoder_rmse: 0.1078 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0214 - 9s/epoch - 286ms/step\n",
      "Epoch 25/200\n",
      "30/30 - 9s - loss: -1.4783e-01 - decoder_loss: -1.4849e-01 - mlp_out_loss: 6.4654e-04 - decoder_cosine: 0.2072 - decoder_mae: 0.1225 - decoder_correlation: 0.8525 - decoder_rmse: 0.1992 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 1.0000 - mlp_out_rmse: 0.0254 - val_loss: -2.7911e-02 - val_decoder_loss: -2.8372e-02 - val_mlp_out_loss: 4.5697e-04 - val_decoder_cosine: 0.0904 - val_decoder_mae: 0.0733 - val_decoder_correlation: 0.9778 - val_decoder_rmse: 0.1092 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0214 - 9s/epoch - 284ms/step\n",
      "Epoch 26/200\n",
      "30/30 - 8s - loss: -1.4819e-01 - decoder_loss: -1.4884e-01 - mlp_out_loss: 6.4655e-04 - decoder_cosine: 0.2018 - decoder_mae: 0.1168 - decoder_correlation: 0.8513 - decoder_rmse: 0.1901 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: nan - mlp_out_rmse: 0.0254 - val_loss: -2.7929e-02 - val_decoder_loss: -2.8388e-02 - val_mlp_out_loss: 4.5673e-04 - val_decoder_cosine: 0.0998 - val_decoder_mae: 0.0677 - val_decoder_correlation: 0.9788 - val_decoder_rmse: 0.1011 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0214 - 8s/epoch - 281ms/step\n",
      "Epoch 27/200\n",
      "30/30 - 8s - loss: -1.4971e-01 - decoder_loss: -1.5035e-01 - mlp_out_loss: 6.4653e-04 - decoder_cosine: 0.2139 - decoder_mae: 0.1118 - decoder_correlation: 0.8503 - decoder_rmse: 0.1806 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 1.0000 - mlp_out_rmse: 0.0254 - val_loss: -2.8508e-02 - val_decoder_loss: -2.8966e-02 - val_mlp_out_loss: 4.5670e-04 - val_decoder_cosine: 0.0989 - val_decoder_mae: 0.0708 - val_decoder_correlation: 0.9746 - val_decoder_rmse: 0.1053 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0214 - 8s/epoch - 280ms/step\n",
      "Epoch 28/200\n",
      "30/30 - 9s - loss: -1.5151e-01 - decoder_loss: -1.5215e-01 - mlp_out_loss: 6.4653e-04 - decoder_cosine: 0.2132 - decoder_mae: 0.1102 - decoder_correlation: 0.8501 - decoder_rmse: 0.1790 - mlp_out_cosine: -5.8128e-03 - mlp_out_mae: 0.0170 - mlp_out_correlation: 1.0000 - mlp_out_rmse: 0.0254 - val_loss: -2.9951e-02 - val_decoder_loss: -3.0408e-02 - val_mlp_out_loss: 4.5670e-04 - val_decoder_cosine: 0.0927 - val_decoder_mae: 0.0639 - val_decoder_correlation: 0.9747 - val_decoder_rmse: 0.0970 - val_mlp_out_cosine: -3.8224e-03 - val_mlp_out_mae: 0.0148 - val_mlp_out_correlation: 1.0000 - val_mlp_out_rmse: 0.0214 - 9s/epoch - 293ms/step\n",
      "================================================================================================\n"
     ]
    }
   ],
   "source": [
    "gkf = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, \n",
    "                                 group_gap = GROUP_GAP, \n",
    "                                 max_train_group_size = MAX_TRAIN_GROUP_SIZE, \n",
    "                                 max_test_group_size  = MAX_TEST_GROUP_SIZE).split(X, y, groups[0])\n",
    "models = []\n",
    "for fold, (train_idx, val_idx) in enumerate(list(gkf)):\n",
    "    x_train, x_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    print(f'>>> AEMLP_FOLD:{fold}')\n",
    "    K.clear_session()\n",
    "    with strategy.scope(): model = build_model(hp, dim = x_train.shape[1], fold=fold)\n",
    "    model_save = tf.keras.callbacks.ModelCheckpoint('./fold-%i.hdf5' %(fold), \n",
    "                                                         monitor = 'val_mlp_out_rmse', verbose = 0, \n",
    "                                                         save_best_only = True, save_weights_only = True,\n",
    "                                                         mode = 'min', save_freq = 'epoch')\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_mlp_out_rmse', patience=15, mode='min', restore_best_weights=True)\n",
    "    history = model.fit(x_train, y_train ,\n",
    "                        epochs          = 200, \n",
    "                        callbacks       = [model_save, early_stop], \n",
    "                        validation_data = (x_val, y_val), \n",
    "                        batch_size      = batch_size[fold],\n",
    "                        verbose         = 2) \n",
    "    print('='*96)\n",
    "    models.append(model)\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_parquet('../Output/valid_scaling.parquet')\n",
    "test = pd.read_parquet('../Output/test_scaling.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_rank(df):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df (pd.DataFrame): including predict column\n",
    "    Returns:\n",
    "        df (pd.DataFrame): df with Rank\n",
    "    \"\"\"\n",
    "    # sort records to set Rank\n",
    "    df = df.sort_values(\"predict\", ascending=False)\n",
    "    # set Rank starting from 0\n",
    "    df.loc[:, \"Rank\"] = np.arange(len(df[\"predict\"]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2, rank_col='Rank') -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df (pd.DataFrame): predicted results\n",
    "        portfolio_size (int): # of equities to buy/sell\n",
    "        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n",
    "    Returns:\n",
    "        (float): sharpe ratio\n",
    "    \"\"\"\n",
    "\n",
    "    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): predicted results\n",
    "            portfolio_size (int): # of equities to buy/sell\n",
    "            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n",
    "        Returns:\n",
    "            (float): spread return\n",
    "        \"\"\"\n",
    "        assert df[rank_col].min() == 0\n",
    "        assert df[rank_col].max() == len(df[rank_col]) - 1\n",
    "        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n",
    "        purchase = (df.sort_values(by=rank_col)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n",
    "        short = (df.sort_values(by=rank_col, ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n",
    "        return purchase - short\n",
    "\n",
    "    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n",
    "    sharpe_ratio = buf.mean() / buf.std()\n",
    "    return sharpe_ratio, buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2625/2625 [==============================] - 24s 9ms/step\n",
      "2625/2625 [==============================] - 4s 2ms/step\n",
      "2625/2625 [==============================] - 4s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "ap = [0.10,0.10,0.80]\n",
    "model_x = list()\n",
    "for i in range(FOLDS):\n",
    "    prediction_x = models[i].predict(valid[col_use])[-1] * ap[i]\n",
    "    model_x.append(prediction_x)\n",
    "model_x = np.mean(model_x, axis = 0)\n",
    "valid['predict'] = model_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson: -0.008765118173432679\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "pearson_score = stats.pearsonr(valid['predict'], valid.Target)[0]\n",
    "print('Pearson:', pearson_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = valid.sort_values([\"Date\", \"predict\"], ascending=[True, False])\n",
    "ranking = valid.groupby(\"Date\").apply(set_rank).reset_index(drop=True)\n",
    "sharp_ratio, _ = calc_spread_return_sharpe(ranking, portfolio_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04001829650484312"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sharp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2313/2313 [==============================] - 22s 9ms/step\n",
      "2313/2313 [==============================] - 4s 1ms/step\n",
      "2313/2313 [==============================] - 4s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "ap = [0.10,0.10,0.80]\n",
    "model_x = list()\n",
    "for i in range(FOLDS):\n",
    "    prediction_x = models[i].predict(test[col_use])[-1] * ap[i]\n",
    "    model_x.append(prediction_x)\n",
    "model_x = np.mean(model_x, axis = 0)\n",
    "test['predict'] = model_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson: -0.01603400219233213\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "pearson_score = stats.pearsonr(test['predict'], test.Target)[0]\n",
    "print('Pearson:', pearson_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.sort_values([\"Date\", \"predict\"], ascending=[True, False])\n",
    "ranking = test.groupby(\"Date\").apply(set_rank).reset_index(drop=True)\n",
    "sharp_ratio, _ = calc_spread_return_sharpe(ranking, portfolio_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026063345426134824"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sharp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2022-01-04   -0.451871\n",
       "2022-01-05    0.176159\n",
       "2022-01-06    1.384017\n",
       "2022-01-07   -1.483591\n",
       "2022-01-11    2.856867\n",
       "2022-01-12    1.364808\n",
       "2022-01-13    1.095467\n",
       "2022-01-14   -0.906043\n",
       "2022-01-17    2.218913\n",
       "2022-01-18   -3.129206\n",
       "2022-01-19    3.333408\n",
       "2022-01-20    0.411250\n",
       "2022-01-21    5.106139\n",
       "2022-01-24   -1.035561\n",
       "2022-01-25    6.211541\n",
       "2022-01-26    1.877186\n",
       "2022-01-27   -4.804883\n",
       "2022-01-28   -0.347941\n",
       "2022-01-31   -4.055925\n",
       "2022-02-01    3.611171\n",
       "2022-02-02   -0.942829\n",
       "2022-02-03    2.831975\n",
       "2022-02-04    1.599319\n",
       "2022-02-07   -1.792207\n",
       "2022-02-08   -2.325364\n",
       "2022-02-09    2.239269\n",
       "2022-02-10    0.086879\n",
       "2022-02-14    0.261467\n",
       "2022-02-15    0.432656\n",
       "2022-02-16   -0.047880\n",
       "2022-02-17    1.899827\n",
       "2022-02-18    0.314594\n",
       "2022-02-21    2.213883\n",
       "2022-02-22   -4.277294\n",
       "2022-02-24   -2.091942\n",
       "2022-02-25   -9.759565\n",
       "2022-02-28   -1.210895\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12543d8aedefe43b1af9739f504be95c6574cbf3bd96045c58756e8a60ca5178"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
